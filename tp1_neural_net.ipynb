{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP1: Réseau de neurones à deux couches, fonction de perte **Entropie croisée**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from utils.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# pour automatiquement recharger les modules externes\n",
    "# voir http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5>MLP à deux couches</font>\n",
    "\n",
    "Ici, nous développerons un **réseau pleinement connecté** à deux couches avec **softmax** et **entropie croisée**.  En principe, vous devriez avoir pris connaissance du notebook \"tp1_simple_neural_net.ipynb\".  Le but ici est d'enchasser les différents éléments d'un réseau de neurones dans des **classes**.\n",
    "\n",
    "Au début, nous commencerons avec un petit réseau à \n",
    "\n",
    "* **10 neurones cachées**\n",
    "* **3 classes** \n",
    "* un vecteur d'entrée de taille **4**.\n",
    "\n",
    "Avant de commencer, vous devez vous familier avec les classes **model.Model**, **layers.Dense** et la loss **cross_entropy_loss** (cette dernière fonction comprend le softmax + la loss)\n",
    "\n",
    "Le code à produire se situe : \n",
    "\n",
    "* dans la classe **Dense** (répertoire layer)\n",
    "* dans la fonction **cross_entropy_loss** (dans utils.model_loss)\n",
    "* dans le fichier **activations.py** (répetoire utils)\n",
    "\n",
    "À ce point-ci, l'entropie croisée (et son gradient) que vous avez codé précédemment **doit être fonctionnelle**.  Ce code peut donc en bonne partie être récupéré ici!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# On crée un modèle jouet pour rapidement tester les différentes composantes #\n",
    "# La classe Model encapsule et relie les différentes couches de notre        #\n",
    "# réseau de neurones.                                                        #\n",
    "##############################################################################\n",
    "from model.Model import Model\n",
    "from layers.Dense import Dense\n",
    "from utils.model_loss import cross_entropy_loss\n",
    "\n",
    "hidden_size = 10\n",
    "num_classes = 3\n",
    "input_size = 4\n",
    "\n",
    "def create_toy_model():\n",
    "    np.random.seed(0)\n",
    "    model = Model()\n",
    "    model.add(Dense(input_size, hidden_size, weight_scale=1e-1, activation='relu'))\n",
    "    model.add(Dense(hidden_size, num_classes, weight_scale=1e-1))\n",
    "    model.add_loss(cross_entropy_loss)\n",
    "    return model\n",
    "\n",
    "model = create_toy_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_toy_data(nb_elements, input_size, nb_classes):\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    X = 10 * np.random.randn(nb_elements, input_size)\n",
    "    y = np.random.randint(nb_classes, size=nb_elements)\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y=create_random_toy_data(1, 5, 3)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commençons avec la prédiction d'**un seul vecteur d'entrée** de taille 4 et dont la cible est 0.  Ici, le **score** est la sortie du réseau avant la **softmax**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your scores:\n",
      "[[-0.81233741 -1.27654624 -0.70335995]]\n",
      "\n",
      "correct scores:\n",
      "[[-0.81233741 -1.27654624 -0.70335995]]\n",
      "\n",
      "Difference between your scores and correct scores:\n",
      "5.897841770519108e-09\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# TODO: Implémenter la méthode forward (propagation avant) de la classe de   #\n",
    "# couche Dense.                                                              #\n",
    "##############################################################################\n",
    "X, y = create_random_toy_data(1, input_size, num_classes)\n",
    "\n",
    "scores = model.forward(X)\n",
    "print('Your scores:')\n",
    "print(scores)\n",
    "print()\n",
    "print('correct scores:')\n",
    "correct_scores = np.asarray([[-0.81233741, -1.27654624, -0.70335995]])\n",
    "print(correct_scores)\n",
    "print()\n",
    "\n",
    "# La différence devrait être assez basse, en principe inférieure à 1e-7.\n",
    "print('Difference between your scores and correct scores:')\n",
    "print(np.sum(np.abs(scores - correct_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax:  [[0.3644621  0.22911264 0.40642526]]\n",
      "Loss:  0.9382860736162932\n",
      "\n",
      "Correct loss:  0.938286073616293\n",
      "Difference between your loss and correct loss:\n",
      "2.220446049250313e-16\n",
      "\n",
      "Difference between your softmax and correct softmax:\n",
      "8.204646462228737e-09\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# TODO: Implémenter le calcul de perte vectorisé adapté pour un réseau de    #\n",
    "# neurones dans la fonction softmax_loss. La différence avec la softmax      #\n",
    "# implémentée dans la partie précédente est que la dérivée retournée par     #\n",
    "# softmax_loss est en fonction des scores, et non en fonction des poids      #\n",
    "##############################################################################\n",
    "\n",
    "loss, _, softmax = model.calculate_loss(scores, y, 0.1)\n",
    "print('Softmax: ', softmax)\n",
    "print('Loss: ', loss)\n",
    "\n",
    "correct_loss = 0.938286073616293\n",
    "correct_softmax = np.asarray([0.3644621, 0.22911264, 0.40642526])\n",
    "\n",
    "print('\\nCorrect loss: ', correct_loss)\n",
    "print('Difference between your loss and correct loss:')\n",
    "# on devrait obtenir une erreur de loss inférieure à 1e-9.\n",
    "print(np.sum(np.abs(loss - correct_loss)))\n",
    "\n",
    "# on devrait obtenir une erreur de softmax inférieure à 1e-7.\n",
    "print('\\nDifference between your softmax and correct softmax:')\n",
    "print(np.sum(np.abs(correct_softmax - softmax)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maintenant allons-y avec la prédiction de **5 vecteurs d'entrée** de taille 4 et dont la cible est 0.  Ici, le **score** est la sortie du réseau avant la **softmax**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your scores:\n",
      "[[-0.81233741 -1.27654624 -0.70335995]\n",
      " [-0.17129677 -1.18803311 -0.47310444]\n",
      " [-0.51590475 -1.01354314 -0.8504215 ]\n",
      " [-0.15419291 -0.48629638 -0.52901952]\n",
      " [-0.00618733 -0.12435261 -0.15226949]]\n",
      "\n",
      "Correct scores:\n",
      "[[-0.81233741 -1.27654624 -0.70335995]\n",
      " [-0.17129677 -1.18803311 -0.47310444]\n",
      " [-0.51590475 -1.01354314 -0.8504215 ]\n",
      " [-0.15419291 -0.48629638 -0.52901952]\n",
      " [-0.00618733 -0.12435261 -0.15226949]]\n",
      "\n",
      "Difference between your scores and correct scores:\n",
      "3.6802720745909845e-08\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# TODO: Implémenter la méthode forward (propagation avant) de la classe de   #\n",
    "# couche Dense.                                                              #\n",
    "##############################################################################\n",
    "X, y = create_random_toy_data(5, input_size, num_classes)\n",
    "\n",
    "scores = model.forward(X)\n",
    "print('Your scores:')\n",
    "print(scores)\n",
    "print()\n",
    "print('Correct scores:')\n",
    "correct_scores = np.asarray([\n",
    "  [-0.81233741, -1.27654624, -0.70335995],\n",
    "  [-0.17129677, -1.18803311, -0.47310444],\n",
    "  [-0.51590475, -1.01354314, -0.8504215 ],\n",
    "  [-0.15419291, -0.48629638, -0.52901952],\n",
    "  [-0.00618733, -0.12435261, -0.15226949]])\n",
    "print(correct_scores)\n",
    "print()\n",
    "\n",
    "# La différence devrait être assez basse, en principe inférieure à 1e-7.\n",
    "print('Difference between your scores and correct scores:')\n",
    "print(np.sum(np.abs(scores - correct_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax:  [[0.3644621  0.22911264 0.40642526]\n",
      " [0.47590629 0.17217039 0.35192332]\n",
      " [0.43035767 0.26164229 0.30800004]\n",
      " [0.41583127 0.2983228  0.28584593]\n",
      " [0.36328815 0.32279939 0.31391246]]\n",
      "Loss:  1.0636830073861907\n",
      "\n",
      "Correct loss:  1.06368300738\n",
      "Difference between your loss and correct loss:\n",
      "6.190603585309873e-12\n",
      "\n",
      "Difference between your softmax and correct softmax:\n",
      "2.9173411658645065e-08\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# TODO: Implémenter le calcul de perte vectorisé adapté pour un réseau de    #\n",
    "# neurones dans la fonction softmax_loss. La différence avec la softmax      #\n",
    "# implémentée dans la partie précédente est que la dérivée retournée par     #\n",
    "# softmax_loss est en fonction des scores, et non en fonction des poids      #\n",
    "#                                                                            #\n",
    "# Code à modifier dans \"calculate_loss\" du fichier model_loss.py             #\n",
    "##############################################################################\n",
    "\n",
    "loss, _, softmax = model.calculate_loss(scores, y, 0.1)\n",
    "print('Softmax: ', softmax)\n",
    "print('Loss: ', loss)\n",
    "\n",
    "correct_loss = 1.06368300738\n",
    "\n",
    "correct_softmax = np.asarray([\n",
    "    [0.3644621,  0.22911264, 0.40642526],\n",
    "    [0.47590629, 0.17217039, 0.35192332],\n",
    "    [0.43035767, 0.26164229, 0.30800004],\n",
    "    [0.41583127, 0.2983228,  0.28584593],\n",
    "    [0.36328815, 0.32279939, 0.31391246]])\n",
    "\n",
    "# on devrait obtenir une erreur de loss inférieure à 1e-10.\n",
    "print('\\nCorrect loss: ', correct_loss)\n",
    "print('Difference between your loss and correct loss:')\n",
    "print(np.sum(np.abs(loss - correct_loss)))\n",
    "\n",
    "# on devrait obtenir une erreur de softmax inférieure à 1e-7.\n",
    "print('\\nDifference between your softmax and correct softmax:')\n",
    "print(np.sum(np.abs(softmax - correct_softmax)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rétropropagation\n",
    "\n",
    "Maintenant vous devez rédiger la **rétro-progatation** de la classe **model**.  Vos gradients seront testés avec un gradient numérique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 3)\n",
      "[[-0.12710758  0.04582253  0.08128505]\n",
      " [-0.10481874  0.03443408  0.07038466]\n",
      " [ 0.08607153 -0.14767154  0.06160001]\n",
      " [-0.11683375  0.05966456  0.05716919]\n",
      " [ 0.07265763  0.06455988 -0.13721751]]\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# TODO: Implémenter la méthode backward (rétro-propagation) de la classe de  #\n",
    "# couche Dense, ainsi que la méthode backward de la classe Model.            #\n",
    "##############################################################################\n",
    "loss, dScores, softmax = model.calculate_loss(scores, y, 0.1)\n",
    "\n",
    "print(dScores.shape)\n",
    "print(dScores)\n",
    "_ = model.backward(dScores)  # les gradients sont stockés dans les objets \"layers\" du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_num :  (4, 10)\n",
      "grad calcul :  (4, 10)\n",
      "L0-W max relative error: 3.561318e-09\n",
      "grad_num :  (10,)\n",
      "grad calcul :  (10,)\n",
      "L0-b max relative error: 6.304492e-10\n",
      "grad_num :  (10, 3)\n",
      "grad calcul :  (10, 3)\n",
      "L1-W max relative error: 3.440708e-09\n",
      "grad_num :  (3,)\n",
      "grad calcul :  (3,)\n",
      "L1-b max relative error: 6.665482e-11\n"
     ]
    }
   ],
   "source": [
    "from utils.gradients import evaluate_numerical_gradient\n",
    "\n",
    "# Retourne l'erreur relative maximale des matrices de gradients passées en paramètre.\n",
    "# Pour chaque paramètre, l'erreur relative devrait être inférieure à environ 1e-8.\n",
    "def rel_error(x, y):\n",
    "    rel = np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y)))\n",
    "    return np.max(rel)\n",
    "\n",
    "gradients = model.gradients()\n",
    "model_params = model.parameters()\n",
    "\n",
    "# Si tout va bien, vous devriez avoir des erreurs inférieurs à 1e-8\n",
    "for layer_name, layer_params in model_params.items():\n",
    "    for param_name, _ in layer_params.items():\n",
    "        grad_num = evaluate_numerical_gradient(X, y, model, layer_name, param_name, reg=0.1)\n",
    "        print(\"grad_num : \", grad_num.shape)\n",
    "        print(\"grad calcul : \", gradients[layer_name][param_name].shape)\n",
    "        max_error = rel_error(grad_num, gradients[layer_name][param_name])\n",
    "        print('%s max relative error: %e' % (layer_name + '-' + param_name, max_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entraînement\n",
    "\n",
    "Il est maintanant temps de rédiger le code de la descente de gradient devant permettre d'entraîner le modèle.  Le code à rédiger est dans le fichier **model/Solver.py**.  Si votre code fonctionne, le graphique de la loss devrait **descendre**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 100: loss 1.029545\n",
      "Final training loss:  0.03298508028218976\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from visualization.utils import visualize_loss\n",
    "from model.Solver import solver, SGD\n",
    "\n",
    "model = create_toy_model()\n",
    "sgd_optimizer = SGD(1e-1, model)\n",
    "\n",
    "#\n",
    "# TODO\n",
    "# Ajouter code de descente de gradient dans la fonction solver du fichier Solver.py\n",
    "#\n",
    "loss_history, _, _ = solver(X, y, X, y, 1e-2, sgd_optimizer, num_iter=100)\n",
    "\n",
    "print('Final training loss: ', loss_history[-1])\n",
    "\n",
    "# Visualisation de l'historique de perte lors de l'entraînement\n",
    "visualize_loss(loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">Classifieur d'images</font>\n",
    "\n",
    "Si votre code fonctionne bien, vous devriez être capable d'entraîner un modèle sur CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Charger la banque de données CIFAR-10, prétraiter les images et ajouter une dimension pour le biais.\n",
    "    \n",
    "    Input :\n",
    "    - num_training : nombre d'images à mettre dans l'ensemble d'entrainement\n",
    "    - num_validation : nombre d'images à mettre dans l'ensemble de validation\n",
    "    - num_test : nombre d'images à mettre dans l'ensemble de test\n",
    "    - num_dev : d'images à mettre dans l'ensemble dev\n",
    "    \n",
    "    Output :\n",
    "    - X_train, y_train : données et cibles d'entrainement\n",
    "    - X_val, y_val: données et cibles de validation\n",
    "    - X_test y_test: données et cibles de test \n",
    "    - X_dev, y_dev: données et cicles dev\n",
    "    \"\"\"\n",
    "    # Charger les données CIFAR-10\n",
    "    cifar10_dir = 'datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "  \n",
    "    # Séparer en ensembles d'entraînement, de validation, de test et de dev\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "\n",
    "    # Normalisation\n",
    "    X_train -= np.mean(X_train, axis = 0)\n",
    "    X_val -= np.mean(X_val, axis = 0)\n",
    "    X_test -= np.mean(X_test, axis = 0)\n",
    "    X_dev -= np.mean(X_dev, axis = 0)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3072)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3072)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3072)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3072)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "# Charger les images pour les différents ensembles de données\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si le modèle et toutes ses composantes ont bien été implémentées\n",
    "# à l'étape du modèle jouet,\n",
    "def create_model(hidden_size):\n",
    "    model = Model()\n",
    "    model.add(Dense(dim_input=3*32*32, dim_output=hidden_size, activation='relu'))\n",
    "    model.add(Dense(dim_input=hidden_size, dim_output=10))\n",
    "    model.add_loss(cross_entropy_loss)\n",
    "    return model\n",
    "    \n",
    "model = create_model(50)\n",
    "\n",
    "scores = model.forward(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.302966\n",
      "sanity check loss: 2.302585\n",
      "dScores shape:  (500, 10)\n",
      "softmax shape:  (500, 10)\n"
     ]
    }
   ],
   "source": [
    "loss, dScores, softmax = model.calculate_loss(scores, y_dev, 0.5)\n",
    "\n",
    "# La loss d'un modèle non-entrainé devrait s'approcher de -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check loss: %f' % (-np.log(0.1)))\n",
    "\n",
    "print('dScores shape: ', dScores.shape)\n",
    "print('softmax shape: ', softmax.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 3000: loss 2.302966\n",
      "iteration 500 / 3000: loss 2.182986\n",
      "iteration 1000 / 3000: loss 1.985094\n"
     ]
    }
   ],
   "source": [
    "optimizer = SGD(8e-5, model)\n",
    "\n",
    "loss_history, train_accuracy, val_accuracy = solver(X_train, y_train, X_val, y_val, 0.5, optimizer, lr_decay=0.98, num_iter=3000)\n",
    "\n",
    "print('Final training loss: ', loss_history[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5>Visualisation</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualization.utils import visualize_loss\n",
    "\n",
    "# Visualisation de l'historique de loss de l'entraînement\n",
    "visualize_loss(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualization.utils import visualize_accuracy\n",
    "\n",
    "# Visualisation de l'historique d'accuracy lors de l'entraînement.\n",
    "# Ceci inclut la accuracy d'entraînement et la accuracy de validation\n",
    "visualize_accuracy(train_accuracy, val_accuracy)\n",
    "\n",
    "# Si la accuracy de validation est proche de 0.4, c'est bon signe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualization.utils import visualize_as_grid\n",
    "\n",
    "def show_net_weights(model):\n",
    "  W1 = model.parameters()['L0']['W']\n",
    "  W1 = W1.reshape(32, 32, 3, -1).transpose(3, 0, 1, 2)\n",
    "  plt.imshow(visualize_as_grid(W1, padding=3).astype('uint8'))\n",
    "  plt.gca().axis('off')\n",
    "  plt.show()\n",
    "\n",
    "show_net_weights(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recherche d'hyper-paramètres\n",
    "\n",
    "Tout comme dans le notebook précédent, veuillez rédiger du code afin de trouver les meilleurs taux d'apprentissage et terme de régularisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = None \n",
    "best_val_acc = 0.0\n",
    "best_loss = None\n",
    "best_train_acc = 0.0\n",
    "best_params = None\n",
    "best_model = None\n",
    "\n",
    "from itertools import product as itprod\n",
    "\n",
    "lr_list = 10 ** np.linspace(-4, -3, 4)\n",
    "reg_list = 10 ** np.linspace(-7, -3, 4)\n",
    "hs = 200\n",
    "nb_iter = 1000\n",
    "\n",
    "# Mettre les résultats des meilleurs hyper-paramètres dans les variables *best_ABC* définis ci-haut.\n",
    "# Avec 1000 iterations par entrainement, vous devriez obtenir une justesse de validation de 39%.\n",
    "# TODO\n",
    "# Mettre code ici.\n",
    "for lr in lr_list:\n",
    "    for reg in reg_list:\n",
    "        temp_loss, temp_train_acc, temp_val_acc = solver(X_train, y_train, X_val, y_val, reg, optimizer, lr, nb_iter, hs)\n",
    "        if np.mean(temp_val_acc) > np.mean(best_val_acc):\n",
    "            best_model = optimizer.model\n",
    "            best_params = np.array([lr, reg])\n",
    "            best_loss = temp_loss\n",
    "            best_train_acc = temp_train_acc\n",
    "            best_val_acc = temp_val_acc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best: {}'.format(best_params))\n",
    "# Visualisation de l'historique de perte lors de l'entraînement\n",
    "visualize_loss(best_loss)\n",
    "\n",
    "# Visualisation de l'historique d'accuracy lors de l'entraînement.\n",
    "# Ceci inclut la accuracy d'entraînement et la accuracy de validation\n",
    "visualize_accuracy(best_train_acc, best_val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_net_weights(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si tout va bien, vous devriez obtenir un justesse sur l'ensemble de test de 45% ou plus.\n",
    "test_acc = (best_model.predict(X_test) == y_test).mean()\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
