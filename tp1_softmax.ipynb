{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP1: Classifieur linéaire, fonction de perte **Entropie croisée**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from utils.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# pour automatiquement recharger les modules externes\n",
    "# voir http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10 Chargement des données et prétraitement\n",
    "\n",
    "### **TODO** assurez-vous d'exécuter le script *./get_datasets.sh* au moins une fois dans un terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500, num_batch=200):\n",
    "    \"\"\"\n",
    "    Charger la banque de données CIFAR-10, prétraiter les images et ajouter une dimension pour le biais.\n",
    "    \n",
    "    Input :\n",
    "    - num_training : nombre d'images à mettre dans l'ensemble d'entrainement\n",
    "    - num_validation : nombre d'images à mettre dans l'ensemble de validation\n",
    "    - num_test : nombre d'images à mettre dans l'ensemble de test\n",
    "    - num_dev : d'images à mettre dans l'ensemble dev\n",
    "    \n",
    "    Output :\n",
    "    - X_train, y_train : données et cibles d'entrainement\n",
    "    - X_val, y_val: données et cibles de validation\n",
    "    - X_test y_test: données et cibles de test \n",
    "    - X_dev, y_dev: données et cibles dev\n",
    "    - X_batch, y_batch: batch de données et de cibles \n",
    "    \"\"\"\n",
    "    # Charger les données CIFAR-10\n",
    "    cifar10_dir = 'datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "  \n",
    "    # Séparer en ensembles d'entraînement, de validation, de test et de dev\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    mask = range(num_batch)\n",
    "    X_batch = X_train[mask]\n",
    "    y_batch = y_train[mask]\n",
    "    \n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    X_batch = np.reshape(X_batch, (X_batch.shape[0], -1))\n",
    "\n",
    "    # Normalisation\n",
    "    X_train -= np.mean(X_train, axis = 0)\n",
    "    X_val -= np.mean(X_val, axis = 0)\n",
    "    X_test -= np.mean(X_test, axis = 0)\n",
    "    X_dev -= np.mean(X_dev, axis = 0)\n",
    "    X_batch -= np.mean(X_batch, axis = 0)\n",
    "\n",
    "    # Ajout du biais\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    X_batch = np.hstack([X_batch, np.ones((X_batch.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev, X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n",
      "batch data shape:  (200, 3073)\n",
      "batch labels shape:  (200,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev, X_batch, y_batch = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)\n",
    "print('batch data shape: ', X_batch.shape)\n",
    "print('batch labels shape: ', y_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "Les prochaines étapes consistent à implanter le calcul de **l'entropie croisée** et de son **gradient**.   Vous commencerez avec une version naïve impliquant une boucle *for* sur l'ensemble des éléments d'une batch pour ensuite implanter une version vectorisée.   Mais avant de commencer à coder, veuillez donner ici la formule de l'entropie croisée et du gradient pour une mini-batch de 500 données contenue dans le tableau\n",
    "\n",
    "$$X \\in R^{500\\times 3073}$$\n",
    "\n",
    "et une matrice de poids $$W \\in R^{3073\\times 10}$$ \n",
    "\n",
    "où 3073 est la dimensionnalité des données et 10 est le nombre de classes.\n",
    "\n",
    "**Votre Réponse:** \n",
    "\n",
    "$$Loss = -ln(S) + lamb*norm(W,2) avec S = exp(X.W) / sum(exp(X.W), 1)$$\n",
    "\n",
    "$$dW = [(S - t)*X].T + 2*lamb*W$$\n",
    "\n",
    "**NOTE IMPORTANT** : la réponse à cette question ne contient aucune boucle, seulement des multiplications matricielles et vectorielles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifieur linéaire SOFTMAX\n",
    "\n",
    "Le code pour cette section est dans le fichier **utils/loss.py**. \n",
    "\n",
    "La fonction `softmax_ce_naive_forward_backward` estime la perte (et le gradient) à l'aide de boucles `for` qui itèrent sur chaque donnée de la mini-batch.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commençons par tester la **forward pass + l'entropie croisée**.  Pour l'instant, ignorons la rétro-propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape :  (1, 3073)\n",
      "W shape :  (3073, 10)\n",
      "y shape :  (1,)\n",
      "Bravo!\n",
      "loss error: 0.000000\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# TODO: Implémenter le calcul de perte de façon naive avec des boucles dans  #\n",
    "#  la fonction softmax_ce_naive_forward_backward située dans le fichier      #\n",
    "#  utils.loss.                                                               #\n",
    "#  On commence par UNE image et UNE cible                                    #\n",
    "##############################################################################\n",
    "\n",
    "from utils.loss import softmax_ce_naive_forward_backward\n",
    "import time\n",
    "\n",
    "# Matrice de poids aléatoires + 1 donnée à tester\n",
    "np.random.seed(0)\n",
    "W = np.random.randn(3073, 10) * 5e-4\n",
    "X_rnd = np.random.randn(1, 3073) * 5\n",
    "y_rnd = np.uint32(np.ones(1))\n",
    "loss, _ = softmax_ce_naive_forward_backward(X_rnd, W, y_rnd, 0.0)\n",
    "\n",
    "# La loss suivante est celle que vous devriez obtenir\n",
    "target_loss = 2.276854\n",
    "loss_error = np.abs(loss - target_loss)\n",
    "if loss_error < 1e-5:\n",
    "    print(\"Bravo!\")\n",
    "else:\n",
    "    print(\"Il y a un bug...\")\n",
    "print('loss error: %f' % loss_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape :  (200, 3073)\n",
      "W shape :  (3073, 10)\n",
      "y shape :  (200,)\n",
      "Bravo!\n",
      "loss error: 0.000000\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# TODO: Implémenter le calcul de perte de façon naive avec des boucles dans  #\n",
    "#  la fonction softmax_ce_naive_forward_backward située dans le fichier      #\n",
    "#  utils.loss.                                                               #\n",
    "#  Maintenant on test avec N=200 images et autant de cibles                  #\n",
    "##############################################################################\n",
    "\n",
    "from utils.loss import softmax_ce_naive_forward_backward\n",
    "import time\n",
    "\n",
    "# Matrice de poids aléatoires + 500 données \n",
    "np.random.seed(0)\n",
    "W = np.random.randn(3073, 10) * 1e-4\n",
    "loss, _ = softmax_ce_naive_forward_backward(X_batch, W, y_batch, 0.0)\n",
    "\n",
    "target_loss = 2.356459\n",
    "loss_error = np.abs(loss - target_loss)\n",
    "if loss_error < 1e-5:\n",
    "    print(\"Bravo!\")\n",
    "else:\n",
    "    print(\"Il y a un bug...\")\n",
    "print('loss error: %f' % loss_error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2:\n",
    "Pourquoi s'attend-on que la loss soit approximativement -np.log(1/nb_classes))?\n",
    "\n",
    "**Votre réponse:** Lorsque l'on initialise aléatoirement les poids, nous savons que l'entropie sera maximale. Pour cela, la prédiction (correspondant ici à une probabilité) associée à chaque classe sera de 1/nb_classes. Vu différemment, nous pouvons voir que, si le modèle n'est pas entraîné, il va prédire de façon égale chaque classe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape :  (200, 3073)\n",
      "W shape :  (3073, 10)\n",
      "y shape :  (200,)\n",
      "Softmax loss: 2.339132\n",
      "Sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "###################################################################################\n",
    "#  Vérification simple: s'assurer que l'entropie-croisée soit proche de           #\n",
    "#  -log(1/nb_classes)                                                             #\n",
    "###################################################################################\n",
    "\n",
    "from utils.loss import softmax_ce_naive_forward_backward\n",
    "import time\n",
    "\n",
    "# Matrice de poids aléatoires + 500 données \n",
    "W = np.random.randn(3073, 10) * 1e-4\n",
    "loss, _ = softmax_ce_naive_forward_backward(X_batch, W, y_batch, 0.0)\n",
    "\n",
    "# La loss d'un modèle non-entrainé devrait s'approcher de -log(0.1).\n",
    "print('Softmax loss: %f' % loss)\n",
    "print('Sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rétro-propagation\n",
    "\n",
    "Maintenant, passons à la **rétro-propagation**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape :  (1, 3073)\n",
      "W shape :  (3073, 10)\n",
      "y shape :  (1,)\n",
      "Bravo pour la loss!\n",
      "loss error: 0.000000\n",
      "Bravo pour le gradient!\n",
      "gradient error 0.000000\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# TODO: Implémenter le calcul de perte ET DE LA RÉTROPROPAGATION de façon    #\n",
    "#       naive avec des boucles dans la fonction                              #\n",
    "#       softmax_ce_naive_forward_backward située dans le fichier utils.loss  #\n",
    "#                                                                            #\n",
    "#  On commence par UNE image et UNE cible                                    #\n",
    "##############################################################################\n",
    "\n",
    "from utils.loss import softmax_ce_naive_forward_backward\n",
    "import time\n",
    "\n",
    "# Matrice de poids aléatoires + une donnée\n",
    "np.random.seed(0)\n",
    "W = np.random.randn(3073, 10) * 1e-4\n",
    "X_rnd = np.random.randn(1, 3073)\n",
    "y_rnd = np.uint32(np.ones(1))\n",
    "loss, dW = softmax_ce_naive_forward_backward(X_rnd, W, y_rnd, 0.0)\n",
    "\n",
    "# La loss suivante est celle que vous devriez obtenir\n",
    "target_loss = 2.30114875\n",
    "loss_error = np.abs(loss - target_loss)\n",
    "if loss_error < 1e-5:\n",
    "    print(\"Bravo pour la loss!\")\n",
    "else:\n",
    "    print(\"Il y a un bug au niveau de la loss...\")\n",
    "print('loss error: %f' % loss_error)\n",
    "\n",
    "# Le gradient suivant est celui que vous devriez obtenir pour les 3 premiers poids\n",
    "target_dW = np.array([-0.1279744 ,  1.15786877, -0.12847105])\n",
    "dW_error = np.mean(np.abs(dW[0,0:3]-target_dW))\n",
    "if dW_error < 1e-7:\n",
    "    print(\"Bravo pour le gradient!\")\n",
    "else:\n",
    "    print(\"Il y a un bug au niveau du gradient...\")\n",
    "print('gradient error %f' % dW_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape :  (200, 3073)\n",
      "W shape :  (3073, 10)\n",
      "y shape :  (200,)\n",
      "[-1.68817739 -1.37204569 -0.46158773  1.9206649 ]\n",
      "Bravo pour la loss!\n",
      "loss error: 0.000000\n",
      "Bravo pour le gradient!\n",
      "gradient error 0.000000\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# TODO: Implémenter le calcul de perte ET DE LA RÉTROPROPAGATION de façon    #\n",
    "#       naive avec des boucles dans la fonction                              #\n",
    "#       softmax_ce_naive_forward_backward située dans le fichier utils.loss  #\n",
    "#                                                                            #\n",
    "#  Maintenant on test avec N=200 images et autant de cibles                  #\n",
    "##############################################################################\n",
    "\n",
    "from utils.loss import softmax_ce_naive_forward_backward\n",
    "import time\n",
    "\n",
    "# Matrice de poids aléatoires + 500 données\n",
    "np.random.seed(1)\n",
    "W = np.random.randn(3073, 10) * 1e-4\n",
    "loss, dW = softmax_ce_naive_forward_backward(X_batch, W, y_batch, 0.0)\n",
    "print(dW[0,0:4])\n",
    "# La loss suivante est celle que vous devriez obtenir\n",
    "target_loss = 2.35680883\n",
    "loss_error = np.abs(loss - target_loss)\n",
    "if loss_error < 1e-5:\n",
    "    print(\"Bravo pour la loss!\")\n",
    "else:\n",
    "    print(\"Il y a un bug au niveau de la loss...\")\n",
    "print('loss error: %f' % loss_error)\n",
    "\n",
    "# Le gradient suivant est celui que vous devriez obtenir pour les 3 premiers poids\n",
    "target_dW = np.array([-1.68817739, -1.37204569, -0.46158773, 1.9206649])\n",
    "dW_error = np.mean(np.abs(dW[0,0:4]-target_dW))\n",
    "if dW_error < 1e-7:\n",
    "    print(\"Bravo pour le gradient!\")\n",
    "else:\n",
    "    print(\"Il y a un bug au niveau du gradient...\")\n",
    "print('gradient error %f' % dW_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encore quelques vérifications d'usage\n",
    "En principe, à ce point-ci, le calcul de l'entropie croisée (et de son gradient) via la fonction *softmax_ce_naive_forward_backward* devrait fonctionner.  Mais avant de passer à la prochaine étape il nous reste deux vérifications à faire : s'assurer que la **régularisation** fonctionne et passer le teste du **gradient numérique**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape :  (500, 3073)\n",
      "W shape :  (3073, 10)\n",
      "y shape :  (500,)\n",
      "X shape :  (500, 3073)\n",
      "W shape :  (3073, 10)\n",
      "y shape :  (500,)\n",
      "2.390226986720188\n",
      "X shape :  (500, 3073)\n",
      "W shape :  (3073, 10)\n",
      "y shape :  (500,)\n",
      "2.3932949915723016\n",
      "X shape :  (500, 3073)\n",
      "W shape :  (3073, 10)\n",
      "y shape :  (500,)\n",
      "2.3969765973948385\n",
      "X shape :  (500, 3073)\n",
      "W shape :  (3073, 10)\n",
      "y shape :  (500,)\n",
      "2.401394524381882\n",
      "X shape :  (500, 3073)\n",
      "W shape :  (3073, 10)\n",
      "y shape :  (500,)\n",
      "2.406696036766335\n",
      "X shape :  (500, 3073)\n",
      "W shape :  (3073, 10)\n",
      "y shape :  (500,)\n",
      "2.4130578516276784\n",
      "X shape :  (500, 3073)\n",
      "W shape :  (3073, 10)\n",
      "y shape :  (500,)\n",
      "2.42069202946129\n",
      "X shape :  (500, 3073)\n",
      "W shape :  (3073, 10)\n",
      "y shape :  (500,)\n",
      "2.4298530428616245\n",
      "X shape :  (500, 3073)\n",
      "W shape :  (3073, 10)\n",
      "y shape :  (500,)\n",
      "2.4408462589420257\n",
      "X shape :  (500, 3073)\n",
      "W shape :  (3073, 10)\n",
      "y shape :  (500,)\n",
      "2.454038118238507\n",
      "Bravo!\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# Vérifions que le fait d'augmenter le terme de régularisation L2            #\n",
    "# augmente la loss...                                                        #\n",
    "##############################################################################\n",
    "success = True\n",
    "\n",
    "# Matrice de poids aléatoires + 500 données\n",
    "np.random.seed(1)\n",
    "W = np.random.randn(3073, 10) * 1e-4\n",
    "prev_loss, _ = softmax_ce_naive_forward_backward(X_dev, W, y_dev, 0.0)\n",
    "\n",
    "reg = 1e2\n",
    "for i in range(10):\n",
    "    loss, _ = softmax_ce_naive_forward_backward(X_dev, W, y_dev, reg)\n",
    "    print(loss)\n",
    "    if loss <= prev_loss:\n",
    "        success = False\n",
    "    prev_loss = loss\n",
    "    reg *= 1.2\n",
    "    \n",
    "if success:\n",
    "    print(\"Bravo!\")\n",
    "else:\n",
    "    print('Erreur!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------\n",
      "Gradient check : reg=0\n",
      "------------\n",
      "\n",
      "X shape :  (500, 3073)\n",
      "W shape :  (3073, 10)\n",
      "y shape :  (500,)\n",
      "X shape :  (500, 3073)\n",
      "W shape :  (3073, 10)\n",
      "y shape :  (500,)\n",
      "X shape :  (500, 3073)\n",
      "W shape :  (3073, 10)\n",
      "y shape :  (500,)\n",
      "numerical: 1.670184, analytic 1.670184, relative error: 3.246125e-10\n",
      "X shape :  (500, 3073)\n",
      "W shape :  (3073, 10)\n",
      "y shape :  (500,)\n",
      "X shape :  (500, 3073)\n",
      "W shape :  (3073, 10)\n",
      "y shape :  (500,)\n",
      "numerical: -0.744577, analytic -0.744577, relative error: 8.045512e-10\n",
      "X shape :  (500, 3073)\n",
      "W shape :  (3073, 10)\n",
      "y shape :  (500,)\n",
      "X shape :  (500, 3073)\n",
      "W shape :  (3073, 10)\n",
      "y shape :  (500,)\n",
      "numerical: -1.504309, analytic -1.504309, relative error: 2.662698e-10\n",
      "X shape :  (500, 3073)\n",
      "W shape :  (3073, 10)\n",
      "y shape :  (500,)\n",
      "X shape :  (500, 3073)\n",
      "W shape :  (3073, 10)\n",
      "y shape :  (500,)\n",
      "numerical: -0.382455, analytic -0.382455, relative error: 5.828874e-10\n",
      "X shape :  (500, 3073)\n",
      "W shape :  (3073, 10)\n",
      "y shape :  (500,)\n",
      "X shape :  (500, 3073)\n",
      "W shape :  (3073, 10)\n",
      "y shape :  (500,)\n",
      "numerical: -0.529962, analytic -0.529962, relative error: 3.103800e-09\n",
      "X shape :  (500, 3073)\n",
      "W shape :  (3073, 10)\n",
      "y shape :  (500,)\n",
      "X shape :  (500, 3073)\n",
      "W shape :  (3073, 10)\n",
      "y shape :  (500,)\n",
      "numerical: 1.235142, analytic 1.235142, relative error: 3.797341e-10\n",
      "X shape :  (500, 3073)\n",
      "W shape :  (3073, 10)\n",
      "y shape :  (500,)\n",
      "X shape :  (500, 3073)\n",
      "W shape :  (3073, 10)\n",
      "y shape :  (500,)\n",
      "numerical: -0.733010, analytic -0.733010, relative error: 2.625478e-10\n",
      "X shape :  (500, 3073)\n",
      "W shape :  (3073, 10)\n",
      "y shape :  (500,)\n",
      "X shape :  (500, 3073)\n",
      "W shape :  (3073, 10)\n",
      "y shape :  (500,)\n",
      "numerical: -0.957061, analytic -0.957061, relative error: 4.238801e-10\n",
      "X shape :  (500, 3073)\n",
      "W shape :  (3073, 10)\n",
      "y shape :  (500,)\n",
      "X shape :  (500, 3073)\n",
      "W shape :  (3073, 10)\n",
      "y shape :  (500,)\n",
      "numerical: 1.165313, analytic 1.165313, relative error: 4.982490e-10\n",
      "X shape :  (500, 3073)\n",
      "W shape :  (3073, 10)\n",
      "y shape :  (500,)\n",
      "X shape :  (500, 3073)\n",
      "W shape :  (3073, 10)\n",
      "y shape :  (500,)\n",
      "numerical: 0.053848, analytic 0.053848, relative error: 8.823036e-09\n",
      "\n",
      "------------\n",
      "Gradient check : reg=1e-2\n",
      "------------\n",
      "\n",
      "X shape :  (500, 3073)\n",
      "W shape :  (3073, 10)\n",
      "y shape :  (500,)\n",
      "X shape :  (500, 3073)\n",
      "W shape :  (3073, 10)\n",
      "y shape :  (500,)\n",
      "X shape :  (500, 3073)\n",
      "W shape :  (3073, 10)\n",
      "y shape :  (500,)\n",
      "numerical: 0.057117, analytic 0.031761, relative error: 2.852980e-01\n",
      "X shape :  (500, 3073)\n",
      "W shape :  (3073, 10)\n",
      "y shape :  (500,)\n",
      "X shape :  (500, 3073)\n",
      "W shape :  (3073, 10)\n",
      "y shape :  (500,)\n",
      "numerical: 1.088092, analytic 1.077112, relative error: 5.071078e-03\n",
      "X shape :  (500, 3073)\n",
      "W shape :  (3073, 10)\n",
      "y shape :  (500,)\n",
      "X shape :  (500, 3073)\n",
      "W shape :  (3073, 10)\n",
      "y shape :  (500,)\n",
      "numerical: 1.538187, analytic 1.526110, relative error: 3.941158e-03\n",
      "X shape :  (500, 3073)\n",
      "W shape :  (3073, 10)\n",
      "y shape :  (500,)\n",
      "X shape :  (500, 3073)\n",
      "W shape :  (3073, 10)\n",
      "y shape :  (500,)\n",
      "numerical: -0.967676, analytic -0.980484, relative error: 6.574056e-03\n",
      "X shape :  (500, 3073)\n",
      "W shape :  (3073, 10)\n",
      "y shape :  (500,)\n",
      "X shape :  (500, 3073)\n",
      "W shape :  (3073, 10)\n",
      "y shape :  (500,)\n",
      "numerical: -3.410474, analytic -3.402896, relative error: 1.112250e-03\n",
      "X shape :  (500, 3073)\n",
      "W shape :  (3073, 10)\n",
      "y shape :  (500,)\n",
      "X shape :  (500, 3073)\n",
      "W shape :  (3073, 10)\n",
      "y shape :  (500,)\n",
      "numerical: -1.791778, analytic -1.791759, relative error: 5.285619e-06\n",
      "X shape :  (500, 3073)\n",
      "W shape :  (3073, 10)\n",
      "y shape :  (500,)\n",
      "X shape :  (500, 3073)\n",
      "W shape :  (3073, 10)\n",
      "y shape :  (500,)\n",
      "numerical: -1.574473, analytic -1.583055, relative error: 2.717773e-03\n",
      "X shape :  (500, 3073)\n",
      "W shape :  (3073, 10)\n",
      "y shape :  (500,)\n",
      "X shape :  (500, 3073)\n",
      "W shape :  (3073, 10)\n",
      "y shape :  (500,)\n",
      "numerical: -0.289474, analytic -0.288608, relative error: 1.499417e-03\n",
      "X shape :  (500, 3073)\n",
      "W shape :  (3073, 10)\n",
      "y shape :  (500,)\n",
      "X shape :  (500, 3073)\n",
      "W shape :  (3073, 10)\n",
      "y shape :  (500,)\n",
      "numerical: 3.308431, analytic 3.302049, relative error: 9.654467e-04\n",
      "X shape :  (500, 3073)\n",
      "W shape :  (3073, 10)\n",
      "y shape :  (500,)\n",
      "X shape :  (500, 3073)\n",
      "W shape :  (3073, 10)\n",
      "y shape :  (500,)\n",
      "numerical: -0.830102, analytic -0.825846, relative error: 2.570264e-03\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# Maintenant testons le gradient numérique avec et sans régularisation       #\n",
    "# Les erreurs relatives devraient être inférieures à 1e-6                    #\n",
    "##############################################################################\n",
    "from utils.gradients import check_gradient_sparse\n",
    "\n",
    "print(\"\\n------------\\nGradient check : reg=0\\n------------\\n\")\n",
    "check_gradient_sparse(softmax_ce_naive_forward_backward, W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# Faire un autre test de gradients avec régularisation \n",
    "print(\"\\n------------\\nGradient check : reg=1e-2\\n------------\\n\")\n",
    "check_gradient_sparse(softmax_ce_naive_forward_backward, W, X_dev, y_dev, 1e2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax et gradients vectorisés\n",
    "Passons maintenant aux choses sérieuses. Vous devez ici coder la version vectorisée de l'entropie croisée et du gradient dans la fonction **softmax_ce_forward_backward**.  Ce code s'apparente à la réponse que vous avec donné au début."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.60997977e+00 -1.17115354e+00  2.26371679e-01 ... -1.14072155e+00\n",
      "   7.10088049e-01 -3.09022721e+00]\n",
      " [-4.79385086e+00 -7.83289319e-01  8.22630915e-01 ... -9.27728499e-01\n",
      "  -3.16351721e-01 -3.87611380e+00]\n",
      " [-6.97257427e+00 -1.11346100e+00  2.28286472e+00 ... -7.41403554e-01\n",
      "  -2.20267488e+00 -5.67867015e+00]\n",
      " ...\n",
      " [-1.37979731e+00 -1.45167853e+00  2.80325727e-01 ... -5.84295254e-01\n",
      "   1.14814880e+00 -1.51468560e+00]\n",
      " [-2.78520330e+00 -2.98929526e+00  1.23506886e+00 ...  1.13419395e+00\n",
      "  -5.27930255e-01 -2.03534592e+00]\n",
      " [ 1.35970615e-03  1.16474969e-02  8.71717193e-03 ... -1.40093310e-02\n",
      "   1.54344915e-02  5.37451709e-03]]\n",
      "naive loss: 2.374887e+00 computed in 0.171999s\n",
      "[[-3.60997977e+00 -1.17115354e+00  2.26371680e-01 ... -1.14072155e+00\n",
      "   7.10088048e-01 -3.09022721e+00]\n",
      " [-4.79385086e+00 -7.83289315e-01  8.22630916e-01 ... -9.27728497e-01\n",
      "  -3.16351721e-01 -3.87611381e+00]\n",
      " [-6.97257427e+00 -1.11346100e+00  2.28286472e+00 ... -7.41403552e-01\n",
      "  -2.20267488e+00 -5.67867016e+00]\n",
      " ...\n",
      " [-1.37979731e+00 -1.45167854e+00  2.80325730e-01 ... -5.84295255e-01\n",
      "   1.14814880e+00 -1.51468561e+00]\n",
      " [-2.78520330e+00 -2.98929526e+00  1.23506886e+00 ...  1.13419395e+00\n",
      "  -5.27930254e-01 -2.03534593e+00]\n",
      " [ 1.35970506e-03  1.16474940e-02  8.71717344e-03 ... -1.40093299e-02\n",
      "   1.54344902e-02  5.37451783e-03]]\n",
      "vectorized loss: 2.374887e+00 computed in 0.005006s\n",
      "bravo pour la loss!\n",
      "Loss difference: 0.000000\n",
      "il y a un bug au niveau du gradient\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# TODO: Implémenter le calcul de perte et du gradient de façon vectorielle   #\n",
    "# dans la fonction softmax_ce_forward_backward située dans le fichier        #\n",
    "# utils.loss.                                                                #\n",
    "# Les deux versions devraient calculer les mêmes résultats, mais la version  #\n",
    "# vectorielle devrait être BEAUCOUP PLUS RAPIDE.                             #\n",
    "##############################################################################\n",
    "start = time.time()\n",
    "loss_naive, grad_naive = softmax_ce_naive_forward_backward(X_dev, W, y_dev, 0.00001)\n",
    "end = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, end - start))\n",
    "\n",
    "from utils.loss import softmax_ce_forward_backward\n",
    "start = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_ce_forward_backward(X_dev, W, y_dev, 0.00001)\n",
    "end = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, end - start))\n",
    "\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "loss_diff = np.abs(loss_naive - loss_vectorized)\n",
    "if loss_diff < 1e-7:\n",
    "    print('bravo pour la loss!')\n",
    "else:\n",
    "    print('il y a un bug au niveau de la loss')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "\n",
    "if grad_difference < 1e-7:\n",
    "    print('bravo pour le gradient !')\n",
    "else:\n",
    "    print('il y a un bug au niveau du gradient')\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraînement\n",
    "À l'aide de la classe **LinearClassifier** ainsi que de la fonction vectorisée **softmax_ce_forward_backward** que vous venez de coder, vous devez maintenant entraîner un réseau de neurones multiclasses linéaire à l'aide d'une **descente de gradient**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train vs val acc 0.101531 / 0.101000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from model.LinearClassifier import LinearClassifier\n",
    "from visualization.utils import visualize_loss\n",
    "import itertools as it\n",
    "lr = 1e-7\n",
    "reg = 1\n",
    "\n",
    "classifier = LinearClassifier(softmax_ce_forward_backward)\n",
    "#\n",
    "# TODO : ajouter code à la fonction train.  Si tout fonctionne bien, la courbe de la loss devrait décroitre\n",
    "#\n",
    "train_loss_history = classifier.train(X_train, y_train, learning_rate=lr, reg=reg, num_iter=3000, verbose = True)\n",
    "\n",
    "y_train_pred = classifier.predict(X_train)\n",
    "y_val_pred = classifier.predict(X_val)\n",
    "\n",
    "acc_train = np.mean(y_train == y_train_pred)\n",
    "acc_val = np.mean(y_val == y_val_pred)\n",
    "\n",
    "print('train vs val acc %f / %f' %(acc_train, acc_val))\n",
    "\n",
    "visualize_loss(train_loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recherche d'hyper-paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.LinearClassifier import LinearClassifier\n",
    "from visualization.utils import visualize_loss\n",
    "import itertools as it\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = np.linspace(1e-7, 1e-5, 5)\n",
    "regularization_strengths = np.linspace(1e3, 1e7, 5)\n",
    "best_loss_history = None\n",
    "best_classifier = None\n",
    "\n",
    "##############################################################################\n",
    "# TODO: Utilisez l'ensemble de validation pour régler les hyper-paramètres   #\n",
    "#  (force de régularisation et vitesse d'apprentissage). Vous devez          #\n",
    "#  expérimenter différentes plages de valeurs pour les taux d'apprentissage  #\n",
    "#  et les forces de régularisation; si tout va bien, avec num_iter = 1000    #\n",
    "#  vous devriez obtenir une précision de classification supérieur à 0.38 sur #\n",
    "#  l'ensemble de validation, et de 0.37 sur l'ensemble de test.              #\n",
    "#  Mettre les résultats des meilleurs hyper-paramètres dans les variables    #\n",
    "#  best_XYZ ci haut.                                                         #\n",
    "##############################################################################\n",
    "\n",
    "\n",
    "################################################################################\n",
    "#                             FIN DE VOTRE CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)\n",
    "\n",
    "visualize_loss(best_loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On évalue le modèle sur l'ensemble de test\n",
    "y_test_pred = best_classifier.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('Test set accuracy: %f' % (test_accuracy) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des poids appris pour chaque classe\n",
    "w = best_classifier.W[:-1,:] # retire le biais\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "\n",
    "    # Redimensionne les poids pour qu'ils soient entre 0 et 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
