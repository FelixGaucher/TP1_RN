{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP1: Classifieur linéaire, fonction de perte **Entropie croisée**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from utils.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# pour automatiquement recharger les modules externes\n",
    "# voir http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10 Chargement des données et prétraitement\n",
    "\n",
    "### **TODO** assurez-vous d'exécuter le script *./get_datasets.sh* au moins une fois dans un terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500, num_batch=200):\n",
    "    \"\"\"\n",
    "    Charger la banque de données CIFAR-10, prétraiter les images et ajouter une dimension pour le biais.\n",
    "    \n",
    "    Input :\n",
    "    - num_training : nombre d'images à mettre dans l'ensemble d'entrainement\n",
    "    - num_validation : nombre d'images à mettre dans l'ensemble de validation\n",
    "    - num_test : nombre d'images à mettre dans l'ensemble de test\n",
    "    - num_dev : d'images à mettre dans l'ensemble dev\n",
    "    \n",
    "    Output :\n",
    "    - X_train, y_train : données et cibles d'entrainement\n",
    "    - X_val, y_val: données et cibles de validation\n",
    "    - X_test y_test: données et cibles de test \n",
    "    - X_dev, y_dev: données et cibles dev\n",
    "    - X_batch, y_batch: batch de données et de cibles \n",
    "    \"\"\"\n",
    "    # Charger les données CIFAR-10\n",
    "    cifar10_dir = 'datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "  \n",
    "    # Séparer en ensembles d'entraînement, de validation, de test et de dev\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    mask = range(num_batch)\n",
    "    X_batch = X_train[mask]\n",
    "    y_batch = y_train[mask]\n",
    "    \n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    X_batch = np.reshape(X_batch, (X_batch.shape[0], -1))\n",
    "\n",
    "    # Normalisation\n",
    "    X_train -= np.mean(X_train, axis = 0)\n",
    "    X_val -= np.mean(X_val, axis = 0)\n",
    "    X_test -= np.mean(X_test, axis = 0)\n",
    "    X_dev -= np.mean(X_dev, axis = 0)\n",
    "    X_batch -= np.mean(X_batch, axis = 0)\n",
    "\n",
    "    # Ajout du biais\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    X_batch = np.hstack([X_batch, np.ones((X_batch.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev, X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n",
      "batch data shape:  (200, 3073)\n",
      "batch labels shape:  (200,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev, X_batch, y_batch = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)\n",
    "print('batch data shape: ', X_batch.shape)\n",
    "print('batch labels shape: ', y_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "Les prochaines étapes consistent à implanter le calcul de **l'entropie croisée** et de son **gradient**.   Vous commencerez avec une version naïve impliquant une boucle *for* sur l'ensemble des éléments d'une batch pour ensuite implanter une version vectorisée.   Mais avant de commencer à coder, veuillez donner ici la formule de l'entropie croisée et du gradient pour une mini-batch de 500 données contenue dans le tableau\n",
    "\n",
    "$$X \\in R^{500\\times 3073}$$\n",
    "\n",
    "et une matrice de poids $$W \\in R^{3073\\times 10}$$ \n",
    "\n",
    "où 3073 est la dimensionnalité des données et 10 est le nombre de classes.\n",
    "\n",
    "**Votre Réponse:** \n",
    "\n",
    "$$Loss = -ln(S) + lamb*norm(W,2) avec S = exp(X.W) / sum(exp(X.W), 1)$$\n",
    "\n",
    "$$dW = [(S - t)*X].T + 2*lamb*W$$\n",
    "\n",
    "**NOTE IMPORTANT** : la réponse à cette question ne contient aucune boucle, seulement des multiplications matricielles et vectorielles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifieur linéaire SOFTMAX\n",
    "\n",
    "Le code pour cette section est dans le fichier **utils/loss.py**. \n",
    "\n",
    "La fonction `softmax_ce_naive_forward_backward` estime la perte (et le gradient) à l'aide de boucles `for` qui itèrent sur chaque donnée de la mini-batch.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commençons par tester la **forward pass + l'entropie croisée**.  Pour l'instant, ignorons la rétro-propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bravo!\n",
      "loss error: 0.000000\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# TODO: Implémenter le calcul de perte de façon naive avec des boucles dans  #\n",
    "#  la fonction softmax_ce_naive_forward_backward située dans le fichier      #\n",
    "#  utils.loss.                                                               #\n",
    "#  On commence par UNE image et UNE cible                                    #\n",
    "##############################################################################\n",
    "\n",
    "from utils.loss import softmax_ce_naive_forward_backward\n",
    "import time\n",
    "\n",
    "# Matrice de poids aléatoires + 1 donnée à tester\n",
    "np.random.seed(0)\n",
    "W = np.random.randn(3073, 10) * 5e-4\n",
    "X_rnd = np.random.randn(1, 3073) * 5\n",
    "y_rnd = np.uint32(np.ones(1))\n",
    "loss, _ = softmax_ce_naive_forward_backward(X_rnd, W, y_rnd, 0.0)\n",
    "\n",
    "# La loss suivante est celle que vous devriez obtenir\n",
    "target_loss = 2.276854\n",
    "loss_error = np.abs(loss - target_loss)\n",
    "if loss_error < 1e-5:\n",
    "    print(\"Bravo!\")\n",
    "else:\n",
    "    print(\"Il y a un bug...\")\n",
    "print('loss error: %f' % loss_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bravo!\n",
      "loss error: 0.000000\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# TODO: Implémenter le calcul de perte de façon naive avec des boucles dans  #\n",
    "#  la fonction softmax_ce_naive_forward_backward située dans le fichier      #\n",
    "#  utils.loss.                                                               #\n",
    "#  Maintenant on test avec N=200 images et autant de cibles                  #\n",
    "##############################################################################\n",
    "\n",
    "from utils.loss import softmax_ce_naive_forward_backward\n",
    "import time\n",
    "\n",
    "# Matrice de poids aléatoires + 500 données \n",
    "np.random.seed(0)\n",
    "W = np.random.randn(3073, 10) * 1e-4\n",
    "loss, _ = softmax_ce_naive_forward_backward(X_batch, W, y_batch, 0.0)\n",
    "\n",
    "target_loss = 2.356459\n",
    "loss_error = np.abs(loss - target_loss)\n",
    "if loss_error < 1e-5:\n",
    "    print(\"Bravo!\")\n",
    "else:\n",
    "    print(\"Il y a un bug...\")\n",
    "print('loss error: %f' % loss_error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2:\n",
    "Pourquoi s'attend-on que la loss soit approximativement -np.log(1/nb_classes))?\n",
    "\n",
    "**Votre réponse:** Lorsque l'on initialise aléatoirement les poids, nous savons que l'entropie sera maximale. Pour cela, la prédiction (correspondant ici à une probabilité) associée à chaque classe sera de 1/nb_classes. Vu différemment, nous pouvons voir que, si le modèle n'est pas entraîné, il va prédire de façon égale chaque classe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax loss: 2.339132\n",
      "Sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "###################################################################################\n",
    "#  Vérification simple: s'assurer que l'entropie-croisée soit proche de           #\n",
    "#  -log(1/nb_classes)                                                             #\n",
    "###################################################################################\n",
    "\n",
    "from utils.loss import softmax_ce_naive_forward_backward\n",
    "import time\n",
    "\n",
    "# Matrice de poids aléatoires + 500 données \n",
    "W = np.random.randn(3073, 10) * 1e-4\n",
    "loss, _ = softmax_ce_naive_forward_backward(X_batch, W, y_batch, 0.0)\n",
    "\n",
    "# La loss d'un modèle non-entrainé devrait s'approcher de -log(0.1).\n",
    "print('Softmax loss: %f' % loss)\n",
    "print('Sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rétro-propagation\n",
    "\n",
    "Maintenant, passons à la **rétro-propagation**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bravo pour la loss!\n",
      "loss error: 0.000000\n",
      "Bravo pour le gradient!\n",
      "gradient error 0.000000\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# TODO: Implémenter le calcul de perte ET DE LA RÉTROPROPAGATION de façon    #\n",
    "#       naive avec des boucles dans la fonction                              #\n",
    "#       softmax_ce_naive_forward_backward située dans le fichier utils.loss  #\n",
    "#                                                                            #\n",
    "#  On commence par UNE image et UNE cible                                    #\n",
    "##############################################################################\n",
    "\n",
    "from utils.loss import softmax_ce_naive_forward_backward\n",
    "import time\n",
    "\n",
    "# Matrice de poids aléatoires + une donnée\n",
    "np.random.seed(0)\n",
    "W = np.random.randn(3073, 10) * 1e-4\n",
    "X_rnd = np.random.randn(1, 3073)\n",
    "y_rnd = np.uint32(np.ones(1))\n",
    "loss, dW = softmax_ce_naive_forward_backward(X_rnd, W, y_rnd, 0.0)\n",
    "\n",
    "# La loss suivante est celle que vous devriez obtenir\n",
    "target_loss = 2.30114875\n",
    "loss_error = np.abs(loss - target_loss)\n",
    "if loss_error < 1e-5:\n",
    "    print(\"Bravo pour la loss!\")\n",
    "else:\n",
    "    print(\"Il y a un bug au niveau de la loss...\")\n",
    "print('loss error: %f' % loss_error)\n",
    "\n",
    "# Le gradient suivant est celui que vous devriez obtenir pour les 3 premiers poids\n",
    "target_dW = np.array([-0.1279744 ,  1.15786877, -0.12847105])\n",
    "dW_error = np.mean(np.abs(dW[0,0:3]-target_dW))\n",
    "if dW_error < 1e-7:\n",
    "    print(\"Bravo pour le gradient!\")\n",
    "else:\n",
    "    print(\"Il y a un bug au niveau du gradient...\")\n",
    "print('gradient error %f' % dW_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.68817739 -1.37204569 -0.46158773  1.9206649 ]\n",
      "Bravo pour la loss!\n",
      "loss error: 0.000000\n",
      "Bravo pour le gradient!\n",
      "gradient error 0.000000\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# TODO: Implémenter le calcul de perte ET DE LA RÉTROPROPAGATION de façon    #\n",
    "#       naive avec des boucles dans la fonction                              #\n",
    "#       softmax_ce_naive_forward_backward située dans le fichier utils.loss  #\n",
    "#                                                                            #\n",
    "#  Maintenant on test avec N=200 images et autant de cibles                  #\n",
    "##############################################################################\n",
    "\n",
    "from utils.loss import softmax_ce_naive_forward_backward\n",
    "import time\n",
    "\n",
    "# Matrice de poids aléatoires + 500 données\n",
    "np.random.seed(1)\n",
    "W = np.random.randn(3073, 10) * 1e-4\n",
    "loss, dW = softmax_ce_naive_forward_backward(X_batch, W, y_batch, 0.0)\n",
    "print(dW[0,0:4])\n",
    "# La loss suivante est celle que vous devriez obtenir\n",
    "target_loss = 2.35680883\n",
    "loss_error = np.abs(loss - target_loss)\n",
    "if loss_error < 1e-5:\n",
    "    print(\"Bravo pour la loss!\")\n",
    "else:\n",
    "    print(\"Il y a un bug au niveau de la loss...\")\n",
    "print('loss error: %f' % loss_error)\n",
    "\n",
    "# Le gradient suivant est celui que vous devriez obtenir pour les 3 premiers poids\n",
    "target_dW = np.array([-1.68817739, -1.37204569, -0.46158773, 1.9206649])\n",
    "dW_error = np.mean(np.abs(dW[0,0:4]-target_dW))\n",
    "if dW_error < 1e-7:\n",
    "    print(\"Bravo pour le gradient!\")\n",
    "else:\n",
    "    print(\"Il y a un bug au niveau du gradient...\")\n",
    "print('gradient error %f' % dW_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encore quelques vérifications d'usage\n",
    "En principe, à ce point-ci, le calcul de l'entropie croisée (et de son gradient) via la fonction *softmax_ce_naive_forward_backward* devrait fonctionner.  Mais avant de passer à la prochaine étape il nous reste deux vérifications à faire : s'assurer que la **régularisation** fonctionne et passer le teste du **gradient numérique**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4040047342513744\n",
      "2.407072739103488\n",
      "2.410754344926025\n",
      "2.4151722719130686\n",
      "2.4204737842975215\n",
      "2.426835599158865\n",
      "2.4344697769924766\n",
      "2.443630790392811\n",
      "2.454624006473212\n",
      "2.4678158657696936\n",
      "Bravo!\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# Vérifions que le fait d'augmenter le terme de régularisation L2            #\n",
    "# augmente la loss...                                                        #\n",
    "##############################################################################\n",
    "success = True\n",
    "\n",
    "# Matrice de poids aléatoires + 500 données\n",
    "np.random.seed(1)\n",
    "W = np.random.randn(3073, 10) * 1e-4\n",
    "prev_loss, _ = softmax_ce_naive_forward_backward(X_dev, W, y_dev, 0.0)\n",
    "\n",
    "reg = 1e2\n",
    "for i in range(10):\n",
    "    loss, _ = softmax_ce_naive_forward_backward(X_dev, W, y_dev, reg)\n",
    "    print(loss)\n",
    "    if loss <= prev_loss:\n",
    "        success = False\n",
    "    prev_loss = loss\n",
    "    reg *= 1.2\n",
    "    \n",
    "if success:\n",
    "    print(\"Bravo!\")\n",
    "else:\n",
    "    print('Erreur!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------\n",
      "Gradient check : reg=0\n",
      "------------\n",
      "\n",
      "numerical: 1.351209, analytic 1.351209, relative error: 1.050318e-09\n",
      "numerical: -0.445107, analytic -0.445107, relative error: 2.933860e-10\n",
      "numerical: -1.626132, analytic -1.626132, relative error: 6.236885e-11\n",
      "numerical: 0.733509, analytic 0.733509, relative error: 1.514229e-09\n",
      "numerical: -1.355055, analytic -1.355055, relative error: 1.890908e-10\n",
      "numerical: 0.400162, analytic 0.400162, relative error: 5.994439e-12\n",
      "numerical: -0.833858, analytic -0.833858, relative error: 2.034417e-10\n",
      "numerical: -1.760989, analytic -1.760989, relative error: 8.126933e-10\n",
      "numerical: 2.210965, analytic 2.210965, relative error: 2.686844e-10\n",
      "numerical: -1.271003, analytic -1.271003, relative error: 3.244208e-10\n",
      "\n",
      "------------\n",
      "Gradient check : reg=1e-2\n",
      "------------\n",
      "\n",
      "numerical: 0.667672, analytic 0.642315, relative error: 1.935649e-02\n",
      "numerical: 0.430970, analytic 0.419990, relative error: 1.290297e-02\n",
      "numerical: 3.338873, analytic 3.326796, relative error: 1.811803e-03\n",
      "numerical: -0.424901, analytic -0.437708, relative error: 1.484718e-02\n",
      "numerical: -2.558436, analytic -2.550857, relative error: 1.483213e-03\n",
      "numerical: -2.345508, analytic -2.345490, relative error: 4.037237e-06\n",
      "numerical: -2.997884, analytic -3.006465, relative error: 1.429204e-03\n",
      "numerical: -0.538963, analytic -0.538097, relative error: 8.047718e-04\n",
      "numerical: 3.105119, analytic 3.098737, relative error: 1.028726e-03\n",
      "numerical: 0.530261, analytic 0.534518, relative error: 3.997285e-03\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# Maintenant testons le gradient numérique avec et sans régularisation       #\n",
    "# Les erreurs relatives devraient être inférieures à 1e-6                    #\n",
    "##############################################################################\n",
    "from utils.gradients import check_gradient_sparse\n",
    "\n",
    "print(\"\\n------------\\nGradient check : reg=0\\n------------\\n\")\n",
    "check_gradient_sparse(softmax_ce_naive_forward_backward, W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# Faire un autre test de gradients avec régularisation \n",
    "print(\"\\n------------\\nGradient check : reg=1e-2\\n------------\\n\")\n",
    "check_gradient_sparse(softmax_ce_naive_forward_backward, W, X_dev, y_dev, 1e2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax et gradients vectorisés\n",
    "Passons maintenant aux choses sérieuses. Vous devez ici coder la version vectorisée de l'entropie croisée et du gradient dans la fonction **softmax_ce_forward_backward**.  Ce code s'apparente à la réponse que vous avec donné au début."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.388665e+00 computed in 0.189987s\n",
      "vectorized loss: 2.388665e+00 computed in 0.004001s\n",
      "bravo pour la loss!\n",
      "Loss difference: 0.000000\n",
      "il y a un bug au niveau du gradient\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# TODO: Implémenter le calcul de perte et du gradient de façon vectorielle   #\n",
    "# dans la fonction softmax_ce_forward_backward située dans le fichier        #\n",
    "# utils.loss.                                                                #\n",
    "# Les deux versions devraient calculer les mêmes résultats, mais la version  #\n",
    "# vectorielle devrait être BEAUCOUP PLUS RAPIDE.                             #\n",
    "##############################################################################\n",
    "start = time.time()\n",
    "loss_naive, grad_naive = softmax_ce_naive_forward_backward(X_dev, W, y_dev, 0.00001)\n",
    "end = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, end - start))\n",
    "\n",
    "from utils.loss import softmax_ce_forward_backward\n",
    "start = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_ce_forward_backward(X_dev, W, y_dev, 0.00001)\n",
    "end = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, end - start))\n",
    "\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "loss_diff = np.abs(loss_naive - loss_vectorized)\n",
    "if loss_diff < 1e-7:\n",
    "    print('bravo pour la loss!')\n",
    "else:\n",
    "    print('il y a un bug au niveau de la loss')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "\n",
    "if grad_difference < 1e-7:\n",
    "    print('bravo pour le gradient !')\n",
    "else:\n",
    "    print('il y a un bug au niveau du gradient')\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7480689642731704e-07"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraînement\n",
    "À l'aide de la classe **LinearClassifier** ainsi que de la fonction vectorisée **softmax_ce_forward_backward** que vous venez de coder, vous devez maintenant entraîner un réseau de neurones multiclasses linéaire à l'aide d'une **descente de gradient**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train vs val acc 0.101531 / 0.101000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from model.LinearClassifier import LinearClassifier\n",
    "from visualization.utils import visualize_loss\n",
    "import itertools as it\n",
    "lr = 1e-7\n",
    "reg = 1\n",
    "\n",
    "classifier = LinearClassifier(softmax_ce_forward_backward)\n",
    "#\n",
    "# TODO : ajouter code à la fonction train.  Si tout fonctionne bien, la courbe de la loss devrait décroitre\n",
    "#\n",
    "train_loss_history = classifier.train(X_train, y_train, learning_rate=lr, reg=reg, num_iter=3000, verbose = True)\n",
    "\n",
    "y_train_pred = classifier.predict(X_train)\n",
    "y_val_pred = classifier.predict(X_val)\n",
    "\n",
    "acc_train = np.mean(y_train == y_train_pred)\n",
    "acc_val = np.mean(y_val == y_val_pred)\n",
    "\n",
    "print('train vs val acc %f / %f' %(acc_train, acc_val))\n",
    "\n",
    "visualize_loss(train_loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recherche d'hyper-paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.LinearClassifier import LinearClassifier\n",
    "from visualization.utils import visualize_loss\n",
    "import itertools as it\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = np.linspace(1e-7, 1e-5, 5)\n",
    "regularization_strengths = np.linspace(1e3, 1e7, 5)\n",
    "best_loss_history = None\n",
    "best_classifier = None\n",
    "\n",
    "##############################################################################\n",
    "# TODO: Utilisez l'ensemble de validation pour régler les hyper-paramètres   #\n",
    "#  (force de régularisation et vitesse d'apprentissage). Vous devez          #\n",
    "#  expérimenter différentes plages de valeurs pour les taux d'apprentissage  #\n",
    "#  et les forces de régularisation; si tout va bien, avec num_iter = 1000    #\n",
    "#  vous devriez obtenir une précision de classification supérieur à 0.38 sur #\n",
    "#  l'ensemble de validation, et de 0.37 sur l'ensemble de test.              #\n",
    "#  Mettre les résultats des meilleurs hyper-paramètres dans les variables    #\n",
    "#  best_XYZ ci haut.                                                         #\n",
    "##############################################################################\n",
    "\n",
    "\n",
    "################################################################################\n",
    "#                             FIN DE VOTRE CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)\n",
    "\n",
    "visualize_loss(best_loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On évalue le modèle sur l'ensemble de test\n",
    "y_test_pred = best_classifier.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('Test set accuracy: %f' % (test_accuracy) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des poids appris pour chaque classe\n",
    "w = best_classifier.W[:-1,:] # retire le biais\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "\n",
    "    # Redimensionne les poids pour qu'ils soient entre 0 et 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
