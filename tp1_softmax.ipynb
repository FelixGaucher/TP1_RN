{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP1: Classifieur linéaire, fonction de perte **Entropie croisée**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from utils.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# pour automatiquement recharger les modules externes\n",
    "# voir http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10 Chargement des données et prétraitement\n",
    "\n",
    "### **TODO** assurez-vous d'exécuter le script *./get_datasets.sh* au moins une fois dans un terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500, num_batch=200):\n",
    "    \"\"\"\n",
    "    Charger la banque de données CIFAR-10, prétraiter les images et ajouter une dimension pour le biais.\n",
    "    \n",
    "    Input :\n",
    "    - num_training : nombre d'images à mettre dans l'ensemble d'entrainement\n",
    "    - num_validation : nombre d'images à mettre dans l'ensemble de validation\n",
    "    - num_test : nombre d'images à mettre dans l'ensemble de test\n",
    "    - num_dev : d'images à mettre dans l'ensemble dev\n",
    "    \n",
    "    Output :\n",
    "    - X_train, y_train : données et cibles d'entrainement\n",
    "    - X_val, y_val: données et cibles de validation\n",
    "    - X_test y_test: données et cibles de test \n",
    "    - X_dev, y_dev: données et cibles dev\n",
    "    - X_batch, y_batch: batch de données et de cibles \n",
    "    \"\"\"\n",
    "    # Charger les données CIFAR-10\n",
    "    cifar10_dir = 'datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "  \n",
    "    # Séparer en ensembles d'entraînement, de validation, de test et de dev\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    mask = range(num_batch)\n",
    "    X_batch = X_train[mask]\n",
    "    y_batch = y_train[mask]\n",
    "    \n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    X_batch = np.reshape(X_batch, (X_batch.shape[0], -1))\n",
    "\n",
    "    # Normalisation\n",
    "    X_train -= np.mean(X_train, axis = 0)\n",
    "    X_val -= np.mean(X_val, axis = 0)\n",
    "    X_test -= np.mean(X_test, axis = 0)\n",
    "    X_dev -= np.mean(X_dev, axis = 0)\n",
    "    X_batch -= np.mean(X_batch, axis = 0)\n",
    "\n",
    "    # Ajout du biais\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    X_batch = np.hstack([X_batch, np.ones((X_batch.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev, X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n",
      "batch data shape:  (200, 3073)\n",
      "batch labels shape:  (200,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev, X_batch, y_batch = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)\n",
    "print('batch data shape: ', X_batch.shape)\n",
    "print('batch labels shape: ', y_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "Les prochaines étapes consistent à implanter le calcul de **l'entropie croisée** et de son **gradient**.   Vous commencerez avec une version naïve impliquant une boucle *for* sur l'ensemble des éléments d'une batch pour ensuite implanter une version vectorisée.   Mais avant de commencer à coder, veuillez donner ici la formule de l'entropie croisée et du gradient pour une mini-batch de 500 données contenue dans le tableau\n",
    "\n",
    "$$X \\in R^{500\\times 3073}$$\n",
    "\n",
    "et une matrice de poids $$W \\in R^{3073\\times 10}$$ \n",
    "\n",
    "où 3073 est la dimensionnalité des données et 10 est le nombre de classes.\n",
    "\n",
    "**Votre Réponse:** \n",
    "\n",
    "$$Loss = -ln(S) + lamb*norm(W,2) avec S = exp(X.W) / sum(exp(X.W), 1)$$\n",
    "\n",
    "$$dW = [(S - t)*X].T + 2*lamb*W$$\n",
    "\n",
    "**NOTE IMPORTANT** : la réponse à cette question ne contient aucune boucle, seulement des multiplications matricielles et vectorielles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifieur linéaire SOFTMAX\n",
    "\n",
    "Le code pour cette section est dans le fichier **utils/loss.py**. \n",
    "\n",
    "La fonction `softmax_ce_naive_forward_backward` estime la perte (et le gradient) à l'aide de boucles `for` qui itèrent sur chaque donnée de la mini-batch.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commençons par tester la **forward pass + l'entropie croisée**.  Pour l'instant, ignorons la rétro-propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.55585138  5.77349909 -0.61236922 ... -0.67253637 -0.87537157\n",
      "  -0.6617125 ]\n",
      " [ 0.17387348 -1.80598346  0.19155258 ...  0.21037321  0.27382122\n",
      "   0.20698745]\n",
      " [-0.28636859  2.97444401 -0.31548597 ... -0.34648343 -0.45098192\n",
      "  -0.34090709]\n",
      " ...\n",
      " [-0.61429695  6.38055963 -0.67675741 ... -0.7432509  -0.96741342\n",
      "  -0.73128895]\n",
      " [ 0.04694006 -0.48755546  0.05171283 ...  0.05679377  0.07392262\n",
      "   0.05587973]\n",
      " [ 0.30834142 -3.20267064  0.33969294 ...  0.37306882  0.48558539\n",
      "   0.36706461]]\n",
      "Bravo!\n",
      "loss error: 0.000000\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# TODO: Implémenter le calcul de perte de façon naive avec des boucles dans  #\n",
    "#  la fonction softmax_ce_naive_forward_backward située dans le fichier      #\n",
    "#  utils.loss.                                                               #\n",
    "#  On commence par UNE image et UNE cible                                    #\n",
    "##############################################################################\n",
    "\n",
    "from utils.loss import softmax_ce_naive_forward_backward\n",
    "import time\n",
    "\n",
    "# Matrice de poids aléatoires + 1 donnée à tester\n",
    "np.random.seed(0)\n",
    "W = np.random.randn(3073, 10) * 5e-4\n",
    "X_rnd = np.random.randn(1, 3073) * 5\n",
    "y_rnd = np.uint32(np.ones(1))\n",
    "loss, _ = softmax_ce_naive_forward_backward(X_rnd, W, y_rnd, 0.0)\n",
    "\n",
    "# La loss suivante est celle que vous devriez obtenir\n",
    "target_loss = 2.276854\n",
    "loss_error = np.abs(loss - target_loss)\n",
    "if loss_error < 1e-5:\n",
    "    print(\"Bravo!\")\n",
    "else:\n",
    "    print(\"Il y a un bug...\")\n",
    "print('loss error: %f' % loss_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.63553662 -0.45649559 -1.05686044 ...  0.86572232 -2.16513188\n",
      "  -2.27858858]\n",
      " [-2.1048624   0.28366624 -1.39517522 ...  0.52994945 -3.74530299\n",
      "  -3.17648768]\n",
      " [-2.93143808 -0.49561605  0.06348929 ...  0.40347084 -5.83713235\n",
      "  -4.95945962]\n",
      " ...\n",
      " [ 0.28318078  2.29331036  0.42042742 ... -0.80513258 -0.17775923\n",
      "  -2.32193285]\n",
      " [ 0.17581504  2.03569961  0.67592454 ...  1.3667073  -1.99752957\n",
      "  -2.84210116]\n",
      " [ 0.02817192 -0.0407199  -0.01173696 ... -0.00808671  0.02431986\n",
      "  -0.00981905]]\n",
      "Bravo!\n",
      "loss error: 0.000000\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# TODO: Implémenter le calcul de perte de façon naive avec des boucles dans  #\n",
    "#  la fonction softmax_ce_naive_forward_backward située dans le fichier      #\n",
    "#  utils.loss.                                                               #\n",
    "#  Maintenant on test avec N=200 images et autant de cibles                  #\n",
    "##############################################################################\n",
    "\n",
    "from utils.loss import softmax_ce_naive_forward_backward\n",
    "import time\n",
    "\n",
    "# Matrice de poids aléatoires + 500 données \n",
    "np.random.seed(0)\n",
    "W = np.random.randn(3073, 10) * 1e-4\n",
    "loss, _ = softmax_ce_naive_forward_backward(X_batch, W, y_batch, 0.0)\n",
    "\n",
    "target_loss = 2.356459\n",
    "loss_error = np.abs(loss - target_loss)\n",
    "if loss_error < 1e-5:\n",
    "    print(\"Bravo!\")\n",
    "else:\n",
    "    print(\"Il y a un bug...\")\n",
    "print('loss error: %f' % loss_error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2:\n",
    "Pourquoi s'attend-on que la loss soit approximativement -np.log(1/nb_classes))?\n",
    "\n",
    "**Votre réponse:** Lorsque l'on initialise aléatoirement les poids, nous savons que l'entropie sera maximale. Pour cela, la prédiction (correspondant ici à une probabilité) associée à chaque classe sera de 1/nb_classes. Vu différemment, nous pouvons voir que, si le modèle n'est pas entraîné, il va prédire de façon égale chaque classe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.43098977e+00 -1.20913328e+00 -6.81027658e-01 ...  6.86160793e-01\n",
      "  -1.24796336e-01 -3.19055346e+00]\n",
      " [-2.81695360e+00 -5.80283642e-01 -1.11202598e+00 ...  4.46219742e-01\n",
      "  -1.48431681e+00 -4.08483887e+00]\n",
      " [-3.79122796e+00 -1.28590230e+00  3.25074256e-01 ...  5.54251616e-01\n",
      "  -3.36434630e+00 -6.11138620e+00]\n",
      " ...\n",
      " [-4.60994269e-01  1.33338845e+00  8.34242035e-01 ... -1.34774259e+00\n",
      "   1.31663061e+00 -9.06365651e-01]\n",
      " [-6.58748327e-01  1.08145126e+00  9.29722438e-01 ...  1.07841747e+00\n",
      "  -3.09743356e-01 -1.59750900e+00]\n",
      " [ 3.00797539e-02 -4.02401683e-02 -1.10506968e-02 ... -6.03864912e-03\n",
      "   2.59833688e-02 -1.36873808e-02]]\n",
      "Softmax loss: 2.339132\n",
      "Sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "###################################################################################\n",
    "#  Vérification simple: s'assurer que l'entropie-croisée soit proche de           #\n",
    "#  -log(1/nb_classes)                                                             #\n",
    "###################################################################################\n",
    "\n",
    "from utils.loss import softmax_ce_naive_forward_backward\n",
    "import time\n",
    "\n",
    "# Matrice de poids aléatoires + 500 données \n",
    "W = np.random.randn(3073, 10) * 1e-4\n",
    "loss, _ = softmax_ce_naive_forward_backward(X_batch, W, y_batch, 0.0)\n",
    "\n",
    "# La loss d'un modèle non-entrainé devrait s'approcher de -log(0.1).\n",
    "print('Softmax loss: %f' % loss)\n",
    "print('Sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rétro-propagation\n",
    "\n",
    "Maintenant, passons à la **rétro-propagation**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.1279744   1.15786877 -0.12847105 ... -0.12895357 -0.13032041\n",
      "  -0.12886991]\n",
      " [ 0.04003112 -0.36218796  0.04018648 ...  0.04033741  0.04076497\n",
      "   0.04031124]\n",
      " [-0.06593102  0.59652141 -0.06618689 ... -0.06643548 -0.06713966\n",
      "  -0.06639238]\n",
      " ...\n",
      " [-0.1414304   1.27961408 -0.14197927 ... -0.14251253 -0.14402308\n",
      "  -0.14242007]\n",
      " [ 0.01080707 -0.0977787   0.01084901 ...  0.01088976  0.01100519\n",
      "   0.01088269]\n",
      " [ 0.07098985 -0.64229201  0.07126535 ...  0.07153302  0.07229123\n",
      "   0.07148661]]\n",
      "Bravo pour la loss!\n",
      "loss error: 0.000000\n",
      "Bravo pour le gradient!\n",
      "gradient error 0.000000\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# TODO: Implémenter le calcul de perte ET DE LA RÉTROPROPAGATION de façon    #\n",
    "#       naive avec des boucles dans la fonction                              #\n",
    "#       softmax_ce_naive_forward_backward située dans le fichier utils.loss  #\n",
    "#                                                                            #\n",
    "#  On commence par UNE image et UNE cible                                    #\n",
    "##############################################################################\n",
    "\n",
    "from utils.loss import softmax_ce_naive_forward_backward\n",
    "import time\n",
    "\n",
    "# Matrice de poids aléatoires + une donnée\n",
    "np.random.seed(0)\n",
    "W = np.random.randn(3073, 10) * 1e-4\n",
    "X_rnd = np.random.randn(1, 3073)\n",
    "y_rnd = np.uint32(np.ones(1))\n",
    "loss, dW = softmax_ce_naive_forward_backward(X_rnd, W, y_rnd, 0.0)\n",
    "\n",
    "# La loss suivante est celle que vous devriez obtenir\n",
    "target_loss = 2.30114875\n",
    "loss_error = np.abs(loss - target_loss)\n",
    "if loss_error < 1e-5:\n",
    "    print(\"Bravo pour la loss!\")\n",
    "else:\n",
    "    print(\"Il y a un bug au niveau de la loss...\")\n",
    "print('loss error: %f' % loss_error)\n",
    "\n",
    "# Le gradient suivant est celui que vous devriez obtenir pour les 3 premiers poids\n",
    "target_dW = np.array([-0.1279744 ,  1.15786877, -0.12847105])\n",
    "dW_error = np.mean(np.abs(dW[0,0:3]-target_dW))\n",
    "if dW_error < 1e-7:\n",
    "    print(\"Bravo pour le gradient!\")\n",
    "else:\n",
    "    print(\"Il y a un bug au niveau du gradient...\")\n",
    "print('gradient error %f' % dW_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.68817739e+00 -1.37204569e+00 -4.61587730e-01 ...  8.34288820e-01\n",
      "  -9.34764094e-01 -4.93670879e+00]\n",
      " [-2.06141741e+00 -9.99790842e-01 -7.64298623e-01 ...  5.35426798e-01\n",
      "  -2.35436909e+00 -6.23144698e+00]\n",
      " [-2.86517804e+00 -1.82217642e+00  7.03969046e-01 ...  4.17467420e-01\n",
      "  -4.13619756e+00 -8.48146735e+00]\n",
      " ...\n",
      " [ 2.33613191e-01  1.53139723e+00  2.61028583e-01 ... -1.30382203e+00\n",
      "   5.89147437e-01 -2.84850674e+00]\n",
      " [ 1.22175450e-01  1.20625036e+00  4.12379388e-01 ...  7.80103824e-01\n",
      "  -9.49227030e-01 -3.60700249e+00]\n",
      " [ 2.80292466e-02 -4.27386483e-02 -1.21210217e-02 ... -7.08090406e-03\n",
      "   2.31554188e-02 -1.50200384e-02]]\n",
      "[-1.68817739 -1.37204569 -0.46158773  1.9206649 ]\n",
      "Bravo pour la loss!\n",
      "loss error: 0.000000\n",
      "Bravo pour le gradient!\n",
      "gradient error 0.000000\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# TODO: Implémenter le calcul de perte ET DE LA RÉTROPROPAGATION de façon    #\n",
    "#       naive avec des boucles dans la fonction                              #\n",
    "#       softmax_ce_naive_forward_backward située dans le fichier utils.loss  #\n",
    "#                                                                            #\n",
    "#  Maintenant on test avec N=200 images et autant de cibles                  #\n",
    "##############################################################################\n",
    "\n",
    "from utils.loss import softmax_ce_naive_forward_backward\n",
    "import time\n",
    "\n",
    "# Matrice de poids aléatoires + 500 données\n",
    "np.random.seed(1)\n",
    "W = np.random.randn(3073, 10) * 1e-4\n",
    "loss, dW = softmax_ce_naive_forward_backward(X_batch, W, y_batch, 0.0)\n",
    "print(dW[0,0:4])\n",
    "# La loss suivante est celle que vous devriez obtenir\n",
    "target_loss = 2.35680883\n",
    "loss_error = np.abs(loss - target_loss)\n",
    "if loss_error < 1e-5:\n",
    "    print(\"Bravo pour la loss!\")\n",
    "else:\n",
    "    print(\"Il y a un bug au niveau de la loss...\")\n",
    "print('loss error: %f' % loss_error)\n",
    "\n",
    "# Le gradient suivant est celui que vous devriez obtenir pour les 3 premiers poids\n",
    "target_dW = np.array([-1.68817739, -1.37204569, -0.46158773, 1.9206649])\n",
    "dW_error = np.mean(np.abs(dW[0,0:4]-target_dW))\n",
    "if dW_error < 1e-7:\n",
    "    print(\"Bravo pour le gradient!\")\n",
    "else:\n",
    "    print(\"Il y a un bug au niveau du gradient...\")\n",
    "print('gradient error %f' % dW_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encore quelques vérifications d'usage\n",
    "En principe, à ce point-ci, le calcul de l'entropie croisée (et de son gradient) via la fonction *softmax_ce_naive_forward_backward* devrait fonctionner.  Mais avant de passer à la prochaine étape il nous reste deux vérifications à faire : s'assurer que la **régularisation** fonctionne et passer le teste du **gradient numérique**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.18351115e+00 -1.37008995e+00  6.98425544e-01 ...  2.30496245e-01\n",
      "  -8.09853658e-01 -4.72660770e+00]\n",
      " [-2.40834363e+00 -8.89049913e-01  6.95096532e-01 ... -2.66999918e-01\n",
      "  -2.17871308e+00 -4.81063512e+00]\n",
      " [-5.08423163e+00 -8.26052606e-01  1.92650427e+00 ... -4.88518516e-01\n",
      "  -3.88500694e+00 -5.65309529e+00]\n",
      " ...\n",
      " [ 6.86631952e-01 -1.37396260e+00 -7.12855292e-01 ... -1.02767013e+00\n",
      "   1.72770673e+00 -1.51801513e+00]\n",
      " [-7.24721080e-01 -1.97282266e+00 -7.91860182e-02 ...  4.73549656e-01\n",
      "  -1.36039080e-01 -1.70402906e+00]\n",
      " [-1.45804820e-02  1.91981833e-02 -9.81783766e-03 ...  1.57138376e-02\n",
      "   5.61016090e-03  7.86815143e-03]]\n",
      "[[-1.15102425e+00 -1.38232508e+00  6.87862109e-01 ...  2.15272107e-01\n",
      "  -8.03472877e-01 -4.73159510e+00]\n",
      " [-2.37910147e+00 -9.30252727e-01  6.88648187e-01 ... -2.84557086e-01\n",
      "  -2.17786880e+00 -4.79897882e+00]\n",
      " [-5.10624401e+00 -8.03158132e-01  1.94453608e+00 ... -5.07233905e-01\n",
      "  -3.89036470e+00 -5.64248818e+00]\n",
      " ...\n",
      " [ 6.94712857e-01 -1.35620205e+00 -7.41371442e-01 ... -1.01205700e+00\n",
      "   1.74980145e+00 -1.50743620e+00]\n",
      " [-7.28532841e-01 -1.97891728e+00 -1.02850982e-01 ...  4.43207580e-01\n",
      "  -1.41682572e-01 -1.69645921e+00]\n",
      " [-3.66985396e-03  4.75763318e-02 -2.49533204e-02 ...  4.35011353e-03\n",
      "   1.85885940e-02  4.40023628e-04]]\n",
      "2.381067040192888\n",
      "[[-1.14452686e+00 -1.38477211e+00  6.85749422e-01 ...  2.12227279e-01\n",
      "  -8.02196720e-01 -4.73259259e+00]\n",
      " [-2.37325304e+00 -9.38493290e-01  6.87358519e-01 ... -2.88068520e-01\n",
      "  -2.17769995e+00 -4.79664756e+00]\n",
      " [-5.11064649e+00 -7.98579237e-01  1.94814245e+00 ... -5.10976983e-01\n",
      "  -3.89143626e+00 -5.64036676e+00]\n",
      " ...\n",
      " [ 6.96329039e-01 -1.35264994e+00 -7.47074672e-01 ... -1.00893437e+00\n",
      "   1.75422039e+00 -1.50532042e+00]\n",
      " [-7.29295193e-01 -1.98013620e+00 -1.07583975e-01 ...  4.37139165e-01\n",
      "  -1.42811270e-01 -1.69494524e+00]\n",
      " [-1.48772835e-03  5.32519615e-02 -2.79804170e-02 ...  2.07736871e-03\n",
      "   2.11842806e-02 -1.04560193e-03]]\n",
      "2.3841350450450016\n",
      "[[-1.13673001e+00 -1.38770854e+00  6.83214198e-01 ...  2.08573486e-01\n",
      "  -8.00665333e-01 -4.73378956e+00]\n",
      " [-2.36623492e+00 -9.48381965e-01  6.85810916e-01 ... -2.92282240e-01\n",
      "  -2.17749732e+00 -4.79385004e+00]\n",
      " [-5.11592946e+00 -7.93084563e-01  1.95247008e+00 ... -5.15468676e-01\n",
      "  -3.89272212e+00 -5.63782105e+00]\n",
      " ...\n",
      " [ 6.98268456e-01 -1.34838741e+00 -7.53918548e-01 ... -1.00518722e+00\n",
      "   1.75952313e+00 -1.50278148e+00]\n",
      " [-7.30210016e-01 -1.98159891e+00 -1.13263567e-01 ...  4.29857067e-01\n",
      "  -1.44165708e-01 -1.69312848e+00]\n",
      " [ 1.13082237e-03  6.00627171e-02 -3.16129328e-02 ... -6.49925071e-04\n",
      "   2.42991046e-02 -2.82835260e-03]]\n",
      "2.3878166508675385\n",
      "[[-1.12737378e+00 -1.39123226e+00  6.80171929e-01 ...  2.04188934e-01\n",
      "  -7.98827667e-01 -4.73522594e+00]\n",
      " [-2.35781318e+00 -9.60248376e-01  6.83953793e-01 ... -2.97338704e-01\n",
      "  -2.17725417e+00 -4.79049303e+00]\n",
      " [-5.12226903e+00 -7.86490955e-01  1.95766324e+00 ... -5.20858708e-01\n",
      "  -3.89426515e+00 -5.63476620e+00]\n",
      " ...\n",
      " [ 7.00595757e-01 -1.34327237e+00 -7.62131199e-01 ... -1.00069064e+00\n",
      "   1.76588641e+00 -1.49973475e+00]\n",
      " [-7.31307803e-01 -1.98335416e+00 -1.20079076e-01 ...  4.21118549e-01\n",
      "  -1.45791034e-01 -1.69094836e+00]\n",
      " [ 4.27308323e-03  6.82356239e-02 -3.59719519e-02 ... -3.92267761e-03\n",
      "   2.80368933e-02 -4.96765341e-03]]\n",
      "2.392234577854582\n",
      "[[-1.1161463  -1.39546072  0.67652121 ...  0.19892747 -0.79662247\n",
      "  -4.73694958]\n",
      " [-2.34770709 -0.97448807  0.68172525 ... -0.30340646 -2.17696239\n",
      "  -4.78646461]\n",
      " [-5.12987651 -0.77857862  1.96389504 ... -0.52732675 -3.8961168\n",
      "  -5.63110039]\n",
      " ...\n",
      " [ 0.70338852 -1.33713432 -0.77198638 ... -0.99529474  1.77352234\n",
      "  -1.49607867]\n",
      " [-0.73262515 -1.98546046 -0.12825769 ...  0.41063233 -0.14774142\n",
      "  -1.68833222]\n",
      " [ 0.0080438   0.07804311 -0.04120277 ... -0.00784998  0.03252224\n",
      "  -0.00753481]]\n",
      "2.397536090239035\n",
      "[[-1.10267333 -1.40053487  0.67214034 ...  0.19261372 -0.79397623\n",
      "  -4.73901796]\n",
      " [-2.33557978 -0.9915757   0.67905099 ... -0.31068777 -2.17661225\n",
      "  -4.78163051]\n",
      " [-5.13900548 -0.76908383  1.97137319 ... -0.53508839 -3.89833877\n",
      "  -5.62670141]\n",
      " ...\n",
      " [ 0.70673983 -1.32976867 -0.7838126  ... -0.98881966  1.78268546\n",
      "  -1.49169138]\n",
      " [-0.73420596 -1.98798802 -0.13807202 ...  0.39804886 -0.15008189\n",
      "  -1.68519285]\n",
      " [ 0.01256865  0.0898121  -0.04747976 ... -0.01256274  0.03790466\n",
      "  -0.01061541]]\n",
      "2.4038979051003784\n",
      "[[-1.08650577 -1.40662385  0.6668833  ...  0.18503721 -0.79080075\n",
      "  -4.74150002]\n",
      " [-2.32102701 -1.01208086  0.67584188 ... -0.31942534 -2.17619208\n",
      "  -4.77582958]\n",
      " [-5.14996025 -0.75769007  1.98034698 ... -0.54440237 -3.90100513\n",
      "  -5.62142263]\n",
      " ...\n",
      " [ 0.71076141 -1.32092988 -0.79800406 ... -0.98104957  1.79368121\n",
      "  -1.48642663]\n",
      " [-0.73610294 -1.99102109 -0.14984922 ...  0.3829487  -0.15289046\n",
      "  -1.68142561]\n",
      " [ 0.01799848  0.10393488 -0.05501215 ... -0.01821806  0.04436355\n",
      "  -0.01431212]]\n",
      "2.41153208293399\n",
      "[[-1.06710469 -1.41393063  0.66057485 ...  0.17594541 -0.78699016\n",
      "  -4.74447848]\n",
      " [-2.30356368 -1.03668705  0.67199095 ... -0.32991043 -2.17568789\n",
      "  -4.76886848]\n",
      " [-5.16310598 -0.74401757  1.99111552 ... -0.55557914 -3.90420477\n",
      "  -5.6150881 ]\n",
      " ...\n",
      " [ 0.7155873  -1.31032333 -0.81503381 ... -0.97172546  1.8068761\n",
      "  -1.48010893]\n",
      " [-0.73837931 -1.99466078 -0.16398186 ...  0.36482851 -0.15626073\n",
      "  -1.67690492]\n",
      " [ 0.02451427  0.12088222 -0.06405101 ... -0.02500444  0.05211423\n",
      "  -0.01874817]]\n",
      "2.4206930963343245\n",
      "[[-1.0438234  -1.42269877  0.65300471 ...  0.16503524 -0.78241746\n",
      "  -4.74805264]\n",
      " [-2.2826077  -1.06621447  0.66736983 ... -0.34249253 -2.17508285\n",
      "  -4.76051515]\n",
      " [-5.17888085 -0.72761056  2.00403777 ... -0.56899126 -3.90804434\n",
      "  -5.60748666]\n",
      " ...\n",
      " [ 0.72137837 -1.29759548 -0.83546952 ... -0.96053653  1.82270998\n",
      "  -1.47252769]\n",
      " [-0.74111095 -1.9990284  -0.18094103 ...  0.34308428 -0.16030506\n",
      "  -1.6714801 ]\n",
      " [ 0.03233322  0.14121903 -0.07489764 ... -0.0331481   0.06141505\n",
      "  -0.02407144]]\n",
      "2.4316863124147257\n",
      "[[-1.01588585 -1.43322053  0.64392054 ...  0.15194304 -0.77693023\n",
      "  -4.75234162]\n",
      " [-2.25746051 -1.10164738  0.66182449 ... -0.35759105 -2.1743568\n",
      "  -4.75049115]\n",
      " [-5.19781069 -0.70792215  2.01954447 ... -0.58508581 -3.91265181\n",
      "  -5.59836493]\n",
      " ...\n",
      " [ 0.72832765 -1.28232205 -0.85999236 ... -0.94710981  1.84171063\n",
      "  -1.46343021]\n",
      " [-0.74438893 -2.00426955 -0.20129204 ...  0.31699121 -0.16515826\n",
      "  -1.66497031]\n",
      " [ 0.04171596  0.1656232  -0.0879136  ... -0.04292048  0.07257602\n",
      "  -0.03045936]]\n",
      "2.444878171711207\n",
      "Bravo!\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# Vérifions que le fait d'augmenter le terme de régularisation L2            #\n",
    "# augmente la loss...                                                        #\n",
    "##############################################################################\n",
    "success = True\n",
    "\n",
    "# Matrice de poids aléatoires + 500 données\n",
    "np.random.seed(1)\n",
    "W = np.random.randn(3073, 10) * 1e-4\n",
    "prev_loss, _ = softmax_ce_naive_forward_backward(X_dev, W, y_dev, 0.0)\n",
    "\n",
    "reg = 1e2\n",
    "for i in range(10):\n",
    "    loss, _ = softmax_ce_naive_forward_backward(X_dev, W, y_dev, reg)\n",
    "    print(loss)\n",
    "    if loss <= prev_loss:\n",
    "        success = False\n",
    "    prev_loss = loss\n",
    "    reg *= 1.2\n",
    "    \n",
    "if success:\n",
    "    print(\"Bravo!\")\n",
    "else:\n",
    "    print('Erreur!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------\n",
      "Gradient check : reg=0\n",
      "------------\n",
      "\n",
      "[[-1.18351115e+00 -1.37008995e+00  6.98425544e-01 ...  2.30496245e-01\n",
      "  -8.09853658e-01 -4.72660770e+00]\n",
      " [-2.40834363e+00 -8.89049913e-01  6.95096532e-01 ... -2.66999918e-01\n",
      "  -2.17871308e+00 -4.81063512e+00]\n",
      " [-5.08423163e+00 -8.26052606e-01  1.92650427e+00 ... -4.88518516e-01\n",
      "  -3.88500694e+00 -5.65309529e+00]\n",
      " ...\n",
      " [ 6.86631952e-01 -1.37396260e+00 -7.12855292e-01 ... -1.02767013e+00\n",
      "   1.72770673e+00 -1.51801513e+00]\n",
      " [-7.24721080e-01 -1.97282266e+00 -7.91860182e-02 ...  4.73549656e-01\n",
      "  -1.36039080e-01 -1.70402906e+00]\n",
      " [-1.45804820e-02  1.91981833e-02 -9.81783766e-03 ...  1.57138376e-02\n",
      "   5.61016090e-03  7.86815143e-03]]\n",
      "[[-1.18351376e+00 -1.37009248e+00  6.98453690e-01 ...  2.30492770e-01\n",
      "  -8.09856312e-01 -4.72661090e+00]\n",
      " [-2.40834695e+00 -8.89053225e-01  6.95131827e-01 ... -2.67004448e-01\n",
      "  -2.17871641e+00 -4.81063954e+00]\n",
      " [-5.08423619e+00 -8.26056798e-01  1.92655147e+00 ... -4.88524644e-01\n",
      "  -3.88501160e+00 -5.65310116e+00]\n",
      " ...\n",
      " [ 6.86631117e-01 -1.37396605e+00 -7.12835480e-01 ... -1.02767305e+00\n",
      "   1.72770442e+00 -1.51801740e+00]\n",
      " [-7.24722619e-01 -1.97282654e+00 -7.91611453e-02 ...  4.73546242e-01\n",
      "  -1.36041922e-01 -1.70403218e+00]\n",
      " [-1.45804460e-02  1.91982420e-02 -9.81811384e-03 ...  1.57138721e-02\n",
      "   5.61018092e-03  7.86819688e-03]]\n",
      "[[-1.18350855e+00 -1.37008743e+00  6.98397399e-01 ...  2.30499719e-01\n",
      "  -8.09851005e-01 -4.72660449e+00]\n",
      " [-2.40834030e+00 -8.89046600e-01  6.95061238e-01 ... -2.66995387e-01\n",
      "  -2.17870974e+00 -4.81063070e+00]\n",
      " [-5.08422707e+00 -8.26048415e-01  1.92645707e+00 ... -4.88512389e-01\n",
      "  -3.88500228e+00 -5.65308942e+00]\n",
      " ...\n",
      " [ 6.86632787e-01 -1.37395916e+00 -7.12875105e-01 ... -1.02766721e+00\n",
      "   1.72770904e+00 -1.51801285e+00]\n",
      " [-7.24719541e-01 -1.97281878e+00 -7.92108923e-02 ...  4.73553071e-01\n",
      "  -1.36036238e-01 -1.70402593e+00]\n",
      " [-1.45805180e-02  1.91981246e-02 -9.81756125e-03 ...  1.57138032e-02\n",
      "   5.61014087e-03  7.86810596e-03]]\n",
      "numerical: 2.490180, analytic 2.490180, relative error: 6.793379e-11\n",
      "[[-1.18352783e+00 -1.36991871e+00  6.98406399e-01 ...  2.30477013e-01\n",
      "  -8.09871258e-01 -4.72662696e+00]\n",
      " [-2.40836152e+00 -8.88862650e-01  6.95075161e-01 ... -2.67021176e-01\n",
      "  -2.17873215e+00 -4.81065654e+00]\n",
      " [-5.08425129e+00 -8.25846359e-01  1.92648088e+00 ... -4.88541712e-01\n",
      "  -3.88502809e+00 -5.65311852e+00]\n",
      " ...\n",
      " [ 6.86624928e-01 -1.37389350e+00 -7.12862837e-01 ... -1.02767745e+00\n",
      "   1.72769996e+00 -1.51802455e+00]\n",
      " [-7.24730050e-01 -1.97273383e+00 -7.91955412e-02 ...  4.73540241e-01\n",
      "  -1.36048370e-01 -1.70404009e+00]\n",
      " [-1.45804198e-02  1.91978561e-02 -9.81778264e-03 ...  1.57138790e-02\n",
      "   5.61012501e-03  7.86827774e-03]]\n",
      "[[-1.18349448e+00 -1.37026120e+00  6.98444690e-01 ...  2.30515477e-01\n",
      "  -8.09836060e-01 -4.72658844e+00]\n",
      " [-2.40832573e+00 -8.89237176e-01  6.95117903e-01 ... -2.66978659e-01\n",
      "  -2.17869400e+00 -4.81061370e+00]\n",
      " [-5.08421196e+00 -8.26258852e-01  1.92652766e+00 ... -4.88495320e-01\n",
      "  -3.88498579e+00 -5.65307205e+00]\n",
      " ...\n",
      " [ 6.86638976e-01 -1.37403170e+00 -7.12847746e-01 ... -1.02766280e+00\n",
      "   1.72771350e+00 -1.51800571e+00]\n",
      " [-7.24712111e-01 -1.97291149e+00 -7.91764950e-02 ...  4.73559072e-01\n",
      "  -1.36029790e-01 -1.70401802e+00]\n",
      " [-1.45805442e-02  1.91985108e-02 -9.81789272e-03 ...  1.57137962e-02\n",
      "   5.61019677e-03  7.86802509e-03]]\n",
      "numerical: -0.231663, analytic -0.231663, relative error: 4.092486e-10\n",
      "[[-1.18352781e+00 -1.37010573e+00  6.98407153e-01 ...  2.30655183e-01\n",
      "  -8.09870370e-01 -4.72662516e+00]\n",
      " [-2.40835872e+00 -8.89064609e-01  6.95079498e-01 ... -2.66854816e-01\n",
      "  -2.17872814e+00 -4.81065126e+00]\n",
      " [-5.08424642e+00 -8.26066884e-01  1.92648759e+00 ... -4.88376755e-01\n",
      "  -3.88502150e+00 -5.65311101e+00]\n",
      " ...\n",
      " [ 6.86620316e-01 -1.37397331e+00 -7.12867798e-01 ... -1.02755782e+00\n",
      "   1.72769483e+00 -1.51802870e+00]\n",
      " [-7.24732850e-01 -1.97283375e+00 -7.91988135e-02 ...  4.73663363e-01\n",
      "  -1.36051199e-01 -1.70404212e+00]\n",
      " [-1.45804282e-02  1.91982830e-02 -9.81777975e-03 ...  1.57134577e-02\n",
      "   5.61015347e-03  7.86825624e-03]]\n",
      "[[-1.18349449e+00 -1.37007418e+00  6.98443935e-01 ...  2.30337308e-01\n",
      "  -8.09836948e-01 -4.72659023e+00]\n",
      " [-2.40832853e+00 -8.89035217e-01  6.95113565e-01 ... -2.67145017e-01\n",
      "  -2.17869801e+00 -4.81061898e+00]\n",
      " [-5.08421684e+00 -8.26038328e-01  1.92652095e+00 ... -4.88660276e-01\n",
      "  -3.88499238e+00 -5.65307957e+00]\n",
      " ...\n",
      " [ 6.86643587e-01 -1.37395190e+00 -7.12842785e-01 ... -1.02778244e+00\n",
      "   1.72771863e+00 -1.51800155e+00]\n",
      " [-7.24709311e-01 -1.97281158e+00 -7.91732228e-02 ...  4.73435951e-01\n",
      "  -1.36026961e-01 -1.70401600e+00]\n",
      " [-1.45805358e-02  1.91980836e-02 -9.81789561e-03 ...  1.57142178e-02\n",
      "   5.61016831e-03  7.86804660e-03]]\n",
      "numerical: -1.294697, analytic -1.294697, relative error: 8.859541e-10\n",
      "[[-1.18353128e+00 -1.36989118e+00  6.98404339e-01 ...  2.30474445e-01\n",
      "  -8.09874333e-01 -4.72663098e+00]\n",
      " [-2.40836640e+00 -8.88830486e-01  6.95072909e-01 ... -2.67024033e-01\n",
      "  -2.17873579e+00 -4.81066075e+00]\n",
      " [-5.08425600e+00 -8.25817290e-01  1.92647904e+00 ... -4.88544110e-01\n",
      "  -3.88503176e+00 -5.65312241e+00]\n",
      " ...\n",
      " [ 6.86620945e-01 -1.37386169e+00 -7.12865505e-01 ... -1.02768040e+00\n",
      "   1.72769615e+00 -1.51802776e+00]\n",
      " [-7.24733303e-01 -1.97270835e+00 -7.91978408e-02 ...  4.73537986e-01\n",
      "  -1.36051531e-01 -1.70404243e+00]\n",
      " [-1.45803832e-02  1.91978728e-02 -9.81782098e-03 ...  1.57138812e-02\n",
      "   5.61010635e-03  7.86827324e-03]]\n",
      "[[-1.18349102e+00 -1.37028873e+00  6.98446750e-01 ...  2.30518044e-01\n",
      "  -8.09832984e-01 -4.72658441e+00]\n",
      " [-2.40832085e+00 -8.89269341e-01  6.95120154e-01 ... -2.66975802e-01\n",
      "  -2.17869036e+00 -4.81060950e+00]\n",
      " [-5.08420725e+00 -8.26287923e-01  1.92652950e+00 ... -4.88492922e-01\n",
      "  -3.88498212e+00 -5.65306816e+00]\n",
      " ...\n",
      " [ 6.86642958e-01 -1.37406352e+00 -7.12845078e-01 ... -1.02765986e+00\n",
      "   1.72771731e+00 -1.51800250e+00]\n",
      " [-7.24708857e-01 -1.97293697e+00 -7.91741953e-02 ...  4.73561328e-01\n",
      "  -1.36026628e-01 -1.70401568e+00]\n",
      " [-1.45805808e-02  1.91984942e-02 -9.81785438e-03 ...  1.57137940e-02\n",
      "   5.61021542e-03  7.86802959e-03]]\n",
      "numerical: 0.305044, analytic 0.305044, relative error: 2.130418e-09\n",
      "[[-1.18351278e+00 -1.37009168e+00  6.98442949e-01 ...  2.30493998e-01\n",
      "  -8.09855041e-01 -4.72661055e+00]\n",
      " [-2.40834311e+00 -8.89050327e-01  6.95099701e-01 ... -2.67001034e-01\n",
      "  -2.17871278e+00 -4.81063652e+00]\n",
      " [-5.08422996e+00 -8.26052071e-01  1.92649872e+00 ... -4.88518920e-01\n",
      "  -3.88500547e+00 -5.65309607e+00]\n",
      " ...\n",
      " [ 6.86629948e-01 -1.37396511e+00 -7.12839645e-01 ... -1.02767351e+00\n",
      "   1.72770620e+00 -1.51801743e+00]\n",
      " [-7.24722971e-01 -1.97282583e+00 -7.91667818e-02 ...  4.73545288e-01\n",
      "  -1.36039654e-01 -1.70403199e+00]\n",
      " [-1.45804323e-02  1.91982588e-02 -9.81834215e-03 ...  1.57139626e-02\n",
      "   5.61017888e-03  7.86820883e-03]]\n",
      "[[-1.18350952e+00 -1.37008823e+00  6.98408140e-01 ...  2.30498492e-01\n",
      "  -8.09852277e-01 -4.72660484e+00]\n",
      " [-2.40834414e+00 -8.89049498e-01  6.95093362e-01 ... -2.66998801e-01\n",
      "  -2.17871337e+00 -4.81063372e+00]\n",
      " [-5.08423330e+00 -8.26053142e-01  1.92650982e+00 ... -4.88518112e-01\n",
      "  -3.88500841e+00 -5.65309450e+00]\n",
      " ...\n",
      " [ 6.86633956e-01 -1.37396010e+00 -7.12870941e-01 ... -1.02766674e+00\n",
      "   1.72770726e+00 -1.51801282e+00]\n",
      " [-7.24719190e-01 -1.97281950e+00 -7.92052559e-02 ...  4.73554025e-01\n",
      "  -1.36038506e-01 -1.70402612e+00]\n",
      " [-1.45805316e-02  1.91981078e-02 -9.81733293e-03 ...  1.57137127e-02\n",
      "   5.61014290e-03  7.86809400e-03]]\n",
      "numerical: 0.625743, analytic 0.625743, relative error: 3.279626e-10\n",
      "[[-1.18353441e+00 -1.37011521e+00  6.98402580e-01 ...  2.30472403e-01\n",
      "  -8.09873327e-01 -4.72663370e+00]\n",
      " [-2.40836931e+00 -8.89078138e-01  6.95071082e-01 ... -2.67026289e-01\n",
      "  -2.17873496e+00 -4.81066417e+00]\n",
      " [-5.08426466e+00 -8.26087998e-01  1.92647146e+00 ... -4.88552338e-01\n",
      "  -3.88503546e+00 -5.65313185e+00]\n",
      " ...\n",
      " [ 6.86620072e-01 -1.37397609e+00 -7.12867119e-01 ... -1.02768302e+00\n",
      "   1.72769539e+00 -1.51803025e+00]\n",
      " [-7.24738115e-01 -1.97284122e+00 -7.92029102e-02 ...  4.73531545e-01\n",
      "  -1.36055326e-01 -1.70404897e+00]\n",
      " [-1.45802554e-02  1.91984156e-02 -9.81767123e-03 ...  1.57140212e-02\n",
      "   5.61024346e-03  7.86840951e-03]]\n",
      "[[-1.18348790e+00 -1.37006470e+00  6.98448509e-01 ...  2.30520087e-01\n",
      "  -8.09833990e-01 -4.72658169e+00]\n",
      " [-2.40831794e+00 -8.89021687e-01  6.95121982e-01 ... -2.66973545e-01\n",
      "  -2.17869119e+00 -4.81060607e+00]\n",
      " [-5.08419860e+00 -8.26017214e-01  1.92653708e+00 ... -4.88484694e-01\n",
      "  -3.88497843e+00 -5.65305872e+00]\n",
      " ...\n",
      " [ 6.86643832e-01 -1.37394912e+00 -7.12843465e-01 ... -1.02765723e+00\n",
      "   1.72771807e+00 -1.51800001e+00]\n",
      " [-7.24704045e-01 -1.97280410e+00 -7.91691259e-02 ...  4.73567769e-01\n",
      "  -1.36022834e-01 -1.70400914e+00]\n",
      " [-1.45807086e-02  1.91979510e-02 -9.81800413e-03 ...  1.57136540e-02\n",
      "   5.61007832e-03  7.86789332e-03]]\n",
      "numerical: 0.108874, analytic 0.108874, relative error: 5.380157e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.18352565e+00 -1.37010642e+00  6.98409538e-01 ...  2.30642758e-01\n",
      "  -8.09869217e-01 -4.72662410e+00]\n",
      " [-2.40836110e+00 -8.89070019e-01  6.95076663e-01 ... -2.66822182e-01\n",
      "  -2.17873214e+00 -4.81065476e+00]\n",
      " [-5.08425540e+00 -8.26079122e-01  1.92647779e+00 ... -4.88282174e-01\n",
      "  -3.88503234e+00 -5.65312127e+00]\n",
      " ...\n",
      " [ 6.86625669e-01 -1.37396971e+00 -7.12862661e-01 ... -1.02760586e+00\n",
      "   1.72769967e+00 -1.51802338e+00]\n",
      " [-7.24731434e-01 -1.97283387e+00 -7.91977030e-02 ...  4.73652658e-01\n",
      "  -1.36050608e-01 -1.70404138e+00]\n",
      " [-1.45804352e-02  1.91982270e-02 -9.81782495e-03 ...  1.57137410e-02\n",
      "   5.61009523e-03  7.86825915e-03]]\n",
      "[[-1.18349666e+00 -1.37007349e+00  6.98441551e-01 ...  2.30349733e-01\n",
      "  -8.09838100e-01 -4.72659130e+00]\n",
      " [-2.40832615e+00 -8.89029807e-01  6.95116400e-01 ... -2.67177652e-01\n",
      "  -2.17869401e+00 -4.81061549e+00]\n",
      " [-5.08420786e+00 -8.26026090e-01  1.92653075e+00 ... -4.88754856e-01\n",
      "  -3.88498154e+00 -5.65306931e+00]\n",
      " ...\n",
      " [ 6.86638235e-01 -1.37395549e+00 -7.12847923e-01 ... -1.02773440e+00\n",
      "   1.72771379e+00 -1.51800688e+00]\n",
      " [-7.24710726e-01 -1.97281146e+00 -7.91743332e-02 ...  4.73446654e-01\n",
      "  -1.36027551e-01 -1.70401674e+00]\n",
      " [-1.45805287e-02  1.91981396e-02 -9.81785042e-03 ...  1.57139346e-02\n",
      "   5.61022654e-03  7.86804368e-03]]\n",
      "numerical: -0.779429, analytic -0.779429, relative error: 1.714693e-11\n",
      "[[-1.18352155e+00 -1.37009996e+00  6.98414726e-01 ...  2.30485774e-01\n",
      "  -8.09859959e-01 -4.72661918e+00]\n",
      " [-2.40835114e+00 -8.89057797e-01  6.95088275e-01 ... -2.67008029e-01\n",
      "  -2.17871703e+00 -4.81064453e+00]\n",
      " [-5.08423868e+00 -8.26059469e-01  1.92649665e+00 ... -4.88526194e-01\n",
      "  -3.88501008e+00 -5.65310374e+00]\n",
      " ...\n",
      " [ 6.86624703e-01 -1.37396961e+00 -7.12862800e-01 ... -1.02767820e+00\n",
      "   1.72770108e+00 -1.51802491e+00]\n",
      " [-7.24726336e-01 -1.97282800e+00 -7.91917339e-02 ...  4.73543197e-01\n",
      "  -1.36042987e-01 -1.70403675e+00]\n",
      " [-1.45804128e-02  1.91983182e-02 -9.81767182e-03 ...  1.57140006e-02\n",
      "   5.61020188e-03  7.86827150e-03]]\n",
      "[[-1.18350075e+00 -1.37007995e+00  6.98436363e-01 ...  2.30506716e-01\n",
      "  -8.09847358e-01 -4.72659621e+00]\n",
      " [-2.40833611e+00 -8.89042028e-01  6.95104788e-01 ... -2.66991806e-01\n",
      "  -2.17870912e+00 -4.81062571e+00]\n",
      " [-5.08422457e+00 -8.26045744e-01  1.92651189e+00 ... -4.88510838e-01\n",
      "  -3.88500381e+00 -5.65308683e+00]\n",
      " ...\n",
      " [ 6.86639200e-01 -1.37395560e+00 -7.12847784e-01 ... -1.02766206e+00\n",
      "   1.72771238e+00 -1.51800534e+00]\n",
      " [-7.24715825e-01 -1.97281732e+00 -7.91803025e-02 ...  4.73556116e-01\n",
      "  -1.36035173e-01 -1.70402136e+00]\n",
      " [-1.45805512e-02  1.91980484e-02 -9.81800354e-03 ...  1.57136746e-02\n",
      "   5.61011990e-03  7.86803134e-03]]\n",
      "numerical: -3.005799, analytic -3.005799, relative error: 1.292235e-10\n",
      "[[-1.18352099e+00 -1.37010090e+00  6.98413820e-01 ...  2.30485195e-01\n",
      "  -8.09862611e-01 -4.72651327e+00]\n",
      " [-2.40835157e+00 -8.89058990e-01  6.95086756e-01 ... -2.67009198e-01\n",
      "  -2.17871987e+00 -4.81055947e+00]\n",
      " [-5.08423764e+00 -8.26059758e-01  1.92649657e+00 ... -4.88525983e-01\n",
      "  -3.88501201e+00 -5.65303708e+00]\n",
      " ...\n",
      " [ 6.86617811e-01 -1.37397617e+00 -7.12868603e-01 ... -1.02768451e+00\n",
      "   1.72769451e+00 -1.51789313e+00]\n",
      " [-7.24733193e-01 -1.97283451e+00 -7.91976756e-02 ...  4.73537007e-01\n",
      "  -1.36049732e-01 -1.70392265e+00]\n",
      " [-1.45804305e-02  1.91982980e-02 -9.81774371e-03 ...  1.57139672e-02\n",
      "   5.61017582e-03  7.86760323e-03]]\n",
      "[[-1.18350132e+00 -1.37007901e+00  6.98437268e-01 ...  2.30507295e-01\n",
      "  -8.09844706e-01 -4.72670213e+00]\n",
      " [-2.40833568e+00 -8.89040835e-01  6.95106307e-01 ... -2.66990637e-01\n",
      "  -2.17870628e+00 -4.81071077e+00]\n",
      " [-5.08422561e+00 -8.26045455e-01  1.92651197e+00 ... -4.88511049e-01\n",
      "  -3.88500187e+00 -5.65315350e+00]\n",
      " ...\n",
      " [ 6.86646093e-01 -1.37394904e+00 -7.12841981e-01 ... -1.02765575e+00\n",
      "   1.72771895e+00 -1.51813712e+00]\n",
      " [-7.24708968e-01 -1.97281081e+00 -7.91743606e-02 ...  4.73562307e-01\n",
      "  -1.36028428e-01 -1.70413547e+00]\n",
      " [-1.45805334e-02  1.91980686e-02 -9.81793165e-03 ...  1.57137080e-02\n",
      "   5.61014596e-03  7.86869989e-03]]\n",
      "numerical: 1.820060, analytic 1.820060, relative error: 2.509484e-10\n",
      "[[-1.18351768e+00 -1.37009428e+00  6.98420962e-01 ...  2.30492226e-01\n",
      "  -8.09795677e-01 -4.72661276e+00]\n",
      " [-2.40834833e+00 -8.89052543e-01  6.95093836e-01 ... -2.67002070e-01\n",
      "  -2.17867368e+00 -4.81063846e+00]\n",
      " [-5.08423550e+00 -8.26053960e-01  1.92650299e+00 ... -4.88519391e-01\n",
      "  -3.88497766e+00 -5.65309765e+00]\n",
      " ...\n",
      " [ 6.86624300e-01 -1.37396884e+00 -7.12861807e-01 ... -1.02767681e+00\n",
      "   1.72777126e+00 -1.51801999e+00]\n",
      " [-7.24728359e-01 -1.97282827e+00 -7.91919925e-02 ...  4.73543292e-01\n",
      "  -1.35976624e-01 -1.70403377e+00]\n",
      " [-1.45805084e-02  1.91982113e-02 -9.81779840e-03 ...  1.57138750e-02\n",
      "   5.61042383e-03  7.86812205e-03]]\n",
      "[[-1.18350463e+00 -1.37008563e+00  6.98430127e-01 ...  2.30500263e-01\n",
      "  -8.09911636e-01 -4.72660263e+00]\n",
      " [-2.40833892e+00 -8.89047283e-01  6.95099227e-01 ... -2.66997766e-01\n",
      "  -2.17875246e+00 -4.81063178e+00]\n",
      " [-5.08422776e+00 -8.26051253e-01  1.92650555e+00 ... -4.88517641e-01\n",
      "  -3.88503622e+00 -5.65309293e+00]\n",
      " ...\n",
      " [ 6.86639603e-01 -1.37395637e+00 -7.12848777e-01 ... -1.02766345e+00\n",
      "   1.72764221e+00 -1.51801026e+00]\n",
      " [-7.24713802e-01 -1.97281706e+00 -7.91800441e-02 ...  4.73556021e-01\n",
      "  -1.36101533e-01 -1.70402435e+00]\n",
      " [-1.45804555e-02  1.91981553e-02 -9.81787696e-03 ...  1.57138002e-02\n",
      "   5.60989824e-03  7.86818079e-03]]\n",
      "numerical: -1.934346, analytic -1.934346, relative error: 2.017029e-10\n",
      "\n",
      "------------\n",
      "Gradient check : reg=1e-2\n",
      "------------\n",
      "\n",
      "[[-1.15102425e+00 -1.38232508e+00  6.87862109e-01 ...  2.15272107e-01\n",
      "  -8.03472877e-01 -4.73159510e+00]\n",
      " [-2.37910147e+00 -9.30252727e-01  6.88648187e-01 ... -2.84557086e-01\n",
      "  -2.17786880e+00 -4.79897882e+00]\n",
      " [-5.10624401e+00 -8.03158132e-01  1.94453608e+00 ... -5.07233905e-01\n",
      "  -3.89036470e+00 -5.64248818e+00]\n",
      " ...\n",
      " [ 6.94712857e-01 -1.35620205e+00 -7.41371442e-01 ... -1.01205700e+00\n",
      "   1.74980145e+00 -1.50743620e+00]\n",
      " [-7.28532841e-01 -1.97891728e+00 -1.02850982e-01 ...  4.43207580e-01\n",
      "  -1.41682572e-01 -1.69645921e+00]\n",
      " [-3.66985396e-03  4.75763318e-02 -2.49533204e-02 ...  4.35011353e-03\n",
      "   1.85885940e-02  4.40023628e-04]]\n",
      "[[-1.15102777e+00 -1.38232855e+00  6.87858753e-01 ...  2.15302502e-01\n",
      "  -8.03474600e-01 -4.73159856e+00]\n",
      " [-2.37910572e+00 -9.30256797e-01  6.88644303e-01 ... -2.84520886e-01\n",
      "  -2.17787119e+00 -4.79898325e+00]\n",
      " [-5.10625072e+00 -8.03164265e-01  1.94452999e+00 ... -5.07176251e-01\n",
      "  -3.89036938e+00 -5.64249530e+00]\n",
      " ...\n",
      " [ 6.94709922e-01 -1.35620430e+00 -7.41374942e-01 ... -1.01203481e+00\n",
      "   1.74979951e+00 -1.50743805e+00]\n",
      " [-7.28539108e-01 -1.97892305e+00 -1.02858000e-01 ...  4.43261536e-01\n",
      "  -1.41688027e-01 -1.69646456e+00]\n",
      " [-3.66981909e-03  4.75764439e-02 -2.49532197e-02 ...  4.34978243e-03\n",
      "   1.85886203e-02  4.40034412e-04]]\n",
      "[[-1.15102072e+00 -1.38232162e+00  6.87865465e-01 ...  2.15241712e-01\n",
      "  -8.03471154e-01 -4.73159165e+00]\n",
      " [-2.37909721e+00 -9.30248657e-01  6.88652072e-01 ... -2.84593285e-01\n",
      "  -2.17786641e+00 -4.79897438e+00]\n",
      " [-5.10623730e+00 -8.03152000e-01  1.94454218e+00 ... -5.07291557e-01\n",
      "  -3.89036003e+00 -5.64248105e+00]\n",
      " ...\n",
      " [ 6.94715793e-01 -1.35619981e+00 -7.41367942e-01 ... -1.01207919e+00\n",
      "   1.74980339e+00 -1.50743436e+00]\n",
      " [-7.28526575e-01 -1.97891151e+00 -1.02843964e-01 ...  4.43153625e-01\n",
      "  -1.41677117e-01 -1.69645386e+00]\n",
      " [-3.66988884e-03  4.75762197e-02 -2.49534212e-02 ...  4.35044488e-03\n",
      "   1.85885677e-02  4.40012817e-04]]\n",
      "numerical: 2.144203, analytic 2.118847, relative error: 5.948029e-03\n",
      "[[-1.15103977e+00 -1.38234168e+00  6.87844834e-01 ...  2.15426276e-01\n",
      "  -8.03489349e-01 -4.73161108e+00]\n",
      " [-2.37911968e+00 -9.30272308e-01  6.88627815e-01 ... -2.84375660e-01\n",
      "  -2.17788837e+00 -4.79899746e+00]\n",
      " [-5.10626796e+00 -8.03183454e-01  1.94450959e+00 ... -5.06998315e-01\n",
      "  -3.89038992e+00 -5.64251285e+00]\n",
      " ...\n",
      " [ 6.94707915e-01 -1.35620746e+00 -7.41376525e-01 ... -1.01200802e+00\n",
      "   1.74979655e+00 -1.50744155e+00]\n",
      " [-7.28541837e-01 -1.97892694e+00 -1.02860421e-01 ...  4.43295308e-01\n",
      "  -1.41691778e-01 -1.69646872e+00]\n",
      " [-3.66979830e-03  4.75763684e-02 -2.49533182e-02 ...  4.35008014e-03\n",
      "   1.85885422e-02  4.40107583e-04]]\n",
      "[[-1.15100872e+00 -1.38230849e+00  6.87879384e-01 ...  2.15117941e-01\n",
      "  -8.03456405e-01 -4.73157913e+00]\n",
      " [-2.37908325e+00 -9.30233146e-01  6.88668560e-01 ... -2.84738507e-01\n",
      "  -2.17784923e+00 -4.79896017e+00]\n",
      " [-5.10622006e+00 -8.03132810e-01  1.94456258e+00 ... -5.07469488e-01\n",
      "  -3.89033949e+00 -5.64246351e+00]\n",
      " ...\n",
      " [ 6.94717800e-01 -1.35619664e+00 -7.41366359e-01 ... -1.01210598e+00\n",
      "   1.74980635e+00 -1.50743086e+00]\n",
      " [-7.28523846e-01 -1.97890762e+00 -1.02841543e-01 ...  4.43119853e-01\n",
      "  -1.41673366e-01 -1.69644970e+00]\n",
      " [-3.66990964e-03  4.75762951e-02 -2.49533226e-02 ...  4.35014722e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1.85886457e-02  4.39939642e-04]]\n",
      "numerical: 0.429259, analytic 0.418279, relative error: 1.295509e-02\n",
      "[[-1.15104273e+00 -1.38234522e+00  6.87838495e-01 ...  2.15250509e-01\n",
      "  -8.03500018e-01 -4.73161549e+00]\n",
      " [-2.37911826e+00 -9.30271518e-01  6.88626157e-01 ... -2.84577110e-01\n",
      "  -2.17789359e+00 -4.79899742e+00]\n",
      " [-5.10626157e+00 -8.03177608e-01  1.94451376e+00 ... -5.07254264e-01\n",
      "  -3.89039056e+00 -5.64250758e+00]\n",
      " ...\n",
      " [ 6.94704373e-01 -1.35621067e+00 -7.41380701e-01 ... -1.01206490e+00\n",
      "   1.74978857e+00 -1.50744342e+00]\n",
      " [-7.28542457e-01 -1.97892721e+00 -1.02861891e-01 ...  4.43198266e-01\n",
      "  -1.41697630e-01 -1.69646759e+00]\n",
      " [-3.66987538e-03  4.75762760e-02 -2.49534205e-02 ...  4.35004782e-03\n",
      "   1.85884006e-02  4.40008217e-04]]\n",
      "[[-1.15100576e+00 -1.38230495e+00  6.87885723e-01 ...  2.15293704e-01\n",
      "  -8.03445736e-01 -4.73157472e+00]\n",
      " [-2.37908467e+00 -9.30233936e-01  6.88670217e-01 ... -2.84537062e-01\n",
      "  -2.17784401e+00 -4.79896021e+00]\n",
      " [-5.10622645e+00 -8.03138657e-01  1.94455841e+00 ... -5.07213546e-01\n",
      "  -3.89033884e+00 -5.64246878e+00]\n",
      " ...\n",
      " [ 6.94721341e-01 -1.35619343e+00 -7.41362183e-01 ... -1.01204909e+00\n",
      "   1.74981433e+00 -1.50742899e+00]\n",
      " [-7.28523226e-01 -1.97890735e+00 -1.02840074e-01 ...  4.43216894e-01\n",
      "  -1.41667514e-01 -1.69645083e+00]\n",
      " [-3.66983256e-03  4.75763875e-02 -2.49532203e-02 ...  4.35017921e-03\n",
      "   1.85887874e-02  4.40039010e-04]]\n",
      "numerical: 1.328649, analytic 1.316572, relative error: 4.565548e-03\n",
      "[[-1.15103743e+00 -1.38233953e+00  6.87847479e-01 ...  2.15407031e-01\n",
      "  -8.03487461e-01 -4.73161084e+00]\n",
      " [-2.37911563e+00 -9.30268772e-01  6.88632027e-01 ... -2.84410976e-01\n",
      "  -2.17788465e+00 -4.79899562e+00]\n",
      " [-5.10626045e+00 -8.03176320e-01  1.94451764e+00 ... -5.07066948e-01\n",
      "  -3.89038299e+00 -5.64250697e+00]\n",
      " ...\n",
      " [ 6.94707668e-01 -1.35620766e+00 -7.41377952e-01 ... -1.01200203e+00\n",
      "   1.74979588e+00 -1.50744374e+00]\n",
      " [-7.28539526e-01 -1.97892461e+00 -1.02859182e-01 ...  4.43277658e-01\n",
      "  -1.41690246e-01 -1.69646797e+00]\n",
      " [-3.66980499e-03  4.75763753e-02 -2.49533022e-02 ...  4.34995992e-03\n",
      "   1.85885503e-02  4.40114918e-04]]\n",
      "[[-1.15101106e+00 -1.38231064e+00  6.87876740e-01 ...  2.15137183e-01\n",
      "  -8.03458293e-01 -4.73157936e+00]\n",
      " [-2.37908730e+00 -9.30236681e-01  6.88664348e-01 ... -2.84703195e-01\n",
      "  -2.17785295e+00 -4.79896202e+00]\n",
      " [-5.10622757e+00 -8.03139944e-01  1.94455453e+00 ... -5.07400861e-01\n",
      "  -3.89034642e+00 -5.64246938e+00]\n",
      " ...\n",
      " [ 6.94718047e-01 -1.35619644e+00 -7.41364932e-01 ... -1.01211197e+00\n",
      "   1.74980702e+00 -1.50742867e+00]\n",
      " [-7.28526157e-01 -1.97890995e+00 -1.02842782e-01 ...  4.43137502e-01\n",
      "  -1.41674897e-01 -1.69645045e+00]\n",
      " [-3.66990295e-03  4.75762883e-02 -2.49533387e-02 ...  4.35026739e-03\n",
      "   1.85886377e-02  4.39932311e-04]]\n",
      "numerical: -0.733814, analytic -0.746621, relative error: 8.651049e-03\n",
      "[[-1.15098444e+00 -1.38232910e+00  6.87858205e-01 ...  2.15267287e-01\n",
      "  -8.03476249e-01 -4.73160023e+00]\n",
      " [-2.37905198e+00 -9.30257792e-01  6.88642835e-01 ... -2.84563349e-01\n",
      "  -2.17787321e+00 -4.79898483e+00]\n",
      " [-5.10616433e+00 -8.03166019e-01  1.94452727e+00 ... -5.07243708e-01\n",
      "  -3.89037211e+00 -5.64249805e+00]\n",
      " ...\n",
      " [ 6.94712344e-01 -1.35620248e+00 -7.41371741e-01 ... -1.01205782e+00\n",
      "   1.74980238e+00 -1.50743647e+00]\n",
      " [-7.28509126e-01 -1.97892053e+00 -1.02853992e-01 ...  4.43204269e-01\n",
      "  -1.41684483e-01 -1.69646219e+00]\n",
      " [-3.66998866e-03  4.75763680e-02 -2.49532911e-02 ...  4.35012583e-03\n",
      "   1.85886020e-02  4.40055611e-04]]\n",
      "[[-1.15106405e+00 -1.38232106e+00  6.87866014e-01 ...  2.15276927e-01\n",
      "  -8.03469504e-01 -4.73158998e+00]\n",
      " [-2.37915095e+00 -9.30247662e-01  6.88653540e-01 ... -2.84550823e-01\n",
      "  -2.17786439e+00 -4.79897281e+00]\n",
      " [-5.10632369e+00 -8.03150245e-01  1.94454490e+00 ... -5.07224102e-01\n",
      "  -3.89035729e+00 -5.64247831e+00]\n",
      " ...\n",
      " [ 6.94713373e-01 -1.35620162e+00 -7.41371143e-01 ... -1.01205618e+00\n",
      "   1.74980052e+00 -1.50743594e+00]\n",
      " [-7.28556555e-01 -1.97891403e+00 -1.02847973e-01 ...  4.43210891e-01\n",
      "  -1.41680660e-01 -1.69645623e+00]\n",
      " [-3.66971898e-03  4.75762955e-02 -2.49533498e-02 ...  4.35010120e-03\n",
      "   1.85885860e-02  4.39991616e-04]]\n",
      "numerical: -2.672237, analytic -2.664659, relative error: 1.419958e-03\n",
      "[[-1.15103444e+00 -1.38233604e+00  6.87852759e-01 ...  2.15368003e-01\n",
      "  -8.03481718e-01 -4.73160670e+00]\n",
      " [-2.37911007e+00 -9.30262158e-01  6.88640562e-01 ... -2.84476979e-01\n",
      "  -2.17787563e+00 -4.79898910e+00]\n",
      " [-5.10625136e+00 -8.03166338e-01  1.94453004e+00 ... -5.07167742e-01\n",
      "  -3.89036988e+00 -5.64249695e+00]\n",
      " ...\n",
      " [ 6.94687687e-01 -1.35622755e+00 -7.41397246e-01 ... -1.01182127e+00\n",
      "   1.74977665e+00 -1.50746327e+00]\n",
      " [-7.28555619e-01 -1.97894015e+00 -1.02874008e-01 ...  4.43420531e-01\n",
      "  -1.41705181e-01 -1.69648331e+00]\n",
      " [-3.66980213e-03  4.75764505e-02 -2.49531848e-02 ...  4.34942139e-03\n",
      "   1.85886317e-02  4.40217073e-04]]\n",
      "[[-1.15101405e+00 -1.38231413e+00  6.87871459e-01 ...  2.15176211e-01\n",
      "  -8.03464036e-01 -4.73158351e+00]\n",
      " [-2.37909286e+00 -9.30243296e-01  6.88655813e-01 ... -2.84637192e-01\n",
      "  -2.17786198e+00 -4.79896854e+00]\n",
      " [-5.10623667e+00 -8.03149926e-01  1.94454212e+00 ... -5.07300067e-01\n",
      "  -3.89035952e+00 -5.64247941e+00]\n",
      " ...\n",
      " [ 6.94738027e-01 -1.35617655e+00 -7.41345637e-01 ... -1.01229273e+00\n",
      "   1.74982625e+00 -1.50740914e+00]\n",
      " [-7.28510064e-01 -1.97889441e+00 -1.02827957e-01 ...  4.42994632e-01\n",
      "  -1.41659963e-01 -1.69643511e+00]\n",
      " [-3.66990581e-03  4.75762130e-02 -2.49534561e-02 ...  4.35080596e-03\n",
      "   1.85885562e-02  4.39830152e-04]]\n",
      "numerical: -2.315728, analytic -2.315709, relative error: 4.089361e-06\n",
      "[[-1.15105153e+00 -1.38235310e+00  6.87833431e-01 ...  2.15243860e-01\n",
      "  -8.03499514e-01 -4.73133653e+00]\n",
      " [-2.37912721e+00 -9.30279920e-01  6.88621095e-01 ... -2.84583906e-01\n",
      "  -2.17789383e+00 -4.79873407e+00]\n",
      " [-5.10626931e+00 -8.03185050e-01  1.94450930e+00 ... -5.07260540e-01\n",
      "  -3.89038925e+00 -5.64224665e+00]\n",
      " ...\n",
      " [ 6.94701455e-01 -1.35621351e+00 -7.41381685e-01 ... -1.01206861e+00\n",
      "   1.74979084e+00 -1.50733429e+00]\n",
      " [-7.28544791e-01 -1.97892913e+00 -1.02861780e-01 ...  4.43195291e-01\n",
      "  -1.41693710e-01 -1.69635214e+00]\n",
      " [-3.66972765e-03  4.75764497e-02 -2.49532069e-02 ...  4.35023893e-03\n",
      "   1.85886208e-02  4.39240942e-04]]\n",
      "[[-1.15099696e+00 -1.38229707e+00  6.87890788e-01 ...  2.15300354e-01\n",
      "  -8.03446239e-01 -4.73185368e+00]\n",
      " [-2.37907572e+00 -9.30225533e-01  6.88675281e-01 ... -2.84530265e-01\n",
      "  -2.17784377e+00 -4.79922356e+00]\n",
      " [-5.10621871e+00 -8.03131214e-01  1.94456287e+00 ... -5.07207269e-01\n",
      "  -3.89034015e+00 -5.64272971e+00]\n",
      " ...\n",
      " [ 6.94724260e-01 -1.35619059e+00 -7.41361198e-01 ... -1.01204539e+00\n",
      "   1.74981206e+00 -1.50753812e+00]\n",
      " [-7.28520891e-01 -1.97890543e+00 -1.02840185e-01 ...  4.43219870e-01\n",
      "  -1.41671433e-01 -1.69656628e+00]\n",
      " [-3.66998029e-03  4.75762139e-02 -2.49534340e-02 ...  4.34998810e-03\n",
      "   1.85885671e-02  4.40806583e-04]]\n",
      "numerical: -0.723798, analytic -0.732380, relative error: 5.893131e-03\n",
      "[[-1.15103840e+00 -1.38233881e+00  6.87849578e-01 ...  2.15259344e-01\n",
      "  -8.03488555e-01 -4.73160906e+00]\n",
      " [-2.37911332e+00 -9.30264385e-01  6.88638049e-01 ... -2.84567417e-01\n",
      "  -2.17788188e+00 -4.79899084e+00]\n",
      " [-5.10625380e+00 -8.03167668e-01  1.94452785e+00 ... -5.07242486e-01\n",
      "  -3.89037621e+00 -5.64249774e+00]\n",
      " ...\n",
      " [ 6.94684903e-01 -1.35622787e+00 -7.41395937e-01 ... -1.01208177e+00\n",
      "   1.74977325e+00 -1.50746209e+00]\n",
      " [-7.28558203e-01 -1.97894027e+00 -1.02872792e-01 ...  4.43185265e-01\n",
      "  -1.41708917e-01 -1.69648188e+00]\n",
      " [-3.66988428e-03  4.75763537e-02 -2.49533045e-02 ...  4.35016167e-03\n",
      "   1.85885074e-02  4.40120107e-04]]\n",
      "[[-1.15101009e+00 -1.38231135e+00  6.87874640e-01 ...  2.15284870e-01\n",
      "  -8.03457199e-01 -4.73158114e+00]\n",
      " [-2.37908962e+00 -9.30241069e-01  6.88658326e-01 ... -2.84546755e-01\n",
      "  -2.17785572e+00 -4.79896680e+00]\n",
      " [-5.10623423e+00 -8.03148597e-01  1.94454431e+00 ... -5.07225323e-01\n",
      "  -3.89035320e+00 -5.64247862e+00]\n",
      " ...\n",
      " [ 6.94740811e-01 -1.35617623e+00 -7.41346947e-01 ... -1.01203223e+00\n",
      "   1.74982965e+00 -1.50741031e+00]\n",
      " [-7.28507481e-01 -1.97889429e+00 -1.02829173e-01 ...  4.43229895e-01\n",
      "  -1.41656227e-01 -1.69643654e+00]\n",
      " [-3.66982367e-03  4.75763098e-02 -2.49533363e-02 ...  4.35006537e-03\n",
      "   1.85886805e-02  4.39927119e-04]]\n",
      "numerical: 1.671133, analytic 1.672000, relative error: 2.592749e-04\n",
      "[[-1.15102792e+00 -1.38233156e+00  6.87854979e-01 ...  2.15264873e-01\n",
      "  -8.03478599e-01 -4.73160123e+00]\n",
      " [-2.37910726e+00 -9.30261785e-01  6.88638361e-01 ... -2.84566781e-01\n",
      "  -2.17787725e+00 -4.79898700e+00]\n",
      " [-5.10625387e+00 -8.03171328e-01  1.94452153e+00 ... -5.07248328e-01\n",
      "  -3.89037768e+00 -5.64250041e+00]\n",
      " ...\n",
      " [ 6.94712874e-01 -1.35620398e+00 -7.41373230e-01 ... -1.01205920e+00\n",
      "   1.74980123e+00 -1.50743745e+00]\n",
      " [-7.28534546e-01 -1.97892117e+00 -1.02854873e-01 ...  4.43203312e-01\n",
      "  -1.41685003e-01 -1.69646236e+00]\n",
      " [-3.66983741e-03  4.75763292e-02 -2.49533182e-02 ...  4.35011137e-03\n",
      "   1.85885793e-02  4.40064610e-04]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.15102057e+00 -1.38231860e+00  6.87869239e-01 ...  2.15279340e-01\n",
      "  -8.03467155e-01 -4.73158898e+00]\n",
      " [-2.37909568e+00 -9.30243669e-01  6.88658013e-01 ... -2.84547392e-01\n",
      "  -2.17786035e+00 -4.79897063e+00]\n",
      " [-5.10623415e+00 -8.03144936e-01  1.94455063e+00 ... -5.07219483e-01\n",
      "  -3.89035173e+00 -5.64247595e+00]\n",
      " ...\n",
      " [ 6.94712840e-01 -1.35620013e+00 -7.41369654e-01 ... -1.01205480e+00\n",
      "   1.74980167e+00 -1.50743496e+00]\n",
      " [-7.28531137e-01 -1.97891338e+00 -1.02847092e-01 ...  4.43211849e-01\n",
      "  -1.41680141e-01 -1.69645606e+00]\n",
      " [-3.66987053e-03  4.75763343e-02 -2.49533227e-02 ...  4.35011566e-03\n",
      "   1.85886087e-02  4.39982619e-04]]\n",
      "numerical: 3.039471, analytic 3.033089, relative error: 1.050968e-03\n",
      "[[-1.15105343e+00 -1.38235546e+00  6.87830735e-01 ...  2.15556311e-01\n",
      "  -8.03504365e-01 -4.73162586e+00]\n",
      " [-2.37912902e+00 -9.30281580e-01  6.88618356e-01 ... -2.84289923e-01\n",
      "  -2.17789826e+00 -4.79900802e+00]\n",
      " [-5.10627159e+00 -8.03186749e-01  1.94450640e+00 ... -5.06967511e-01\n",
      "  -3.89039442e+00 -5.64251692e+00]\n",
      " ...\n",
      " [ 6.94702429e-01 -1.35621163e+00 -7.41382270e-01 ... -1.01195918e+00\n",
      "   1.74979105e+00 -1.50744729e+00]\n",
      " [-7.28544014e-01 -1.97892743e+00 -1.02862062e-01 ...  4.43311693e-01\n",
      "  -1.41693982e-01 -1.69647064e+00]\n",
      " [-3.66980336e-03  4.75763746e-02 -2.49532886e-02 ...  4.35004540e-03\n",
      "   1.85885166e-02  4.40120866e-04]]\n",
      "[[-1.15099506e+00 -1.38229471e+00  6.87893483e-01 ...  2.14987904e-01\n",
      "  -8.03441389e-01 -4.73156435e+00]\n",
      " [-2.37907392e+00 -9.30223874e-01  6.88678019e-01 ... -2.84824247e-01\n",
      "  -2.17783934e+00 -4.79894962e+00]\n",
      " [-5.10621643e+00 -8.03129515e-01  1.94456577e+00 ... -5.07500296e-01\n",
      "  -3.89033499e+00 -5.64245943e+00]\n",
      " ...\n",
      " [ 6.94723286e-01 -1.35619247e+00 -7.41360614e-01 ... -1.01215481e+00\n",
      "   1.74981185e+00 -1.50742512e+00]\n",
      " [-7.28521669e-01 -1.97890713e+00 -1.02839903e-01 ...  4.43103468e-01\n",
      "  -1.41671162e-01 -1.69644778e+00]\n",
      " [-3.66990458e-03  4.75762890e-02 -2.49533522e-02 ...  4.35018195e-03\n",
      "   1.85886714e-02  4.39926358e-04]]\n",
      "numerical: 0.128786, analytic 0.133042, relative error: 1.625582e-02\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# Maintenant testons le gradient numérique avec et sans régularisation       #\n",
    "# Les erreurs relatives devraient être inférieures à 1e-6                    #\n",
    "##############################################################################\n",
    "from utils.gradients import check_gradient_sparse\n",
    "\n",
    "print(\"\\n------------\\nGradient check : reg=0\\n------------\\n\")\n",
    "check_gradient_sparse(softmax_ce_naive_forward_backward, W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# Faire un autre test de gradients avec régularisation \n",
    "print(\"\\n------------\\nGradient check : reg=1e-2\\n------------\\n\")\n",
    "check_gradient_sparse(softmax_ce_naive_forward_backward, W, X_dev, y_dev, 1e2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax et gradients vectorisés\n",
    "Passons maintenant aux choses sérieuses. Vous devez ici coder la version vectorisée de l'entropie croisée et du gradient dans la fonction **softmax_ce_forward_backward**.  Ce code s'apparente à la réponse que vous avec donné au début."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.18351115e+00 -1.37008996e+00  6.98425543e-01 ...  2.30496243e-01\n",
      "  -8.09853658e-01 -4.72660770e+00]\n",
      " [-2.40834362e+00 -8.89049917e-01  6.95096531e-01 ... -2.66999919e-01\n",
      "  -2.17871308e+00 -4.81063512e+00]\n",
      " [-5.08423163e+00 -8.26052604e-01  1.92650427e+00 ... -4.88518518e-01\n",
      "  -3.88500694e+00 -5.65309529e+00]\n",
      " ...\n",
      " [ 6.86631952e-01 -1.37396260e+00 -7.12855295e-01 ... -1.02767013e+00\n",
      "   1.72770673e+00 -1.51801513e+00]\n",
      " [-7.24721081e-01 -1.97282266e+00 -7.91860206e-02 ...  4.73549653e-01\n",
      "  -1.36039080e-01 -1.70402906e+00]\n",
      " [-1.45804809e-02  1.91981861e-02 -9.81783918e-03 ...  1.57138365e-02\n",
      "   5.61016220e-03  7.86815069e-03]]\n",
      "naive loss: 2.365727e+00 computed in 0.199234s\n",
      "vectorized loss: 2.365727e+00 computed in 0.010970s\n",
      "bravo pour la loss!\n",
      "Loss difference: 0.000000\n",
      "il y a un bug au niveau du gradient\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# TODO: Implémenter le calcul de perte et du gradient de façon vectorielle   #\n",
    "# dans la fonction softmax_ce_forward_backward située dans le fichier        #\n",
    "# utils.loss.                                                                #\n",
    "# Les deux versions devraient calculer les mêmes résultats, mais la version  #\n",
    "# vectorielle devrait être BEAUCOUP PLUS RAPIDE.                             #\n",
    "##############################################################################\n",
    "start = time.time()\n",
    "loss_naive, grad_naive = softmax_ce_naive_forward_backward(X_dev, W, y_dev, 0.00001)\n",
    "end = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, end - start))\n",
    "\n",
    "from utils.loss import softmax_ce_forward_backward\n",
    "start = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_ce_forward_backward(X_dev, W, y_dev, 0.00001)\n",
    "end = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, end - start))\n",
    "\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "loss_diff = np.abs(loss_naive - loss_vectorized)\n",
    "if loss_diff < 1e-7:\n",
    "    print('bravo pour la loss!')\n",
    "else:\n",
    "    print('il y a un bug au niveau de la loss')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "\n",
    "if grad_difference < 1e-7:\n",
    "    print('bravo pour le gradient !')\n",
    "else:\n",
    "    print('il y a un bug au niveau du gradient')\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraînement\n",
    "À l'aide de la classe **LinearClassifier** ainsi que de la fonction vectorisée **softmax_ce_forward_backward** que vous venez de coder, vous devez maintenant entraîner un réseau de neurones multiclasses linéaire à l'aide d'une **descente de gradient**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train vs val acc 0.280490 / 0.296000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from model.LinearClassifier import LinearClassifier\n",
    "from visualization.utils import visualize_loss\n",
    "import itertools as it\n",
    "lr = 1e-7\n",
    "reg = 1\n",
    "\n",
    "classifier = LinearClassifier(softmax_ce_forward_backward)\n",
    "#\n",
    "# TODO : ajouter code à la fonction train.  Si tout fonctionne bien, la courbe de la loss devrait décroitre\n",
    "#\n",
    "train_loss_history = classifier.train(X_train, y_train, learning_rate=lr, reg=reg, num_iter=3000, verbose = False)\n",
    "\n",
    "y_train_pred = classifier.predict(X_train)\n",
    "y_val_pred = classifier.predict(X_val)\n",
    "\n",
    "acc_train = np.mean(y_train == y_train_pred)\n",
    "acc_val = np.mean(y_val == y_val_pred)\n",
    "\n",
    "print('train vs val acc %f / %f' %(acc_train, acc_val))\n",
    "\n",
    "visualize_loss(train_loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recherche d'hyper-paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best validation accuracy achieved during cross-validation: -1.000000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x, y, and format string must not be None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-d638f9c15d06>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'best validation accuracy achieved during cross-validation: %f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbest_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mvisualize_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_loss_history\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\TP Réseaux de neurones\\TP1\\TP1_RN\\visualization\\utils.py\u001b[0m in \u001b[0;36mvisualize_loss\u001b[1;34m(loss_history, y_label, x_label, title, infos, save)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m111\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mautoscale_on\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_history\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m     \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_ylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1663\u001b[0m         \"\"\"\n\u001b[0;32m   1664\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_alias_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1665\u001b[1;33m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1666\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1667\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    223\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m                 \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m             \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[1;34m(self, tup, kwargs)\u001b[0m\n\u001b[0;32m    375\u001b[0m         \u001b[1;31m# downstream.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 377\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"x, y, and format string must not be None\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    378\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m         \u001b[0mkw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: x, y, and format string must not be None"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from model.LinearClassifier import LinearClassifier\n",
    "from visualization.utils import visualize_loss\n",
    "import itertools as it\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = np.linspace(1e-7, 1e-5, 5)\n",
    "regularization_strengths = np.linspace(1e3, 1e7, 5)\n",
    "best_loss_history = None\n",
    "best_classifier = None\n",
    "\n",
    "##############################################################################\n",
    "# TODO: Utilisez l'ensemble de validation pour régler les hyper-paramètres   #\n",
    "#  (force de régularisation et vitesse d'apprentissage). Vous devez          #\n",
    "#  expérimenter différentes plages de valeurs pour les taux d'apprentissage  #\n",
    "#  et les forces de régularisation; si tout va bien, avec num_iter = 1000    #\n",
    "#  vous devriez obtenir une précision de classification supérieur à 0.38 sur #\n",
    "#  l'ensemble de validation, et de 0.37 sur l'ensemble de test.              #\n",
    "#  Mettre les résultats des meilleurs hyper-paramètres dans les variables    #\n",
    "#  best_XYZ ci haut.                                                         #\n",
    "##############################################################################\n",
    "\n",
    "\n",
    "################################################################################\n",
    "#                             FIN DE VOTRE CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)\n",
    "\n",
    "visualize_loss(best_loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On évalue le modèle sur l'ensemble de test\n",
    "y_test_pred = best_classifier.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('Test set accuracy: %f' % (test_accuracy) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des poids appris pour chaque classe\n",
    "w = best_classifier.W[:-1,:] # retire le biais\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "\n",
    "    # Redimensionne les poids pour qu'ils soient entre 0 et 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
