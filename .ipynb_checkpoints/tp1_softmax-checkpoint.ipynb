{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP1: Classifieur linéaire, fonction de perte **Entropie croisée**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from utils.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# pour automatiquement recharger les modules externes\n",
    "# voir http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10 Chargement des données et prétraitement\n",
    "\n",
    "### **TODO** assurez-vous d'exécuter le script *./get_datasets.sh* au moins une fois dans un terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500, num_batch=200):\n",
    "    \"\"\"\n",
    "    Charger la banque de données CIFAR-10, prétraiter les images et ajouter une dimension pour le biais.\n",
    "    \n",
    "    Input :\n",
    "    - num_training : nombre d'images à mettre dans l'ensemble d'entrainement\n",
    "    - num_validation : nombre d'images à mettre dans l'ensemble de validation\n",
    "    - num_test : nombre d'images à mettre dans l'ensemble de test\n",
    "    - num_dev : d'images à mettre dans l'ensemble dev\n",
    "    \n",
    "    Output :\n",
    "    - X_train, y_train : données et cibles d'entrainement\n",
    "    - X_val, y_val: données et cibles de validation\n",
    "    - X_test y_test: données et cibles de test \n",
    "    - X_dev, y_dev: données et cibles dev\n",
    "    - X_batch, y_batch: batch de données et de cibles \n",
    "    \"\"\"\n",
    "    # Charger les données CIFAR-10\n",
    "    cifar10_dir = 'datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "  \n",
    "    # Séparer en ensembles d'entraînement, de validation, de test et de dev\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    mask = range(num_batch)\n",
    "    X_batch = X_train[mask]\n",
    "    y_batch = y_train[mask]\n",
    "    \n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    X_batch = np.reshape(X_batch, (X_batch.shape[0], -1))\n",
    "\n",
    "    # Normalisation\n",
    "    X_train -= np.mean(X_train, axis = 0)\n",
    "    X_val -= np.mean(X_val, axis = 0)\n",
    "    X_test -= np.mean(X_test, axis = 0)\n",
    "    X_dev -= np.mean(X_dev, axis = 0)\n",
    "    X_batch -= np.mean(X_batch, axis = 0)\n",
    "\n",
    "    # Ajout du biais\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    X_batch = np.hstack([X_batch, np.ones((X_batch.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev, X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n",
      "batch data shape:  (200, 3073)\n",
      "batch labels shape:  (200,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev, X_batch, y_batch = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)\n",
    "print('batch data shape: ', X_batch.shape)\n",
    "print('batch labels shape: ', y_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "Les prochaines étapes consistent à implanter le calcul de **l'entropie croisée** et de son **gradient**.   Vous commencerez avec une version naïve impliquant une boucle *for* sur l'ensemble des éléments d'une batch pour ensuite implanter une version vectorisée.   Mais avant de commencer à coder, veuillez donner ici la formule de l'entropie croisée et du gradient pour une mini-batch de 500 données contenue dans le tableau\n",
    "\n",
    "$$X \\in R^{500\\times 3073}$$\n",
    "\n",
    "et une matrice de poids $$W \\in R^{3073\\times 10}$$ \n",
    "\n",
    "où 3073 est la dimensionnalité des données et 10 est le nombre de classes.\n",
    "\n",
    "**Votre Réponse:** \n",
    "\n",
    "$$Loss = -ln(S) + lamb*norm(W,2) avec S = exp(X.W) / sum(exp(X.W), 1)$$\n",
    "\n",
    "$$dW = [(S - t)*X].T + 2*lamb*W$$\n",
    "\n",
    "**NOTE IMPORTANT** : la réponse à cette question ne contient aucune boucle, seulement des multiplications matricielles et vectorielles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifieur linéaire SOFTMAX\n",
    "\n",
    "Le code pour cette section est dans le fichier **utils/loss.py**. \n",
    "\n",
    "La fonction `softmax_ce_naive_forward_backward` estime la perte (et le gradient) à l'aide de boucles `for` qui itèrent sur chaque donnée de la mini-batch.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commençons par tester la **forward pass + l'entropie croisée**.  Pour l'instant, ignorons la rétro-propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.55585138  5.77349909 -0.61236922 ... -0.67253637 -0.87537157\n",
      "  -0.6617125 ]\n",
      " [ 0.17387348 -1.80598346  0.19155258 ...  0.21037321  0.27382122\n",
      "   0.20698745]\n",
      " [-0.28636859  2.97444401 -0.31548597 ... -0.34648343 -0.45098192\n",
      "  -0.34090709]\n",
      " ...\n",
      " [-0.61429695  6.38055963 -0.67675741 ... -0.7432509  -0.96741342\n",
      "  -0.73128895]\n",
      " [ 0.04694006 -0.48755546  0.05171283 ...  0.05679377  0.07392262\n",
      "   0.05587973]\n",
      " [ 0.30834142 -3.20267064  0.33969294 ...  0.37306882  0.48558539\n",
      "   0.36706461]]\n",
      "Bravo!\n",
      "loss error: 0.000000\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# TODO: Implémenter le calcul de perte de façon naive avec des boucles dans  #\n",
    "#  la fonction softmax_ce_naive_forward_backward située dans le fichier      #\n",
    "#  utils.loss.                                                               #\n",
    "#  On commence par UNE image et UNE cible                                    #\n",
    "##############################################################################\n",
    "\n",
    "from utils.loss import softmax_ce_naive_forward_backward\n",
    "import time\n",
    "\n",
    "# Matrice de poids aléatoires + 1 donnée à tester\n",
    "np.random.seed(0)\n",
    "W = np.random.randn(3073, 10) * 5e-4\n",
    "X_rnd = np.random.randn(1, 3073) * 5\n",
    "y_rnd = np.uint32(np.ones(1))\n",
    "loss, _ = softmax_ce_naive_forward_backward(X_rnd, W, y_rnd, 0.0)\n",
    "\n",
    "# La loss suivante est celle que vous devriez obtenir\n",
    "target_loss = 2.276854\n",
    "loss_error = np.abs(loss - target_loss)\n",
    "if loss_error < 1e-5:\n",
    "    print(\"Bravo!\")\n",
    "else:\n",
    "    print(\"Il y a un bug...\")\n",
    "print('loss error: %f' % loss_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.63553662 -0.45649559 -1.05686044 ...  0.86572232 -2.16513188\n",
      "  -2.27858858]\n",
      " [-2.1048624   0.28366624 -1.39517522 ...  0.52994945 -3.74530299\n",
      "  -3.17648768]\n",
      " [-2.93143808 -0.49561605  0.06348929 ...  0.40347084 -5.83713235\n",
      "  -4.95945962]\n",
      " ...\n",
      " [ 0.28318078  2.29331036  0.42042742 ... -0.80513258 -0.17775923\n",
      "  -2.32193285]\n",
      " [ 0.17581504  2.03569961  0.67592454 ...  1.3667073  -1.99752957\n",
      "  -2.84210116]\n",
      " [ 0.02817192 -0.0407199  -0.01173696 ... -0.00808671  0.02431986\n",
      "  -0.00981905]]\n",
      "Bravo!\n",
      "loss error: 0.000000\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# TODO: Implémenter le calcul de perte de façon naive avec des boucles dans  #\n",
    "#  la fonction softmax_ce_naive_forward_backward située dans le fichier      #\n",
    "#  utils.loss.                                                               #\n",
    "#  Maintenant on test avec N=200 images et autant de cibles                  #\n",
    "##############################################################################\n",
    "\n",
    "from utils.loss import softmax_ce_naive_forward_backward\n",
    "import time\n",
    "\n",
    "# Matrice de poids aléatoires + 500 données \n",
    "np.random.seed(0)\n",
    "W = np.random.randn(3073, 10) * 1e-4\n",
    "loss, _ = softmax_ce_naive_forward_backward(X_batch, W, y_batch, 0.0)\n",
    "\n",
    "target_loss = 2.356459\n",
    "loss_error = np.abs(loss - target_loss)\n",
    "if loss_error < 1e-5:\n",
    "    print(\"Bravo!\")\n",
    "else:\n",
    "    print(\"Il y a un bug...\")\n",
    "print('loss error: %f' % loss_error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2:\n",
    "Pourquoi s'attend-on que la loss soit approximativement -np.log(1/nb_classes))?\n",
    "\n",
    "**Votre réponse:** Lorsque l'on initialise aléatoirement les poids, nous savons que l'entropie sera maximale. Pour cela, la prédiction (correspondant ici à une probabilité) associée à chaque classe sera de 1/nb_classes. Vu différemment, nous pouvons voir que, si le modèle n'est pas entraîné, il va prédire de façon égale chaque classe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.43098977e+00 -1.20913328e+00 -6.81027658e-01 ...  6.86160793e-01\n",
      "  -1.24796336e-01 -3.19055346e+00]\n",
      " [-2.81695360e+00 -5.80283642e-01 -1.11202598e+00 ...  4.46219742e-01\n",
      "  -1.48431681e+00 -4.08483887e+00]\n",
      " [-3.79122796e+00 -1.28590230e+00  3.25074256e-01 ...  5.54251616e-01\n",
      "  -3.36434630e+00 -6.11138620e+00]\n",
      " ...\n",
      " [-4.60994269e-01  1.33338845e+00  8.34242035e-01 ... -1.34774259e+00\n",
      "   1.31663061e+00 -9.06365651e-01]\n",
      " [-6.58748327e-01  1.08145126e+00  9.29722438e-01 ...  1.07841747e+00\n",
      "  -3.09743356e-01 -1.59750900e+00]\n",
      " [ 3.00797539e-02 -4.02401683e-02 -1.10506968e-02 ... -6.03864912e-03\n",
      "   2.59833688e-02 -1.36873808e-02]]\n",
      "Softmax loss: 2.339132\n",
      "Sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "###################################################################################\n",
    "#  Vérification simple: s'assurer que l'entropie-croisée soit proche de           #\n",
    "#  -log(1/nb_classes)                                                             #\n",
    "###################################################################################\n",
    "\n",
    "from utils.loss import softmax_ce_naive_forward_backward\n",
    "import time\n",
    "\n",
    "# Matrice de poids aléatoires + 500 données \n",
    "W = np.random.randn(3073, 10) * 1e-4\n",
    "loss, _ = softmax_ce_naive_forward_backward(X_batch, W, y_batch, 0.0)\n",
    "\n",
    "# La loss d'un modèle non-entrainé devrait s'approcher de -log(0.1).\n",
    "print('Softmax loss: %f' % loss)\n",
    "print('Sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rétro-propagation\n",
    "\n",
    "Maintenant, passons à la **rétro-propagation**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.1279744   1.15786877 -0.12847105 ... -0.12895357 -0.13032041\n",
      "  -0.12886991]\n",
      " [ 0.04003112 -0.36218796  0.04018648 ...  0.04033741  0.04076497\n",
      "   0.04031124]\n",
      " [-0.06593102  0.59652141 -0.06618689 ... -0.06643548 -0.06713966\n",
      "  -0.06639238]\n",
      " ...\n",
      " [-0.1414304   1.27961408 -0.14197927 ... -0.14251253 -0.14402308\n",
      "  -0.14242007]\n",
      " [ 0.01080707 -0.0977787   0.01084901 ...  0.01088976  0.01100519\n",
      "   0.01088269]\n",
      " [ 0.07098985 -0.64229201  0.07126535 ...  0.07153302  0.07229123\n",
      "   0.07148661]]\n",
      "Bravo pour la loss!\n",
      "loss error: 0.000000\n",
      "Bravo pour le gradient!\n",
      "gradient error 0.000000\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# TODO: Implémenter le calcul de perte ET DE LA RÉTROPROPAGATION de façon    #\n",
    "#       naive avec des boucles dans la fonction                              #\n",
    "#       softmax_ce_naive_forward_backward située dans le fichier utils.loss  #\n",
    "#                                                                            #\n",
    "#  On commence par UNE image et UNE cible                                    #\n",
    "##############################################################################\n",
    "\n",
    "from utils.loss import softmax_ce_naive_forward_backward\n",
    "import time\n",
    "\n",
    "# Matrice de poids aléatoires + une donnée\n",
    "np.random.seed(0)\n",
    "W = np.random.randn(3073, 10) * 1e-4\n",
    "X_rnd = np.random.randn(1, 3073)\n",
    "y_rnd = np.uint32(np.ones(1))\n",
    "loss, dW = softmax_ce_naive_forward_backward(X_rnd, W, y_rnd, 0.0)\n",
    "\n",
    "# La loss suivante est celle que vous devriez obtenir\n",
    "target_loss = 2.30114875\n",
    "loss_error = np.abs(loss - target_loss)\n",
    "if loss_error < 1e-5:\n",
    "    print(\"Bravo pour la loss!\")\n",
    "else:\n",
    "    print(\"Il y a un bug au niveau de la loss...\")\n",
    "print('loss error: %f' % loss_error)\n",
    "\n",
    "# Le gradient suivant est celui que vous devriez obtenir pour les 3 premiers poids\n",
    "target_dW = np.array([-0.1279744 ,  1.15786877, -0.12847105])\n",
    "dW_error = np.mean(np.abs(dW[0,0:3]-target_dW))\n",
    "if dW_error < 1e-7:\n",
    "    print(\"Bravo pour le gradient!\")\n",
    "else:\n",
    "    print(\"Il y a un bug au niveau du gradient...\")\n",
    "print('gradient error %f' % dW_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.68817739e+00 -1.37204569e+00 -4.61587730e-01 ...  8.34288820e-01\n",
      "  -9.34764094e-01 -4.93670879e+00]\n",
      " [-2.06141741e+00 -9.99790842e-01 -7.64298623e-01 ...  5.35426798e-01\n",
      "  -2.35436909e+00 -6.23144698e+00]\n",
      " [-2.86517804e+00 -1.82217642e+00  7.03969046e-01 ...  4.17467420e-01\n",
      "  -4.13619756e+00 -8.48146735e+00]\n",
      " ...\n",
      " [ 2.33613191e-01  1.53139723e+00  2.61028583e-01 ... -1.30382203e+00\n",
      "   5.89147437e-01 -2.84850674e+00]\n",
      " [ 1.22175450e-01  1.20625036e+00  4.12379388e-01 ...  7.80103824e-01\n",
      "  -9.49227030e-01 -3.60700249e+00]\n",
      " [ 2.80292466e-02 -4.27386483e-02 -1.21210217e-02 ... -7.08090406e-03\n",
      "   2.31554188e-02 -1.50200384e-02]]\n",
      "[-1.68817739 -1.37204569 -0.46158773  1.9206649 ]\n",
      "Bravo pour la loss!\n",
      "loss error: 0.000000\n",
      "Bravo pour le gradient!\n",
      "gradient error 0.000000\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# TODO: Implémenter le calcul de perte ET DE LA RÉTROPROPAGATION de façon    #\n",
    "#       naive avec des boucles dans la fonction                              #\n",
    "#       softmax_ce_naive_forward_backward située dans le fichier utils.loss  #\n",
    "#                                                                            #\n",
    "#  Maintenant on test avec N=200 images et autant de cibles                  #\n",
    "##############################################################################\n",
    "\n",
    "from utils.loss import softmax_ce_naive_forward_backward\n",
    "import time\n",
    "\n",
    "# Matrice de poids aléatoires + 500 données\n",
    "np.random.seed(1)\n",
    "W = np.random.randn(3073, 10) * 1e-4\n",
    "loss, dW = softmax_ce_naive_forward_backward(X_batch, W, y_batch, 0.0)\n",
    "print(dW[0,0:4])\n",
    "# La loss suivante est celle que vous devriez obtenir\n",
    "target_loss = 2.35680883\n",
    "loss_error = np.abs(loss - target_loss)\n",
    "if loss_error < 1e-5:\n",
    "    print(\"Bravo pour la loss!\")\n",
    "else:\n",
    "    print(\"Il y a un bug au niveau de la loss...\")\n",
    "print('loss error: %f' % loss_error)\n",
    "\n",
    "# Le gradient suivant est celui que vous devriez obtenir pour les 3 premiers poids\n",
    "target_dW = np.array([-1.68817739, -1.37204569, -0.46158773, 1.9206649])\n",
    "dW_error = np.mean(np.abs(dW[0,0:4]-target_dW))\n",
    "if dW_error < 1e-7:\n",
    "    print(\"Bravo pour le gradient!\")\n",
    "else:\n",
    "    print(\"Il y a un bug au niveau du gradient...\")\n",
    "print('gradient error %f' % dW_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encore quelques vérifications d'usage\n",
    "En principe, à ce point-ci, le calcul de l'entropie croisée (et de son gradient) via la fonction *softmax_ce_naive_forward_backward* devrait fonctionner.  Mais avant de passer à la prochaine étape il nous reste deux vérifications à faire : s'assurer que la **régularisation** fonctionne et passer le teste du **gradient numérique**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.20889157e+00  8.55946783e-01  4.33642890e-01 ... -2.30310464e+00\n",
      "  -7.39824397e-01 -5.40602462e+00]\n",
      " [-2.33256154e+00  1.18290672e+00 -3.43496299e-01 ... -1.62099142e+00\n",
      "  -2.14529413e+00 -5.69620283e+00]\n",
      " [-4.29754230e+00  1.57679042e+00  1.35943946e+00 ... -1.65769056e+00\n",
      "  -4.24488399e+00 -7.26521658e+00]\n",
      " ...\n",
      " [ 2.87871036e-01 -1.00032530e+00 -1.10916642e+00 ... -7.64116247e-01\n",
      "   2.00131136e+00 -2.51598538e+00]\n",
      " [-1.04512376e+00 -1.10959924e+00 -3.83294753e-01 ...  3.43349189e-02\n",
      "   5.74772718e-01 -3.38503665e+00]\n",
      " [ 2.78182276e-02  5.32359514e-03 -2.18825315e-02 ...  9.49655403e-03\n",
      "  -5.95555110e-03 -2.82670427e-02]]\n",
      "[[-1.17640466e+00  8.43711654e-01  4.23079455e-01 ... -2.31832878e+00\n",
      "  -7.33443615e-01 -5.41101203e+00]\n",
      " [-2.30331938e+00  1.14170391e+00 -3.49944643e-01 ... -1.63854859e+00\n",
      "  -2.14444985e+00 -5.68454653e+00]\n",
      " [-4.31955468e+00  1.59968489e+00  1.37747127e+00 ... -1.67640595e+00\n",
      "  -4.25024175e+00 -7.25460947e+00]\n",
      " ...\n",
      " [ 2.95951942e-01 -9.82564743e-01 -1.13768257e+00 ... -7.48503117e-01\n",
      "   2.02340607e+00 -2.50540646e+00]\n",
      " [-1.04893552e+00 -1.11569385e+00 -4.06959717e-01 ...  3.99284279e-03\n",
      "   5.69129226e-01 -3.37746680e+00]\n",
      " [ 3.87288556e-02  3.37017436e-02 -3.70180142e-02 ... -1.86717006e-03\n",
      "   7.02288200e-03 -3.56951705e-02]]\n",
      "2.3900819193943943\n",
      "[[-1.16990728e+00  8.41264629e-01  4.20966768e-01 ... -2.32137360e+00\n",
      "  -7.32167459e-01 -5.41200951e+00]\n",
      " [-2.29747095e+00  1.13346334e+00 -3.51234312e-01 ... -1.64206002e+00\n",
      "  -2.14428100e+00 -5.68221527e+00]\n",
      " [-4.32395716e+00  1.60426379e+00  1.38107763e+00 ... -1.68014903e+00\n",
      "  -4.25131330e+00 -7.25248805e+00]\n",
      " ...\n",
      " [ 2.97568123e-01 -9.79012632e-01 -1.14338580e+00 ... -7.45380490e-01\n",
      "   2.02782502e+00 -2.50329067e+00]\n",
      " [-1.04969787e+00 -1.11691278e+00 -4.11692710e-01 ... -2.07557244e-03\n",
      "   5.68000528e-01 -3.37595283e+00]\n",
      " [ 4.09109812e-02  3.93773733e-02 -4.00451108e-02 ... -4.13991488e-03\n",
      "   9.61856862e-03 -3.71807960e-02]]\n",
      "2.393149924246508\n",
      "[[-1.16211042e+00  8.38328198e-01  4.18431543e-01 ... -2.32502740e+00\n",
      "  -7.30636071e-01 -5.41320649e+00]\n",
      " [-2.29045283e+00  1.12357467e+00 -3.52781914e-01 ... -1.64627375e+00\n",
      "  -2.14407837e+00 -5.67941775e+00]\n",
      " [-4.32924013e+00  1.60975846e+00  1.38540527e+00 ... -1.68464072e+00\n",
      "  -4.25259916e+00 -7.24994234e+00]\n",
      " ...\n",
      " [ 2.99507540e-01 -9.74750100e-01 -1.15022968e+00 ... -7.41633339e-01\n",
      "   2.03312775e+00 -2.50075173e+00]\n",
      " [-1.05061269e+00 -1.11837548e+00 -4.17372302e-01 ... -9.35767071e-03\n",
      "   5.66646090e-01 -3.37413607e+00]\n",
      " [ 4.35295319e-02  4.61881290e-02 -4.36776266e-02 ... -6.86720866e-03\n",
      "   1.27333926e-02 -3.89635467e-02]]\n",
      "2.396831530069045\n",
      "[[-1.15275419  0.83480448  0.41538927 ... -2.32941195 -0.72879841\n",
      "  -5.41464286]\n",
      " [-2.28203109  1.11170826 -0.35463904 ... -1.65133021 -2.14383522\n",
      "  -5.67606074]\n",
      " [-4.33557969  1.61635207  1.39059843 ... -1.69003075 -4.2541422\n",
      "  -7.24688749]\n",
      " ...\n",
      " [ 0.30183484 -0.96963506 -1.15844233 ... -0.73713676  2.03949103\n",
      "  -2.497705  ]\n",
      " [-1.05171048 -1.12013073 -0.42418781 ... -0.01809619  0.56502076\n",
      "  -3.37195595]\n",
      " [ 0.04667179  0.05436104 -0.04803665 ... -0.01013996  0.01647118\n",
      "  -0.04110285]]\n",
      "2.4012494570560885\n",
      "[[-1.14152672  0.83057602  0.41173855 ... -2.33467341 -0.72659321\n",
      "  -5.41636651]\n",
      " [-2.271925    1.09746857 -0.35686759 ... -1.65739797 -2.14354344\n",
      "  -5.67203232]\n",
      " [-4.34318717  1.6242644   1.39683023 ... -1.69649879 -4.25599384\n",
      "  -7.24322168]\n",
      " ...\n",
      " [ 0.3046276  -0.96349701 -1.16829751 ... -0.73174086  2.04712697\n",
      "  -2.49404893]\n",
      " [-1.05302782 -1.12223703 -0.43236642 ... -0.02858241  0.56307037\n",
      "  -3.36933981]\n",
      " [ 0.05044251  0.06416852 -0.05326747 ... -0.01406726  0.02095653\n",
      "  -0.04367001]]\n",
      "2.4065509694405414\n",
      "[[-1.12805375  0.82550187  0.40735768 ... -2.34098717 -0.72394697\n",
      "  -5.41843488]\n",
      " [-2.25979769  1.08038093 -0.35954184 ... -1.66467928 -2.1431933\n",
      "  -5.66719822]\n",
      " [-4.35231615  1.6337592   1.40430838 ... -1.70426044 -4.25821581\n",
      "  -7.2388227 ]\n",
      " ...\n",
      " [ 0.30797892 -0.95613136 -1.18012373 ... -0.72526578  2.05629009\n",
      "  -2.48966164]\n",
      " [-1.05460864 -1.12476459 -0.44218076 ... -0.04116588  0.5607299\n",
      "  -3.36620045]\n",
      " [ 0.05496736  0.07593751 -0.05954446 ... -0.01878003  0.02633894\n",
      "  -0.0467506 ]]\n",
      "2.4129127843018847\n",
      "[[-1.11188618  0.81941289  0.40210064 ... -2.34856367 -0.72077148\n",
      "  -5.42091694]\n",
      " [-2.24524492  1.05987578 -0.36275095 ... -1.67341685 -2.14277313\n",
      "  -5.66139729]\n",
      " [-4.36327092  1.64515295  1.41328216 ... -1.71357441 -4.26088218\n",
      "  -7.23354392]\n",
      " ...\n",
      " [ 0.31200049 -0.94729257 -1.19431519 ... -0.71749569  2.06728583\n",
      "  -2.48439689]\n",
      " [-1.05650561 -1.12779766 -0.45395796 ... -0.05626603  0.55792134\n",
      "  -3.3624332 ]\n",
      " [ 0.06039719  0.09006029 -0.06707684 ... -0.02443534  0.03279784\n",
      "  -0.05044731]]\n",
      "2.4205469621354965\n",
      "[[-1.0924851   0.81210611  0.39579219 ... -2.35765548 -0.7169609\n",
      "  -5.4238954 ]\n",
      " [-2.2277816   1.03526959 -0.36660188 ... -1.68390193 -2.14226894\n",
      "  -5.65443619]\n",
      " [-4.37641665  1.65882546  1.42405071 ... -1.72475118 -4.26408181\n",
      "  -7.22720939]\n",
      " ...\n",
      " [ 0.31682638 -0.93668602 -1.21134495 ... -0.70817158  2.08048073\n",
      "  -2.47807919]\n",
      " [-1.05878199 -1.13143735 -0.4680906  ... -0.07438623  0.55455107\n",
      "  -3.35791252]\n",
      " [ 0.06691298  0.10700763 -0.0761157  ... -0.03122172  0.04054852\n",
      "  -0.05488337]]\n",
      "2.429707975535831\n",
      "[[-1.06920381  0.80333797  0.38822205 ... -2.36856565 -0.7123882\n",
      "  -5.42746956]\n",
      " [-2.20682561  1.00574216 -0.371223   ... -1.69648403 -2.1416639\n",
      "  -5.64608286]\n",
      " [-4.39219152  1.67523247  1.43697296 ... -1.73816331 -4.26792138\n",
      "  -7.21960795]\n",
      " ...\n",
      " [ 0.32261745 -0.92395817 -1.23178065 ... -0.69698264  2.0963146\n",
      "  -2.47049795]\n",
      " [-1.06151363 -1.13580497 -0.48504977 ... -0.09613045  0.55050674\n",
      "  -3.35248769]\n",
      " [ 0.07473193  0.12734444 -0.08696234 ... -0.03936538  0.04984934\n",
      "  -0.06020663]]\n",
      "2.440701191616232\n",
      "[[-1.04126626  0.79281621  0.37913789 ... -2.38165785 -0.70690096\n",
      "  -5.43175855]\n",
      " [-2.18167842  0.97030925 -0.37676834 ... -1.71158256 -2.14093785\n",
      "  -5.63605886]\n",
      " [-4.41112136  1.69492088  1.45247966 ... -1.75425785 -4.27252886\n",
      "  -7.21048622]\n",
      " ...\n",
      " [ 0.32956673 -0.90868474 -1.2563035  ... -0.68355592  2.11531525\n",
      "  -2.46140046]\n",
      " [-1.06479161 -1.14104612 -0.50540077 ... -0.12222353  0.54565354\n",
      "  -3.3459779 ]\n",
      " [ 0.08411467  0.15174861 -0.0999783  ... -0.04913777  0.06101031\n",
      "  -0.06659455]]\n",
      "2.4538930509127135\n",
      "Bravo!\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# Vérifions que le fait d'augmenter le terme de régularisation L2            #\n",
    "# augmente la loss...                                                        #\n",
    "##############################################################################\n",
    "success = True\n",
    "\n",
    "# Matrice de poids aléatoires + 500 données\n",
    "np.random.seed(1)\n",
    "W = np.random.randn(3073, 10) * 1e-4\n",
    "prev_loss, _ = softmax_ce_naive_forward_backward(X_dev, W, y_dev, 0.0)\n",
    "\n",
    "reg = 1e2\n",
    "for i in range(10):\n",
    "    loss, _ = softmax_ce_naive_forward_backward(X_dev, W, y_dev, reg)\n",
    "    print(loss)\n",
    "    if loss <= prev_loss:\n",
    "        success = False\n",
    "    prev_loss = loss\n",
    "    reg *= 1.2\n",
    "    \n",
    "if success:\n",
    "    print(\"Bravo!\")\n",
    "else:\n",
    "    print('Erreur!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------\n",
      "Gradient check : reg=0\n",
      "------------\n",
      "\n",
      "[[-1.20889157e+00  8.55946783e-01  4.33642890e-01 ... -2.30310464e+00\n",
      "  -7.39824397e-01 -5.40602462e+00]\n",
      " [-2.33256154e+00  1.18290672e+00 -3.43496299e-01 ... -1.62099142e+00\n",
      "  -2.14529413e+00 -5.69620283e+00]\n",
      " [-4.29754230e+00  1.57679042e+00  1.35943946e+00 ... -1.65769056e+00\n",
      "  -4.24488399e+00 -7.26521658e+00]\n",
      " ...\n",
      " [ 2.87871036e-01 -1.00032530e+00 -1.10916642e+00 ... -7.64116247e-01\n",
      "   2.00131136e+00 -2.51598538e+00]\n",
      " [-1.04512376e+00 -1.10959924e+00 -3.83294753e-01 ...  3.43349189e-02\n",
      "   5.74772718e-01 -3.38503665e+00]\n",
      " [ 2.78182276e-02  5.32359514e-03 -2.18825315e-02 ...  9.49655403e-03\n",
      "  -5.95555110e-03 -2.82670427e-02]]\n",
      "[[-1.20889085e+00  8.55948076e-01  4.33635381e-01 ... -2.30310288e+00\n",
      "  -7.39823943e-01 -5.40602332e+00]\n",
      " [-2.33256156e+00  1.18290715e+00 -3.43498568e-01 ... -1.62099015e+00\n",
      "  -2.14529408e+00 -5.69620220e+00]\n",
      " [-4.29754378e+00  1.57678940e+00  1.35945058e+00 ... -1.65769063e+00\n",
      "  -4.24488523e+00 -7.26521747e+00]\n",
      " ...\n",
      " [ 2.87869796e-01 -1.00032633e+00 -1.10915965e+00 ... -7.64116281e-01\n",
      "   2.00130924e+00 -2.51598423e+00]\n",
      " [-1.04512579e+00 -1.10960096e+00 -3.83279579e-01 ...  3.43340489e-02\n",
      "   5.74769876e-01 -3.38503630e+00]\n",
      " [ 2.78182529e-02  5.32364047e-03 -2.18827929e-02 ...  9.49658593e-03\n",
      "  -5.95552384e-03 -2.82670058e-02]]\n",
      "[[-1.20889229e+00  8.55945489e-01  4.33650400e-01 ... -2.30310639e+00\n",
      "  -7.39824852e-01 -5.40602592e+00]\n",
      " [-2.33256152e+00  1.18290629e+00 -3.43494028e-01 ... -1.62099270e+00\n",
      "  -2.14529417e+00 -5.69620346e+00]\n",
      " [-4.29754081e+00  1.57679145e+00  1.35942834e+00 ... -1.65769049e+00\n",
      "  -4.24488274e+00 -7.26521568e+00]\n",
      " ...\n",
      " [ 2.87872276e-01 -1.00032426e+00 -1.10917319e+00 ... -7.64116212e-01\n",
      "   2.00131347e+00 -2.51598653e+00]\n",
      " [-1.04512173e+00 -1.10959751e+00 -3.83309927e-01 ...  3.43357891e-02\n",
      "   5.74775559e-01 -3.38503700e+00]\n",
      " [ 2.78182022e-02  5.32354978e-03 -2.18822699e-02 ...  9.49652212e-03\n",
      "  -5.95557838e-03 -2.82670795e-02]]\n",
      "numerical: 0.288061, analytic 0.288061, relative error: 6.854788e-10\n",
      "[[-1.20891206e+00  8.56136609e-01  4.33622828e-01 ... -2.30312418e+00\n",
      "  -7.39844603e-01 -5.40604450e+00]\n",
      " [-2.33258406e+00  1.18311780e+00 -3.43518773e-01 ... -1.62101341e+00\n",
      "  -2.14531629e+00 -5.69622554e+00]\n",
      " [-4.29756715e+00  1.57702519e+00  1.35941425e+00 ... -1.65771528e+00\n",
      "  -4.24490874e+00 -7.26524176e+00]\n",
      " ...\n",
      " [ 2.87863941e-01 -1.00026065e+00 -1.10917239e+00 ... -7.64122206e-01\n",
      "   2.00130432e+00 -2.51599241e+00]\n",
      " [-1.04513193e+00 -1.10952230e+00 -3.83302054e-01 ...  3.43278329e-02\n",
      "   5.74764666e-01 -3.38504520e+00]\n",
      " [ 2.78183066e-02  5.32322924e-03 -2.18824816e-02 ...  9.49657348e-03\n",
      "  -5.95558056e-03 -2.82669359e-02]]\n",
      "[[-1.20887107e+00  8.55756956e-01  4.33662952e-01 ... -2.30308510e+00\n",
      "  -7.39804192e-01 -5.40600474e+00]\n",
      " [-2.33253902e+00  1.18269564e+00 -3.43473824e-01 ... -1.62096944e+00\n",
      "  -2.14527197e+00 -5.69618012e+00]\n",
      " [-4.29751744e+00  1.57655565e+00  1.35946466e+00 ... -1.65766584e+00\n",
      "  -4.24485923e+00 -7.26519140e+00]\n",
      " ...\n",
      " [ 2.87878131e-01 -1.00038994e+00 -1.10916046e+00 ... -7.64110287e-01\n",
      "   2.00131839e+00 -2.51597836e+00]\n",
      " [-1.04511559e+00 -1.10967617e+00 -3.83287453e-01 ...  3.43420050e-02\n",
      "   5.74780770e-01 -3.38502810e+00]\n",
      " [ 2.78181485e-02  5.32396132e-03 -2.18825814e-02 ...  9.49653456e-03\n",
      "  -5.95552167e-03 -2.82671495e-02]]\n",
      "numerical: 0.642088, analytic 0.642088, relative error: 2.777532e-10\n",
      "[[-1.20890352e+00  8.55935258e-01  4.33631106e-01 ... -2.30299128e+00\n",
      "  -7.39835889e-01 -5.40603691e+00]\n",
      " [-2.33257299e+00  1.18289589e+00 -3.43507192e-01 ... -1.62088431e+00\n",
      "  -2.14530473e+00 -5.69621459e+00]\n",
      " [-4.29755291e+00  1.57678067e+00  1.35942943e+00 ... -1.65759007e+00\n",
      "  -4.24489398e+00 -7.26522764e+00]\n",
      " ...\n",
      " [ 2.87862276e-01 -1.00033324e+00 -1.10917478e+00 ... -7.64035364e-01\n",
      "   2.00130228e+00 -2.51599422e+00]\n",
      " [-1.04513080e+00 -1.10960568e+00 -3.83301358e-01 ...  3.44025894e-02\n",
      "   5.74764908e-01 -3.38504377e+00]\n",
      " [ 2.78182580e-02  5.32365644e-03 -2.18825262e-02 ...  9.49643030e-03\n",
      "  -5.95558419e-03 -2.82669698e-02]]\n",
      "[[-1.20887961e+00  8.55958307e-01  4.33654673e-01 ... -2.30321800e+00\n",
      "  -7.39812906e-01 -5.40601232e+00]\n",
      " [-2.33255008e+00  1.18291755e+00 -3.43485406e-01 ... -1.62109854e+00\n",
      "  -2.14528352e+00 -5.69619107e+00]\n",
      " [-4.29753168e+00  1.57680017e+00  1.35944948e+00 ... -1.65779105e+00\n",
      "  -4.24487399e+00 -7.26520551e+00]\n",
      " ...\n",
      " [ 2.87879796e-01 -1.00031735e+00 -1.10915807e+00 ... -7.64197129e-01\n",
      "   2.00132043e+00 -2.51597654e+00]\n",
      " [-1.04511672e+00 -1.10959280e+00 -3.83288148e-01 ...  3.42672486e-02\n",
      "   5.74780527e-01 -3.38502952e+00]\n",
      " [ 2.78181971e-02  5.32353381e-03 -2.18825368e-02 ...  9.49667801e-03\n",
      "  -5.95551803e-03 -2.82671155e-02]]\n",
      "numerical: -1.048708, analytic -1.048708, relative error: 7.989456e-10\n",
      "[[-1.20891208e+00  8.56133773e-01  4.33623036e-01 ... -2.30312446e+00\n",
      "  -7.39844210e-01 -5.40604568e+00]\n",
      " [-2.33258318e+00  1.18310430e+00 -3.43517028e-01 ... -1.62101225e+00\n",
      "  -2.14531469e+00 -5.69622550e+00]\n",
      " [-4.29756522e+00  1.57700010e+00  1.35941738e+00 ... -1.65771261e+00\n",
      "  -4.24490573e+00 -7.26524045e+00]\n",
      " ...\n",
      " [ 2.87859150e-01 -1.00022733e+00 -1.10917668e+00 ... -7.64125768e-01\n",
      "   2.00130097e+00 -2.51599766e+00]\n",
      " [-1.04513553e+00 -1.10950039e+00 -3.83305007e-01 ...  3.43254310e-02\n",
      "   5.74761862e-01 -3.38504873e+00]\n",
      " [ 2.78183091e-02  5.32325846e-03 -2.18825043e-02 ...  9.49658999e-03\n",
      "  -5.95559640e-03 -2.82669181e-02]]\n",
      "[[-1.20887105e+00  8.55759793e-01  4.33662743e-01 ... -2.30308482e+00\n",
      "  -7.39804586e-01 -5.40600356e+00]\n",
      " [-2.33253990e+00  1.18270914e+00 -3.43475570e-01 ... -1.62097060e+00\n",
      "  -2.14527356e+00 -5.69618017e+00]\n",
      " [-4.29751937e+00  1.57658074e+00  1.35946153e+00 ... -1.65766851e+00\n",
      "  -4.24486224e+00 -7.26519271e+00]\n",
      " ...\n",
      " [ 2.87882922e-01 -1.00042326e+00 -1.10915616e+00 ... -7.64106725e-01\n",
      "   2.00132174e+00 -2.51597311e+00]\n",
      " [-1.04511198e+00 -1.10969809e+00 -3.83284499e-01 ...  3.43444070e-02\n",
      "   5.74783574e-01 -3.38502457e+00]\n",
      " [ 2.78181460e-02  5.32393208e-03 -2.18825587e-02 ...  9.49651805e-03\n",
      "  -5.95550582e-03 -2.82671673e-02]]\n",
      "numerical: 1.213440, analytic 1.213440, relative error: 3.330754e-10\n",
      "[[-1.20888955e+00  8.55948670e-01  4.33628040e-01 ... -2.30310263e+00\n",
      "  -7.39820527e-01 -5.40602601e+00]\n",
      " [-2.33255854e+00  1.18290977e+00 -3.43520858e-01 ... -1.62098849e+00\n",
      "  -2.14528900e+00 -5.69620328e+00]\n",
      " [-4.29753875e+00  1.57679400e+00  1.35941015e+00 ... -1.65768690e+00\n",
      "  -4.24487814e+00 -7.26521675e+00]\n",
      " ...\n",
      " [ 2.87872008e-01 -1.00032474e+00 -1.10917058e+00 ... -7.64116824e-01\n",
      "   2.00131196e+00 -2.51598590e+00]\n",
      " [-1.04512139e+00 -1.10959739e+00 -3.83310695e-01 ...  3.43358692e-02\n",
      "   5.74774448e-01 -3.38503574e+00]\n",
      " [ 2.78182888e-02  5.32365725e-03 -2.18829940e-02 ...  9.49666090e-03\n",
      "  -5.95552908e-03 -2.82669996e-02]]\n",
      "[[-1.20889359e+00  8.55944895e-01  4.33657740e-01 ... -2.30310665e+00\n",
      "  -7.39828268e-01 -5.40602322e+00]\n",
      " [-2.33256454e+00  1.18290367e+00 -3.43471740e-01 ... -1.62099436e+00\n",
      "  -2.14529926e+00 -5.69620238e+00]\n",
      " [-4.29754584e+00  1.57678684e+00  1.35946876e+00 ... -1.65769422e+00\n",
      "  -4.24488983e+00 -7.26521641e+00]\n",
      " ...\n",
      " [ 2.87870064e-01 -1.00032585e+00 -1.10916227e+00 ... -7.64115669e-01\n",
      "   2.00131075e+00 -2.51598486e+00]\n",
      " [-1.04512612e+00 -1.10960109e+00 -3.83278812e-01 ...  3.43339688e-02\n",
      "   5.74770988e-01 -3.38503756e+00]\n",
      " [ 2.78181663e-02  5.32353300e-03 -2.18820687e-02 ...  9.49644714e-03\n",
      "  -5.95557314e-03 -2.82670858e-02]]\n",
      "numerical: -0.274271, analytic -0.274271, relative error: 3.103886e-09\n",
      "[[-1.20891485e+00  8.55922677e-01  4.33621037e-01 ... -2.30312577e+00\n",
      "  -7.39844354e-01 -5.40604956e+00]\n",
      " [-2.33258648e+00  1.18288064e+00 -3.43520338e-01 ... -1.62101443e+00\n",
      "  -2.14531552e+00 -5.69623038e+00]\n",
      " [-4.29757299e+00  1.57675877e+00  1.35940988e+00 ... -1.65771908e+00\n",
      "  -4.24491020e+00 -7.26524985e+00]\n",
      " ...\n",
      " [ 2.87858874e-01 -1.00033744e+00 -1.10917819e+00 ... -7.64127458e-01\n",
      "   2.00129972e+00 -2.51599811e+00]\n",
      " [-1.04514026e+00 -1.10961596e+00 -3.83311216e-01 ...  3.43190983e-02\n",
      "   5.74757023e-01 -3.38505428e+00]\n",
      " [ 2.78184307e-02  5.32380482e-03 -2.18823861e-02 ...  9.49671590e-03\n",
      "  -5.95549097e-03 -2.82667989e-02]]\n",
      "[[-1.20886828e+00  8.55970889e-01  4.33664743e-01 ... -2.30308351e+00\n",
      "  -7.39804441e-01 -5.40599968e+00]\n",
      " [-2.33253659e+00  1.18293280e+00 -3.43472259e-01 ... -1.62096842e+00\n",
      "  -2.14527273e+00 -5.69617528e+00]\n",
      " [-4.29751160e+00  1.57682207e+00  1.35946903e+00 ... -1.65766204e+00\n",
      "  -4.24485777e+00 -7.26518330e+00]\n",
      " ...\n",
      " [ 2.87883199e-01 -1.00031315e+00 -1.10915466e+00 ... -7.64105035e-01\n",
      "   2.00132299e+00 -2.51597265e+00]\n",
      " [-1.04510725e+00 -1.10958252e+00 -3.83278291e-01 ...  3.43507398e-02\n",
      "   5.74788412e-01 -3.38501902e+00]\n",
      " [ 2.78180244e-02  5.32338542e-03 -2.18826769e-02 ...  9.49639214e-03\n",
      "  -5.95561126e-03 -2.82672865e-02]]\n",
      "numerical: 0.415765, analytic 0.415765, relative error: 1.150022e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.20890933e+00  8.55929121e-01  4.33625017e-01 ... -2.30294091e+00\n",
      "  -7.39842587e-01 -5.40604168e+00]\n",
      " [-2.33258209e+00  1.18288578e+00 -3.43517489e-01 ... -1.62079840e+00\n",
      "  -2.14531498e+00 -5.69622378e+00]\n",
      " [-4.29756941e+00  1.57676272e+00  1.35941134e+00 ... -1.65743574e+00\n",
      "  -4.24491108e+00 -7.26524465e+00]\n",
      " ...\n",
      " [ 2.87865174e-01 -1.00033114e+00 -1.10917187e+00 ... -7.64062438e-01\n",
      "   2.00130604e+00 -2.51599184e+00]\n",
      " [-1.04513348e+00 -1.10960897e+00 -3.83304669e-01 ...  3.44259073e-02\n",
      "   5.74763850e-01 -3.38504764e+00]\n",
      " [ 2.78182637e-02  5.32362754e-03 -2.18825320e-02 ...  9.49654974e-03\n",
      "  -5.95562268e-03 -2.82669621e-02]]\n",
      "[[-1.20887380e+00  8.55964445e-01  4.33660762e-01 ... -2.30326837e+00\n",
      "  -7.39806209e-01 -5.40600755e+00]\n",
      " [-2.33254099e+00  1.18292766e+00 -3.43475109e-01 ... -1.62118444e+00\n",
      "  -2.14527328e+00 -5.69618189e+00]\n",
      " [-4.29751518e+00  1.57681812e+00  1.35946757e+00 ... -1.65794538e+00\n",
      "  -4.24485689e+00 -7.26518851e+00]\n",
      " ...\n",
      " [ 2.87876898e-01 -1.00031945e+00 -1.10916098e+00 ... -7.64170056e-01\n",
      "   2.00131667e+00 -2.51597892e+00]\n",
      " [-1.04511404e+00 -1.10958951e+00 -3.83284837e-01 ...  3.42439301e-02\n",
      "   5.74781586e-01 -3.38502566e+00]\n",
      " [ 2.78181914e-02  5.32356270e-03 -2.18825310e-02 ...  9.49655864e-03\n",
      "  -5.95547955e-03 -2.82671233e-02]]\n",
      "numerical: 0.279661, analytic 0.279661, relative error: 2.819420e-09\n",
      "[[-1.20889421e+00  8.55944478e-01  4.33640461e-01 ... -2.30310723e+00\n",
      "  -7.39824817e-01 -5.40602834e+00]\n",
      " [-2.33256238e+00  1.18290584e+00 -3.43496626e-01 ... -1.62099239e+00\n",
      "  -2.14529280e+00 -5.69620479e+00]\n",
      " [-4.29754153e+00  1.57679051e+00  1.35944018e+00 ... -1.65769038e+00\n",
      "  -4.24488133e+00 -7.26521730e+00]\n",
      " ...\n",
      " [ 2.87864511e-01 -1.00033139e+00 -1.10917226e+00 ... -7.64122459e-01\n",
      "   2.00130612e+00 -2.51599215e+00]\n",
      " [-1.04512880e+00 -1.10960415e+00 -3.83298951e-01 ...  3.43301259e-02\n",
      "   5.74769004e-01 -3.38504225e+00]\n",
      " [ 2.78182607e-02  5.32367268e-03 -2.18824296e-02 ...  9.49665772e-03\n",
      "  -5.95552412e-03 -2.82669601e-02]]\n",
      "[[-1.20888892e+00  8.55949088e-01  4.33645318e-01 ... -2.30310204e+00\n",
      "  -7.39823978e-01 -5.40602090e+00]\n",
      " [-2.33256070e+00  1.18290760e+00 -3.43495971e-01 ... -1.62099045e+00\n",
      "  -2.14529545e+00 -5.69620088e+00]\n",
      " [-4.29754306e+00  1.57679033e+00  1.35943873e+00 ... -1.65769074e+00\n",
      "  -4.24488664e+00 -7.26521586e+00]\n",
      " ...\n",
      " [ 2.87877561e-01 -1.00031920e+00 -1.10916058e+00 ... -7.64110035e-01\n",
      "   2.00131659e+00 -2.51597862e+00]\n",
      " [-1.04511871e+00 -1.10959432e+00 -3.83290556e-01 ...  3.43397120e-02\n",
      "   5.74776432e-01 -3.38503104e+00]\n",
      " [ 2.78181944e-02  5.32351757e-03 -2.18826334e-02 ...  9.49645032e-03\n",
      "  -5.95557811e-03 -2.82671253e-02]]\n",
      "numerical: -3.084558, analytic -3.084558, relative error: 1.250313e-10\n",
      "[[-1.20889784e+00  8.55940537e-01  4.33636961e-01 ... -2.30310963e+00\n",
      "  -7.39829341e-01 -5.40596959e+00]\n",
      " [-2.33256704e+00  1.18290133e+00 -3.43500926e-01 ... -1.62099543e+00\n",
      "  -2.14529801e+00 -5.69615625e+00]\n",
      " [-4.29754656e+00  1.57678676e+00  1.35943670e+00 ... -1.65769281e+00\n",
      "  -4.24488637e+00 -7.26518519e+00]\n",
      " ...\n",
      " [ 2.87861009e-01 -1.00033578e+00 -1.10917640e+00 ... -7.64126353e-01\n",
      "   2.00130254e+00 -2.51589479e+00]\n",
      " [-1.04513185e+00 -1.10960776e+00 -3.83302745e-01 ...  3.43267188e-02\n",
      "   5.74765648e-01 -3.38496296e+00]\n",
      " [ 2.78182893e-02  5.32370549e-03 -2.18824577e-02 ...  9.49665058e-03\n",
      "  -5.95552368e-03 -2.82675794e-02]]\n",
      "[[-1.20888530e+00  8.55953028e-01  4.33648818e-01 ... -2.30309964e+00\n",
      "  -7.39819454e-01 -5.40607965e+00]\n",
      " [-2.33255603e+00  1.18291211e+00 -3.43491671e-01 ... -1.62098741e+00\n",
      "  -2.14529024e+00 -5.69624941e+00]\n",
      " [-4.29753804e+00  1.57679408e+00  1.35944221e+00 ... -1.65768831e+00\n",
      "  -4.24488160e+00 -7.26524796e+00]\n",
      " ...\n",
      " [ 2.87881063e-01 -1.00031481e+00 -1.10915645e+00 ... -7.64106140e-01\n",
      "   2.00132017e+00 -2.51607598e+00]\n",
      " [-1.04511567e+00 -1.10959071e+00 -3.83286761e-01 ...  3.43431193e-02\n",
      "   5.74779788e-01 -3.38511034e+00]\n",
      " [ 2.78181658e-02  5.32348476e-03 -2.18826053e-02 ...  9.49645746e-03\n",
      "  -5.95557854e-03 -2.82665057e-02]]\n",
      "numerical: -0.102945, analytic -0.102945, relative error: 1.341210e-09\n",
      "[[-1.20889108e+00  8.55948193e-01  4.33644988e-01 ... -2.30310263e+00\n",
      "  -7.39828149e-01 -5.40602489e+00]\n",
      " [-2.33256001e+00  1.18290903e+00 -3.43492478e-01 ... -1.62098810e+00\n",
      "  -2.14530928e+00 -5.69620180e+00]\n",
      " [-4.29753974e+00  1.57679344e+00  1.35944416e+00 ... -1.65768628e+00\n",
      "  -4.24490649e+00 -7.26521452e+00]\n",
      " ...\n",
      " [ 2.87867206e-01 -1.00032968e+00 -1.10917022e+00 ... -7.64120302e-01\n",
      "   2.00135017e+00 -2.51598937e+00]\n",
      " [-1.04512664e+00 -1.10960286e+00 -3.83297390e-01 ...  3.43317942e-02\n",
      "   5.74803847e-01 -3.38503977e+00]\n",
      " [ 2.78182234e-02  5.32361176e-03 -2.18824794e-02 ...  9.49657934e-03\n",
      "  -5.95541225e-03 -2.82670685e-02]]\n",
      "[[-1.20889205e+00  8.55945372e-01  4.33640792e-01 ... -2.30310665e+00\n",
      "  -7.39820645e-01 -5.40602435e+00]\n",
      " [-2.33256307e+00  1.18290441e+00 -3.43500120e-01 ... -1.62099475e+00\n",
      "  -2.14527897e+00 -5.69620386e+00]\n",
      " [-4.29754485e+00  1.57678740e+00  1.35943475e+00 ... -1.65769484e+00\n",
      "  -4.24486147e+00 -7.26521864e+00]\n",
      " ...\n",
      " [ 2.87874866e-01 -1.00032091e+00 -1.10916262e+00 ... -7.64112192e-01\n",
      "   2.00127254e+00 -2.51598139e+00]\n",
      " [-1.04512087e+00 -1.10959562e+00 -3.83292116e-01 ...  3.43380437e-02\n",
      "   5.74741590e-01 -3.38503353e+00]\n",
      " [ 2.78182317e-02  5.32357849e-03 -2.18825836e-02 ...  9.49652870e-03\n",
      "  -5.95568967e-03 -2.82670169e-02]]\n",
      "numerical: -0.786645, analytic -0.786645, relative error: 1.039521e-09\n",
      "\n",
      "------------\n",
      "Gradient check : reg=1e-2\n",
      "------------\n",
      "\n",
      "[[-1.17640466e+00  8.43711654e-01  4.23079455e-01 ... -2.31832878e+00\n",
      "  -7.33443615e-01 -5.41101203e+00]\n",
      " [-2.30331938e+00  1.14170391e+00 -3.49944643e-01 ... -1.63854859e+00\n",
      "  -2.14444985e+00 -5.68454653e+00]\n",
      " [-4.31955468e+00  1.59968489e+00  1.37747127e+00 ... -1.67640595e+00\n",
      "  -4.25024175e+00 -7.25460947e+00]\n",
      " ...\n",
      " [ 2.95951942e-01 -9.82564743e-01 -1.13768257e+00 ... -7.48503117e-01\n",
      "   2.02340607e+00 -2.50540646e+00]\n",
      " [-1.04893552e+00 -1.11569385e+00 -4.06959717e-01 ...  3.99284279e-03\n",
      "   5.69129226e-01 -3.37746680e+00]\n",
      " [ 3.87288556e-02  3.37017436e-02 -3.70180142e-02 ... -1.86717006e-03\n",
      "   7.02288200e-03 -3.56951705e-02]]\n",
      "[[-1.17640287e+00  8.43714008e-01  4.23083353e-01 ... -2.31835065e+00\n",
      "  -7.33440335e-01 -5.41101053e+00]\n",
      " [-2.30331869e+00  1.14170529e+00 -3.49941873e-01 ... -1.63856055e+00\n",
      "  -2.14444776e+00 -5.68454578e+00]\n",
      " [-4.31955627e+00  1.59968433e+00  1.37747152e+00 ... -1.67639700e+00\n",
      "  -4.25024182e+00 -7.25461084e+00]\n",
      " ...\n",
      " [ 2.95949977e-01 -9.82568582e-01 -1.13768493e+00 ... -7.48483427e-01\n",
      "   2.02340376e+00 -2.50540747e+00]\n",
      " [-1.04894004e+00 -1.11570003e+00 -4.06964900e-01 ...  4.03725954e-03\n",
      "   5.69124344e-01 -3.37747073e+00]\n",
      " [ 3.87288725e-02  3.37018232e-02 -3.70179437e-02 ... -1.86733846e-03\n",
      "   7.02291347e-03 -3.56951944e-02]]\n",
      "[[-1.17640644e+00  8.43709300e-01  4.23075556e-01 ... -2.31830691e+00\n",
      "  -7.33446896e-01 -5.41101352e+00]\n",
      " [-2.30332007e+00  1.14170252e+00 -3.49947413e-01 ... -1.63853663e+00\n",
      "  -2.14445195e+00 -5.68454728e+00]\n",
      " [-4.31955309e+00  1.59968546e+00  1.37747102e+00 ... -1.67641489e+00\n",
      "  -4.25024168e+00 -7.25460810e+00]\n",
      " ...\n",
      " [ 2.95953907e-01 -9.82560904e-01 -1.13768021e+00 ... -7.48522808e-01\n",
      "   2.02340839e+00 -2.50540545e+00]\n",
      " [-1.04893100e+00 -1.11568768e+00 -4.06954534e-01 ...  3.94842463e-03\n",
      "   5.69134108e-01 -3.37746288e+00]\n",
      " [ 3.87288386e-02  3.37016640e-02 -3.70180848e-02 ... -1.86700140e-03\n",
      "   7.02285049e-03 -3.56951466e-02]]\n",
      "numerical: 0.941358, analytic 0.916001, relative error: 1.365205e-02\n",
      "[[-1.17641523e+00  8.43701013e-01  4.23067733e-01 ... -2.31822747e+00\n",
      "  -7.33453810e-01 -5.41102340e+00]\n",
      " [-2.30333349e+00  1.14168938e+00 -3.49960128e-01 ... -1.63841132e+00\n",
      "  -2.14446392e+00 -5.68456211e+00]\n",
      " [-4.31957460e+00  1.59966448e+00  1.37744968e+00 ... -1.67621300e+00\n",
      "  -4.25026173e+00 -7.25463135e+00]\n",
      " ...\n",
      " [ 2.95951952e-01 -9.82564203e-01 -1.13768027e+00 ... -7.48511601e-01\n",
      "   2.02340839e+00 -2.50540598e+00]\n",
      " [-1.04893865e+00 -1.11569630e+00 -4.06961079e-01 ...  4.01674127e-03\n",
      "   5.69128191e-01 -3.37747020e+00]\n",
      " [ 3.87289020e-02  3.37017594e-02 -3.70180309e-02 ... -1.86707908e-03\n",
      "   7.02282873e-03 -3.56951406e-02]]\n",
      "[[-1.17639409e+00  8.43722296e-01  4.23091177e-01 ... -2.31843008e+00\n",
      "  -7.33433421e-01 -5.41100065e+00]\n",
      " [-2.30330527e+00  1.14171843e+00 -3.49929159e-01 ... -1.63868586e+00\n",
      "  -2.14443579e+00 -5.68453095e+00]\n",
      " [-4.31953475e+00  1.59970531e+00  1.37749286e+00 ... -1.67659889e+00\n",
      "  -4.25022176e+00 -7.25458759e+00]\n",
      " ...\n",
      " [ 2.95951932e-01 -9.82565283e-01 -1.13768488e+00 ... -7.48494633e-01\n",
      "   2.02340376e+00 -2.50540693e+00]\n",
      " [-1.04893239e+00 -1.11569141e+00 -4.06958356e-01 ...  3.96894392e-03\n",
      "   5.69130261e-01 -3.37746340e+00]\n",
      " [ 3.87288092e-02  3.37017279e-02 -3.70179975e-02 ... -1.86726074e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   7.02293523e-03 -3.56952004e-02]]\n",
      "numerical: 0.084971, analytic 0.073991, relative error: 6.907225e-02\n",
      "[[-1.17642457e+00  8.43692738e-01  4.23056971e-01 ... -2.31834978e+00\n",
      "  -7.33468525e-01 -5.41103075e+00]\n",
      " [-2.30333656e+00  1.14168750e+00 -3.49963714e-01 ... -1.63856706e+00\n",
      "  -2.14447176e+00 -5.68456271e+00]\n",
      " [-4.31957137e+00  1.59966882e+00  1.37745325e+00 ... -1.67642386e+00\n",
      "  -4.25026300e+00 -7.25462508e+00]\n",
      " ...\n",
      " [ 2.95941055e-01 -9.82573172e-01 -1.13769238e+00 ... -7.48512159e-01\n",
      "   2.02339288e+00 -2.50541459e+00]\n",
      " [-1.04894540e+00 -1.11570133e+00 -4.06968519e-01 ...  3.98453588e-03\n",
      "   5.69116674e-01 -3.37747391e+00]\n",
      " [ 3.87287902e-02  3.37016711e-02 -3.70181192e-02 ... -1.86726006e-03\n",
      "   7.02269696e-03 -3.56952005e-02]]\n",
      "[[-1.17638475e+00  8.43730570e-01  4.23101937e-01 ... -2.31830778e+00\n",
      "  -7.33418707e-01 -5.41099330e+00]\n",
      " [-2.30330220e+00  1.14172032e+00 -3.49925573e-01 ... -1.63853013e+00\n",
      "  -2.14442794e+00 -5.68453035e+00]\n",
      " [-4.31953799e+00  1.59970096e+00  1.37748929e+00 ... -1.67638803e+00\n",
      "  -4.25022049e+00 -7.25459386e+00]\n",
      " ...\n",
      " [ 2.95962828e-01 -9.82556314e-01 -1.13767277e+00 ... -7.48494074e-01\n",
      "   2.02341927e+00 -2.50539832e+00]\n",
      " [-1.04892564e+00 -1.11568638e+00 -4.06950916e-01 ...  4.00114945e-03\n",
      "   5.69141778e-01 -3.37745969e+00]\n",
      " [ 3.87289210e-02  3.37018161e-02 -3.70179093e-02 ... -1.86708009e-03\n",
      "   7.02306700e-03 -3.56951405e-02]]\n",
      "numerical: 1.392644, analytic 1.380567, relative error: 4.354836e-03\n",
      "[[-1.17641977e+00  8.43696437e-01  4.23063832e-01 ... -2.31818508e+00\n",
      "  -7.33460455e-01 -5.41102654e+00]\n",
      " [-2.30333606e+00  1.14168706e+00 -3.49961793e-01 ... -1.63838986e+00\n",
      "  -2.14446805e+00 -5.68456310e+00]\n",
      " [-4.31957257e+00  1.59966643e+00  1.37745289e+00 ... -1.67623383e+00\n",
      "  -4.25026134e+00 -7.25462781e+00]\n",
      " ...\n",
      " [ 2.95944232e-01 -9.82572153e-01 -1.13769013e+00 ... -7.48433703e-01\n",
      "   2.02339796e+00 -2.50541432e+00]\n",
      " [-1.04894361e+00 -1.11570221e+00 -4.06968477e-01 ...  4.07124932e-03\n",
      "   5.69120150e-01 -3.37747586e+00]\n",
      " [ 3.87288879e-02  3.37017735e-02 -3.70180005e-02 ... -1.86721714e-03\n",
      "   7.02282766e-03 -3.56950911e-02]]\n",
      "[[-1.17638955e+00  8.43726872e-01  4.23095078e-01 ... -2.31847247e+00\n",
      "  -7.33426777e-01 -5.41099751e+00]\n",
      " [-2.30330270e+00  1.14172075e+00 -3.49927493e-01 ... -1.63870732e+00\n",
      "  -2.14443165e+00 -5.68452995e+00]\n",
      " [-4.31953679e+00  1.59970336e+00  1.37748965e+00 ... -1.67657807e+00\n",
      "  -4.25022215e+00 -7.25459113e+00]\n",
      " ...\n",
      " [ 2.95959652e-01 -9.82557333e-01 -1.13767501e+00 ... -7.48572530e-01\n",
      "   2.02341418e+00 -2.50539860e+00]\n",
      " [-1.04892742e+00 -1.11568550e+00 -4.06950958e-01 ...  3.91443635e-03\n",
      "   5.69138302e-01 -3.37745774e+00]\n",
      " [ 3.87288232e-02  3.37017138e-02 -3.70180280e-02 ... -1.86712273e-03\n",
      "   7.02293630e-03 -3.56952498e-02]]\n",
      "numerical: 0.416292, analytic 0.403485, relative error: 1.562291e-02\n",
      "[[-1.17643021e+00  8.43714984e-01  4.23082264e-01 ... -2.31832621e+00\n",
      "  -7.33441447e-01 -5.41100890e+00]\n",
      " [-2.30332883e+00  1.14170540e+00 -3.49943501e-01 ... -1.63854755e+00\n",
      "  -2.14444937e+00 -5.68454528e+00]\n",
      " [-4.31953492e+00  1.59968345e+00  1.37746921e+00 ... -1.67640831e+00\n",
      "  -4.25024423e+00 -7.25461162e+00]\n",
      " ...\n",
      " [ 2.95961685e-01 -9.82566219e-01 -1.13768337e+00 ... -7.48503898e-01\n",
      "   2.02340453e+00 -2.50540666e+00]\n",
      " [-1.04890646e+00 -1.11569701e+00 -4.06962730e-01 ...  3.98977587e-03\n",
      "   5.69125986e-01 -3.37746921e+00]\n",
      " [ 3.87288269e-02  3.37017664e-02 -3.70179788e-02 ... -1.86716125e-03\n",
      "   7.02288125e-03 -3.56951614e-02]]\n",
      "[[-1.17637911e+00  8.43708325e-01  4.23076645e-01 ... -2.31833134e+00\n",
      "  -7.33445785e-01 -5.41101516e+00]\n",
      " [-2.30330993e+00  1.14170241e+00 -3.49945785e-01 ... -1.63854964e+00\n",
      "  -2.14445033e+00 -5.68454777e+00]\n",
      " [-4.31957444e+00  1.59968633e+00  1.37747333e+00 ... -1.67640359e+00\n",
      "  -4.25023927e+00 -7.25460732e+00]\n",
      " ...\n",
      " [ 2.95942199e-01 -9.82563266e-01 -1.13768178e+00 ... -7.48502335e-01\n",
      "   2.02340762e+00 -2.50540626e+00]\n",
      " [-1.04896457e+00 -1.11569069e+00 -4.06956705e-01 ...  3.99590975e-03\n",
      "   5.69132466e-01 -3.37746439e+00]\n",
      " [ 3.87288845e-02  3.37017208e-02 -3.70180496e-02 ... -1.86717889e-03\n",
      "   7.02288271e-03 -3.56951795e-02]]\n",
      "numerical: -1.765782, analytic -1.758204, relative error: 2.150454e-03\n",
      "[[-1.17641259e+00  8.43706074e-01  4.23072532e-01 ... -2.31826042e+00\n",
      "  -7.33451293e-01 -5.41101988e+00]\n",
      " [-2.30332626e+00  1.14169954e+00 -3.49949509e-01 ... -1.63849354e+00\n",
      "  -2.14445588e+00 -5.68455297e+00]\n",
      " [-4.31956013e+00  1.59968256e+00  1.37746887e+00 ... -1.67636723e+00\n",
      "  -4.25024606e+00 -7.25461390e+00]\n",
      " ...\n",
      " [ 2.95928775e-01 -9.82588584e-01 -1.13770800e+00 ... -7.48283493e-01\n",
      "   2.02338148e+00 -2.50543144e+00]\n",
      " [-1.04895630e+00 -1.11571523e+00 -4.06982418e-01 ...  4.19136113e-03\n",
      "   5.69106826e-01 -3.37748897e+00]\n",
      " [ 3.87288819e-02  3.37018242e-02 -3.70179215e-02 ... -1.86760154e-03\n",
      "   7.02290366e-03 -3.56950256e-02]]\n",
      "[[-1.17639673e+00  8.43717234e-01  4.23086377e-01 ... -2.31839713e+00\n",
      "  -7.33435938e-01 -5.41100417e+00]\n",
      " [-2.30331250e+00  1.14170827e+00 -3.49939777e-01 ... -1.63860364e+00\n",
      "  -2.14444383e+00 -5.68454009e+00]\n",
      " [-4.31954923e+00  1.59968723e+00  1.37747367e+00 ... -1.67644467e+00\n",
      "  -4.25023743e+00 -7.25460503e+00]\n",
      " ...\n",
      " [ 2.95975109e-01 -9.82540902e-01 -1.13765715e+00 ... -7.48722740e-01\n",
      "   2.02343066e+00 -2.50538148e+00]\n",
      " [-1.04891473e+00 -1.11567248e+00 -4.06937017e-01 ...  3.79432508e-03\n",
      "   5.69151626e-01 -3.37744463e+00]\n",
      " [ 3.87288292e-02  3.37016630e-02 -3.70181069e-02 ... -1.86673833e-03\n",
      "   7.02286031e-03 -3.56953154e-02]]\n",
      "numerical: -1.146794, analytic -1.146775, relative error: 8.257103e-06\n",
      "[[-1.17643206e+00  8.43685172e-01  4.23053089e-01 ... -2.31835442e+00\n",
      "  -7.33467566e-01 -5.41076849e+00]\n",
      " [-2.30334487e+00  1.14167922e+00 -3.49968938e-01 ... -1.63857240e+00\n",
      "  -2.14447167e+00 -5.68432151e+00]\n",
      " [-4.31957983e+00  1.59966069e+00  1.37744768e+00 ... -1.67642939e+00\n",
      "  -4.25026322e+00 -7.25438834e+00]\n",
      " ...\n",
      " [ 2.95940855e-01 -9.82575270e-01 -1.13769238e+00 ... -7.48512751e-01\n",
      "   2.02339703e+00 -2.50531284e+00]\n",
      " [-1.04894582e+00 -1.11570357e+00 -4.06968463e-01 ...  3.98412150e-03\n",
      "   5.69120783e-01 -3.37738023e+00]\n",
      " [ 3.87289923e-02  3.37018701e-02 -3.70179186e-02 ... -1.86707590e-03\n",
      "   7.02289489e-03 -3.56959670e-02]]\n",
      "[[-1.17637726e+00  8.43738137e-01  4.23105821e-01 ... -2.31830313e+00\n",
      "  -7.33419666e-01 -5.41125557e+00]\n",
      " [-2.30329388e+00  1.14172860e+00 -3.49920347e-01 ... -1.63852478e+00\n",
      "  -2.14442803e+00 -5.68477155e+00]\n",
      " [-4.31952953e+00  1.59970910e+00  1.37749486e+00 ... -1.67638251e+00\n",
      "  -4.25022028e+00 -7.25483060e+00]\n",
      " ...\n",
      " [ 2.95963029e-01 -9.82554216e-01 -1.13767277e+00 ... -7.48493482e-01\n",
      "   2.02341512e+00 -2.50550008e+00]\n",
      " [-1.04892522e+00 -1.11568413e+00 -4.06950972e-01 ...  4.00156437e-03\n",
      "   5.69137669e-01 -3.37755337e+00]\n",
      " [ 3.87287188e-02  3.37016171e-02 -3.70181098e-02 ... -1.86726425e-03\n",
      "   7.02286908e-03 -3.56943737e-02]]\n",
      "numerical: -0.917646, analytic -0.926227, relative error: 4.654030e-03\n",
      "[[-1.17641529e+00  8.43702330e-01  4.23070998e-01 ... -2.31833781e+00\n",
      "  -7.33453012e-01 -5.41102203e+00]\n",
      " [-2.30332880e+00  1.14169597e+00 -3.49951664e-01 ... -1.63855614e+00\n",
      "  -2.14445748e+00 -5.68455500e+00]\n",
      " [-4.31956195e+00  1.59967960e+00  1.37746687e+00 ... -1.67641106e+00\n",
      "  -4.25024726e+00 -7.25461523e+00]\n",
      " ...\n",
      " [ 2.95927223e-01 -9.82588902e-01 -1.13770587e+00 ... -7.48526103e-01\n",
      "   2.02338160e+00 -2.50543082e+00]\n",
      " [-1.04895788e+00 -1.11571567e+00 -4.06980565e-01 ...  3.97214853e-03\n",
      "   5.69106891e-01 -3.37748832e+00]\n",
      " [ 3.87288656e-02  3.37018052e-02 -3.70179837e-02 ... -1.86711222e-03\n",
      "   7.02284749e-03 -3.56950572e-02]]\n",
      "[[-1.17639403e+00  8.43720979e-01  4.23087911e-01 ... -2.31831975e+00\n",
      "  -7.33434219e-01 -5.41100202e+00]\n",
      " [-2.30330996e+00  1.14171184e+00 -3.49937621e-01 ... -1.63854104e+00\n",
      "  -2.14444222e+00 -5.68453805e+00]\n",
      " [-4.31954741e+00  1.59969018e+00  1.37747567e+00 ... -1.67640084e+00\n",
      "  -4.25023623e+00 -7.25460371e+00]\n",
      " ...\n",
      " [ 2.95976661e-01 -9.82540584e-01 -1.13765928e+00 ... -7.48480130e-01\n",
      "   2.02343055e+00 -2.50538210e+00]\n",
      " [-1.04891315e+00 -1.11567203e+00 -4.06938870e-01 ...  4.01353727e-03\n",
      "   5.69151561e-01 -3.37744528e+00]\n",
      " [ 3.87288456e-02  3.37016820e-02 -3.70180447e-02 ... -1.86722793e-03\n",
      "   7.02291647e-03 -3.56952837e-02]]\n",
      "numerical: 0.972931, analytic 0.973798, relative error: 4.452536e-04\n",
      "[[-1.17640595e+00  8.43709506e-01  4.23079317e-01 ... -2.31832960e+00\n",
      "  -7.33444190e-01 -5.41101320e+00]\n",
      " [-2.30332466e+00  1.14169739e+00 -3.49949860e-01 ... -1.63855371e+00\n",
      "  -2.14445513e+00 -5.68455272e+00]\n",
      " [-4.31956462e+00  1.59967360e+00  1.37746056e+00 ... -1.67641614e+00\n",
      "  -4.25025256e+00 -7.25462099e+00]\n",
      " ...\n",
      " [ 2.95952112e-01 -9.82565748e-01 -1.13768283e+00 ... -7.48502509e-01\n",
      "   2.02340647e+00 -2.50540661e+00]\n",
      " [-1.04893873e+00 -1.11569833e+00 -4.06963843e-01 ...  3.99013823e-03\n",
      "   5.69125893e-01 -3.37747085e+00]\n",
      " [ 3.87288733e-02  3.37017551e-02 -3.70180010e-02 ... -1.86716503e-03\n",
      "   7.02287826e-03 -3.56951601e-02]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.17640337e+00  8.43713802e-01  4.23079592e-01 ... -2.31832795e+00\n",
      "  -7.33443042e-01 -5.41101085e+00]\n",
      " [-2.30331409e+00  1.14171042e+00 -3.49939427e-01 ... -1.63854347e+00\n",
      "  -2.14444458e+00 -5.68454034e+00]\n",
      " [-4.31954474e+00  1.59969619e+00  1.37748198e+00 ... -1.67639576e+00\n",
      "  -4.25023093e+00 -7.25459795e+00]\n",
      " ...\n",
      " [ 2.95951772e-01 -9.82563737e-01 -1.13768231e+00 ... -7.48503724e-01\n",
      "   2.02340568e+00 -2.50540631e+00]\n",
      " [-1.04893231e+00 -1.11568938e+00 -4.06955592e-01 ...  3.99554718e-03\n",
      "   5.69132559e-01 -3.37746275e+00]\n",
      " [ 3.87288379e-02  3.37017321e-02 -3.70180274e-02 ... -1.86717512e-03\n",
      "   7.02288570e-03 -3.56951809e-02]]\n",
      "numerical: 2.105342, analytic 2.098960, relative error: 1.517985e-03\n",
      "[[-1.17643628e+00  8.43679003e-01  4.23044490e-01 ... -2.31802138e+00\n",
      "  -7.33477670e-01 -5.41104601e+00]\n",
      " [-2.30334853e+00  1.14167348e+00 -3.49976771e-01 ... -1.63826503e+00\n",
      "  -2.14448119e+00 -5.68457809e+00]\n",
      " [-4.31958355e+00  1.59965465e+00  1.37743978e+00 ... -1.67612563e+00\n",
      "  -4.25027281e+00 -7.25464045e+00]\n",
      " ...\n",
      " [ 2.95942160e-01 -9.82573038e-01 -1.13769096e+00 ... -7.48417608e-01\n",
      "   2.02339701e+00 -2.50541670e+00]\n",
      " [-1.04894585e+00 -1.11570240e+00 -4.06968351e-01 ...  4.08246960e-03\n",
      "   5.69119645e-01 -3.37747769e+00]\n",
      " [ 3.87288692e-02  3.37017483e-02 -3.70180529e-02 ... -1.86694349e-03\n",
      "   7.02276526e-03 -3.56950939e-02]]\n",
      "[[-1.17637304e+00  8.43744305e-01  4.23114420e-01 ... -2.31863617e+00\n",
      "  -7.33409562e-01 -5.41097804e+00]\n",
      " [-2.30329023e+00  1.14173433e+00 -3.49912515e-01 ... -1.63883215e+00\n",
      "  -2.14441852e+00 -5.68451496e+00]\n",
      " [-4.31952580e+00  1.59971514e+00  1.37750276e+00 ... -1.67668626e+00\n",
      "  -4.25021069e+00 -7.25457849e+00]\n",
      " ...\n",
      " [ 2.95961724e-01 -9.82556448e-01 -1.13767419e+00 ... -7.48588625e-01\n",
      "   2.02341514e+00 -2.50539622e+00]\n",
      " [-1.04892518e+00 -1.11568531e+00 -4.06951084e-01 ...  3.90321513e-03\n",
      "   5.69138807e-01 -3.37745592e+00]\n",
      " [ 3.87288419e-02  3.37017389e-02 -3.70179755e-02 ... -1.86739632e-03\n",
      "   7.02299870e-03 -3.56952471e-02]]\n",
      "numerical: -1.370163, analytic -1.365907, relative error: 1.555599e-03\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# Maintenant testons le gradient numérique avec et sans régularisation       #\n",
    "# Les erreurs relatives devraient être inférieures à 1e-6                    #\n",
    "##############################################################################\n",
    "from utils.gradients import check_gradient_sparse\n",
    "\n",
    "print(\"\\n------------\\nGradient check : reg=0\\n------------\\n\")\n",
    "check_gradient_sparse(softmax_ce_naive_forward_backward, W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# Faire un autre test de gradients avec régularisation \n",
    "print(\"\\n------------\\nGradient check : reg=1e-2\\n------------\\n\")\n",
    "check_gradient_sparse(softmax_ce_naive_forward_backward, W, X_dev, y_dev, 1e2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax et gradients vectorisés\n",
    "Passons maintenant aux choses sérieuses. Vous devez ici coder la version vectorisée de l'entropie croisée et du gradient dans la fonction **softmax_ce_forward_backward**.  Ce code s'apparente à la réponse que vous avec donné au début."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.20889156e+00  8.55946781e-01  4.33642889e-01 ... -2.30310464e+00\n",
      "  -7.39824397e-01 -5.40602462e+00]\n",
      " [-2.33256153e+00  1.18290672e+00 -3.43496300e-01 ... -1.62099142e+00\n",
      "  -2.14529413e+00 -5.69620283e+00]\n",
      " [-4.29754230e+00  1.57679042e+00  1.35943946e+00 ... -1.65769056e+00\n",
      "  -4.24488399e+00 -7.26521658e+00]\n",
      " ...\n",
      " [ 2.87871037e-01 -1.00032529e+00 -1.10916643e+00 ... -7.64116245e-01\n",
      "   2.00131136e+00 -2.51598538e+00]\n",
      " [-1.04512376e+00 -1.10959924e+00 -3.83294755e-01 ...  3.43349159e-02\n",
      "   5.74772717e-01 -3.38503665e+00]\n",
      " [ 2.78182287e-02  5.32359798e-03 -2.18825330e-02 ...  9.49655290e-03\n",
      "  -5.95554980e-03 -2.82670434e-02]]\n",
      "naive loss: 2.374742e+00 computed in 0.176996s\n",
      "vectorized loss: 2.374742e+00 computed in 0.006005s\n",
      "bravo pour la loss!\n",
      "Loss difference: 0.000000\n",
      "il y a un bug au niveau du gradient\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# TODO: Implémenter le calcul de perte et du gradient de façon vectorielle   #\n",
    "# dans la fonction softmax_ce_forward_backward située dans le fichier        #\n",
    "# utils.loss.                                                                #\n",
    "# Les deux versions devraient calculer les mêmes résultats, mais la version  #\n",
    "# vectorielle devrait être BEAUCOUP PLUS RAPIDE.                             #\n",
    "##############################################################################\n",
    "start = time.time()\n",
    "loss_naive, grad_naive = softmax_ce_naive_forward_backward(X_dev, W, y_dev, 0.00001)\n",
    "end = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, end - start))\n",
    "\n",
    "from utils.loss import softmax_ce_forward_backward\n",
    "start = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_ce_forward_backward(X_dev, W, y_dev, 0.00001)\n",
    "end = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, end - start))\n",
    "\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "loss_diff = np.abs(loss_naive - loss_vectorized)\n",
    "if loss_diff < 1e-7:\n",
    "    print('bravo pour la loss!')\n",
    "else:\n",
    "    print('il y a un bug au niveau de la loss')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "\n",
    "if grad_difference < 1e-7:\n",
    "    print('bravo pour le gradient !')\n",
    "else:\n",
    "    print('il y a un bug au niveau du gradient')\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraînement\n",
    "À l'aide de la classe **LinearClassifier** ainsi que de la fonction vectorisée **softmax_ce_forward_backward** que vous venez de coder, vous devez maintenant entraîner un réseau de neurones multiclasses linéaire à l'aide d'une **descente de gradient**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n",
      "verbose\n"
     ]
    }
   ],
   "source": [
    "from model.LinearClassifier import LinearClassifier\n",
    "from visualization.utils import visualize_loss\n",
    "import itertools as it\n",
    "lr = 1e-7\n",
    "reg = 1\n",
    "\n",
    "classifier = LinearClassifier(softmax_ce_forward_backward)\n",
    "#\n",
    "# TODO : ajouter code à la fonction train.  Si tout fonctionne bien, la courbe de la loss devrait décroitre\n",
    "#\n",
    "train_loss_history = classifier.train(X_train, y_train, learning_rate=lr, reg=reg, num_iter=3000, verbose = False)\n",
    "\n",
    "y_train_pred = classifier.predict(X_train)\n",
    "y_val_pred = classifier.predict(X_val)\n",
    "\n",
    "acc_train = np.mean(y_train == y_train_pred)\n",
    "acc_val = np.mean(y_val == y_val_pred)\n",
    "\n",
    "print('train vs val acc %f / %f' %(acc_train, acc_val))\n",
    "\n",
    "visualize_loss(train_loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recherche d'hyper-paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.LinearClassifier import LinearClassifier\n",
    "from visualization.utils import visualize_loss\n",
    "import itertools as it\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = np.linspace(1e-7, 1e-5, 5)\n",
    "regularization_strengths = np.linspace(1e3, 1e7, 5)\n",
    "best_loss_history = None\n",
    "best_classifier = None\n",
    "\n",
    "##############################################################################\n",
    "# TODO: Utilisez l'ensemble de validation pour régler les hyper-paramètres   #\n",
    "#  (force de régularisation et vitesse d'apprentissage). Vous devez          #\n",
    "#  expérimenter différentes plages de valeurs pour les taux d'apprentissage  #\n",
    "#  et les forces de régularisation; si tout va bien, avec num_iter = 1000    #\n",
    "#  vous devriez obtenir une précision de classification supérieur à 0.38 sur #\n",
    "#  l'ensemble de validation, et de 0.37 sur l'ensemble de test.              #\n",
    "#  Mettre les résultats des meilleurs hyper-paramètres dans les variables    #\n",
    "#  best_XYZ ci haut.                                                         #\n",
    "##############################################################################\n",
    "\n",
    "\n",
    "################################################################################\n",
    "#                             FIN DE VOTRE CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)\n",
    "\n",
    "visualize_loss(best_loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On évalue le modèle sur l'ensemble de test\n",
    "y_test_pred = best_classifier.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('Test set accuracy: %f' % (test_accuracy) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des poids appris pour chaque classe\n",
    "w = best_classifier.W[:-1,:] # retire le biais\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "\n",
    "    # Redimensionne les poids pour qu'ils soient entre 0 et 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
