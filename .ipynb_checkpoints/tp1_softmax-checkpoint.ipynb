{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP1: Classifieur linéaire, fonction de perte **Entropie croisée**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from utils.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# pour automatiquement recharger les modules externes\n",
    "# voir http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10 Chargement des données et prétraitement\n",
    "\n",
    "### **TODO** assurez-vous d'exécuter le script *./get_datasets.sh* au moins une fois dans un terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500, num_batch=200):\n",
    "    \"\"\"\n",
    "    Charger la banque de données CIFAR-10, prétraiter les images et ajouter une dimension pour le biais.\n",
    "    \n",
    "    Input :\n",
    "    - num_training : nombre d'images à mettre dans l'ensemble d'entrainement\n",
    "    - num_validation : nombre d'images à mettre dans l'ensemble de validation\n",
    "    - num_test : nombre d'images à mettre dans l'ensemble de test\n",
    "    - num_dev : d'images à mettre dans l'ensemble dev\n",
    "    \n",
    "    Output :\n",
    "    - X_train, y_train : données et cibles d'entrainement\n",
    "    - X_val, y_val: données et cibles de validation\n",
    "    - X_test y_test: données et cibles de test \n",
    "    - X_dev, y_dev: données et cibles dev\n",
    "    - X_batch, y_batch: batch de données et de cibles \n",
    "    \"\"\"\n",
    "    # Charger les données CIFAR-10\n",
    "    cifar10_dir = 'datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "  \n",
    "    # Séparer en ensembles d'entraînement, de validation, de test et de dev\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    mask = range(num_batch)\n",
    "    X_batch = X_train[mask]\n",
    "    y_batch = y_train[mask]\n",
    "    \n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    X_batch = np.reshape(X_batch, (X_batch.shape[0], -1))\n",
    "\n",
    "    # Normalisation\n",
    "    X_train -= np.mean(X_train, axis = 0)\n",
    "    X_val -= np.mean(X_val, axis = 0)\n",
    "    X_test -= np.mean(X_test, axis = 0)\n",
    "    X_dev -= np.mean(X_dev, axis = 0)\n",
    "    X_batch -= np.mean(X_batch, axis = 0)\n",
    "\n",
    "    # Ajout du biais\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    X_batch = np.hstack([X_batch, np.ones((X_batch.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev, X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n",
      "batch data shape:  (200, 3073)\n",
      "batch labels shape:  (200,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev, X_batch, y_batch = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)\n",
    "print('batch data shape: ', X_batch.shape)\n",
    "print('batch labels shape: ', y_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "Les prochaines étapes consistent à implanter le calcul de **l'entropie croisée** et de son **gradient**.   Vous commencerez avec une version naïve impliquant une boucle *for* sur l'ensemble des éléments d'une batch pour ensuite implanter une version vectorisée.   Mais avant de commencer à coder, veuillez donner ici la formule de l'entropie croisée et du gradient pour une mini-batch de 500 données contenue dans le tableau\n",
    "\n",
    "$$X \\in R^{500\\times 3073}$$\n",
    "\n",
    "et une matrice de poids $$W \\in R^{3073\\times 10}$$ \n",
    "\n",
    "où 3073 est la dimensionnalité des données et 10 est le nombre de classes.\n",
    "\n",
    "**Votre Réponse:** \n",
    "\n",
    "$$Loss = -ln(S) + lamb*norm(W,2) avec S = exp(X.W) / sum(exp(X.W), 1)$$\n",
    "\n",
    "$$dW = [(S - t)*X].T + 2*lamb*W$$\n",
    "\n",
    "**NOTE IMPORTANT** : la réponse à cette question ne contient aucune boucle, seulement des multiplications matricielles et vectorielles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifieur linéaire SOFTMAX\n",
    "\n",
    "Le code pour cette section est dans le fichier **utils/loss.py**. \n",
    "\n",
    "La fonction `softmax_ce_naive_forward_backward` estime la perte (et le gradient) à l'aide de boucles `for` qui itèrent sur chaque donnée de la mini-batch.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commençons par tester la **forward pass + l'entropie croisée**.  Pour l'instant, ignorons la rétro-propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.55585138  5.77349909 -0.61236922 ... -0.67253637 -0.87537157\n",
      "  -0.6617125 ]\n",
      " [ 0.17387348 -1.80598346  0.19155258 ...  0.21037321  0.27382122\n",
      "   0.20698745]\n",
      " [-0.28636859  2.97444401 -0.31548597 ... -0.34648343 -0.45098192\n",
      "  -0.34090709]\n",
      " ...\n",
      " [-0.61429695  6.38055963 -0.67675741 ... -0.7432509  -0.96741342\n",
      "  -0.73128895]\n",
      " [ 0.04694006 -0.48755546  0.05171283 ...  0.05679377  0.07392262\n",
      "   0.05587973]\n",
      " [ 0.30834142 -3.20267064  0.33969294 ...  0.37306882  0.48558539\n",
      "   0.36706461]]\n",
      "Bravo!\n",
      "loss error: 0.000000\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# TODO: Implémenter le calcul de perte de façon naive avec des boucles dans  #\n",
    "#  la fonction softmax_ce_naive_forward_backward située dans le fichier      #\n",
    "#  utils.loss.                                                               #\n",
    "#  On commence par UNE image et UNE cible                                    #\n",
    "##############################################################################\n",
    "\n",
    "from utils.loss import softmax_ce_naive_forward_backward\n",
    "import time\n",
    "\n",
    "# Matrice de poids aléatoires + 1 donnée à tester\n",
    "np.random.seed(0)\n",
    "W = np.random.randn(3073, 10) * 5e-4\n",
    "X_rnd = np.random.randn(1, 3073) * 5\n",
    "y_rnd = np.uint32(np.ones(1))\n",
    "loss, _ = softmax_ce_naive_forward_backward(X_rnd, W, y_rnd, 0.0)\n",
    "\n",
    "# La loss suivante est celle que vous devriez obtenir\n",
    "target_loss = 2.276854\n",
    "loss_error = np.abs(loss - target_loss)\n",
    "if loss_error < 1e-5:\n",
    "    print(\"Bravo!\")\n",
    "else:\n",
    "    print(\"Il y a un bug...\")\n",
    "print('loss error: %f' % loss_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.63553662 -0.45649559 -1.05686044 ...  0.86572232 -2.16513188\n",
      "  -2.27858858]\n",
      " [-2.1048624   0.28366624 -1.39517522 ...  0.52994945 -3.74530299\n",
      "  -3.17648768]\n",
      " [-2.93143808 -0.49561605  0.06348929 ...  0.40347084 -5.83713235\n",
      "  -4.95945962]\n",
      " ...\n",
      " [ 0.28318078  2.29331036  0.42042742 ... -0.80513258 -0.17775923\n",
      "  -2.32193285]\n",
      " [ 0.17581504  2.03569961  0.67592454 ...  1.3667073  -1.99752957\n",
      "  -2.84210116]\n",
      " [ 0.02817192 -0.0407199  -0.01173696 ... -0.00808671  0.02431986\n",
      "  -0.00981905]]\n",
      "Bravo!\n",
      "loss error: 0.000000\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# TODO: Implémenter le calcul de perte de façon naive avec des boucles dans  #\n",
    "#  la fonction softmax_ce_naive_forward_backward située dans le fichier      #\n",
    "#  utils.loss.                                                               #\n",
    "#  Maintenant on test avec N=200 images et autant de cibles                  #\n",
    "##############################################################################\n",
    "\n",
    "from utils.loss import softmax_ce_naive_forward_backward\n",
    "import time\n",
    "\n",
    "# Matrice de poids aléatoires + 500 données \n",
    "np.random.seed(0)\n",
    "W = np.random.randn(3073, 10) * 1e-4\n",
    "loss, _ = softmax_ce_naive_forward_backward(X_batch, W, y_batch, 0.0)\n",
    "\n",
    "target_loss = 2.356459\n",
    "loss_error = np.abs(loss - target_loss)\n",
    "if loss_error < 1e-5:\n",
    "    print(\"Bravo!\")\n",
    "else:\n",
    "    print(\"Il y a un bug...\")\n",
    "print('loss error: %f' % loss_error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2:\n",
    "Pourquoi s'attend-on que la loss soit approximativement -np.log(1/nb_classes))?\n",
    "\n",
    "**Votre réponse:** Lorsque l'on initialise aléatoirement les poids, nous savons que l'entropie sera maximale. Pour cela, la prédiction (correspondant ici à une probabilité) associée à chaque classe sera de 1/nb_classes. Vu différemment, nous pouvons voir que, si le modèle n'est pas entraîné, il va prédire de façon égale chaque classe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.43098977e+00 -1.20913328e+00 -6.81027658e-01 ...  6.86160793e-01\n",
      "  -1.24796336e-01 -3.19055346e+00]\n",
      " [-2.81695360e+00 -5.80283642e-01 -1.11202598e+00 ...  4.46219742e-01\n",
      "  -1.48431681e+00 -4.08483887e+00]\n",
      " [-3.79122796e+00 -1.28590230e+00  3.25074256e-01 ...  5.54251616e-01\n",
      "  -3.36434630e+00 -6.11138620e+00]\n",
      " ...\n",
      " [-4.60994269e-01  1.33338845e+00  8.34242035e-01 ... -1.34774259e+00\n",
      "   1.31663061e+00 -9.06365651e-01]\n",
      " [-6.58748327e-01  1.08145126e+00  9.29722438e-01 ...  1.07841747e+00\n",
      "  -3.09743356e-01 -1.59750900e+00]\n",
      " [ 3.00797539e-02 -4.02401683e-02 -1.10506968e-02 ... -6.03864912e-03\n",
      "   2.59833688e-02 -1.36873808e-02]]\n",
      "Softmax loss: 2.339132\n",
      "Sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "###################################################################################\n",
    "#  Vérification simple: s'assurer que l'entropie-croisée soit proche de           #\n",
    "#  -log(1/nb_classes)                                                             #\n",
    "###################################################################################\n",
    "\n",
    "from utils.loss import softmax_ce_naive_forward_backward\n",
    "import time\n",
    "\n",
    "# Matrice de poids aléatoires + 500 données \n",
    "W = np.random.randn(3073, 10) * 1e-4\n",
    "loss, _ = softmax_ce_naive_forward_backward(X_batch, W, y_batch, 0.0)\n",
    "\n",
    "# La loss d'un modèle non-entrainé devrait s'approcher de -log(0.1).\n",
    "print('Softmax loss: %f' % loss)\n",
    "print('Sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rétro-propagation\n",
    "\n",
    "Maintenant, passons à la **rétro-propagation**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.1279744   1.15786877 -0.12847105 ... -0.12895357 -0.13032041\n",
      "  -0.12886991]\n",
      " [ 0.04003112 -0.36218796  0.04018648 ...  0.04033741  0.04076497\n",
      "   0.04031124]\n",
      " [-0.06593102  0.59652141 -0.06618689 ... -0.06643548 -0.06713966\n",
      "  -0.06639238]\n",
      " ...\n",
      " [-0.1414304   1.27961408 -0.14197927 ... -0.14251253 -0.14402308\n",
      "  -0.14242007]\n",
      " [ 0.01080707 -0.0977787   0.01084901 ...  0.01088976  0.01100519\n",
      "   0.01088269]\n",
      " [ 0.07098985 -0.64229201  0.07126535 ...  0.07153302  0.07229123\n",
      "   0.07148661]]\n",
      "Bravo pour la loss!\n",
      "loss error: 0.000000\n",
      "Bravo pour le gradient!\n",
      "gradient error 0.000000\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# TODO: Implémenter le calcul de perte ET DE LA RÉTROPROPAGATION de façon    #\n",
    "#       naive avec des boucles dans la fonction                              #\n",
    "#       softmax_ce_naive_forward_backward située dans le fichier utils.loss  #\n",
    "#                                                                            #\n",
    "#  On commence par UNE image et UNE cible                                    #\n",
    "##############################################################################\n",
    "\n",
    "from utils.loss import softmax_ce_naive_forward_backward\n",
    "import time\n",
    "\n",
    "# Matrice de poids aléatoires + une donnée\n",
    "np.random.seed(0)\n",
    "W = np.random.randn(3073, 10) * 1e-4\n",
    "X_rnd = np.random.randn(1, 3073)\n",
    "y_rnd = np.uint32(np.ones(1))\n",
    "loss, dW = softmax_ce_naive_forward_backward(X_rnd, W, y_rnd, 0.0)\n",
    "\n",
    "# La loss suivante est celle que vous devriez obtenir\n",
    "target_loss = 2.30114875\n",
    "loss_error = np.abs(loss - target_loss)\n",
    "if loss_error < 1e-5:\n",
    "    print(\"Bravo pour la loss!\")\n",
    "else:\n",
    "    print(\"Il y a un bug au niveau de la loss...\")\n",
    "print('loss error: %f' % loss_error)\n",
    "\n",
    "# Le gradient suivant est celui que vous devriez obtenir pour les 3 premiers poids\n",
    "target_dW = np.array([-0.1279744 ,  1.15786877, -0.12847105])\n",
    "dW_error = np.mean(np.abs(dW[0,0:3]-target_dW))\n",
    "if dW_error < 1e-7:\n",
    "    print(\"Bravo pour le gradient!\")\n",
    "else:\n",
    "    print(\"Il y a un bug au niveau du gradient...\")\n",
    "print('gradient error %f' % dW_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.68817739e+00 -1.37204569e+00 -4.61587730e-01 ...  8.34288820e-01\n",
      "  -9.34764094e-01 -4.93670879e+00]\n",
      " [-2.06141741e+00 -9.99790842e-01 -7.64298623e-01 ...  5.35426798e-01\n",
      "  -2.35436909e+00 -6.23144698e+00]\n",
      " [-2.86517804e+00 -1.82217642e+00  7.03969046e-01 ...  4.17467420e-01\n",
      "  -4.13619756e+00 -8.48146735e+00]\n",
      " ...\n",
      " [ 2.33613191e-01  1.53139723e+00  2.61028583e-01 ... -1.30382203e+00\n",
      "   5.89147437e-01 -2.84850674e+00]\n",
      " [ 1.22175450e-01  1.20625036e+00  4.12379388e-01 ...  7.80103824e-01\n",
      "  -9.49227030e-01 -3.60700249e+00]\n",
      " [ 2.80292466e-02 -4.27386483e-02 -1.21210217e-02 ... -7.08090406e-03\n",
      "   2.31554188e-02 -1.50200384e-02]]\n",
      "[-1.68817739 -1.37204569 -0.46158773  1.9206649 ]\n",
      "Bravo pour la loss!\n",
      "loss error: 0.000000\n",
      "Bravo pour le gradient!\n",
      "gradient error 0.000000\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# TODO: Implémenter le calcul de perte ET DE LA RÉTROPROPAGATION de façon    #\n",
    "#       naive avec des boucles dans la fonction                              #\n",
    "#       softmax_ce_naive_forward_backward située dans le fichier utils.loss  #\n",
    "#                                                                            #\n",
    "#  Maintenant on test avec N=200 images et autant de cibles                  #\n",
    "##############################################################################\n",
    "\n",
    "from utils.loss import softmax_ce_naive_forward_backward\n",
    "import time\n",
    "\n",
    "# Matrice de poids aléatoires + 500 données\n",
    "np.random.seed(1)\n",
    "W = np.random.randn(3073, 10) * 1e-4\n",
    "loss, dW = softmax_ce_naive_forward_backward(X_batch, W, y_batch, 0.0)\n",
    "print(dW[0,0:4])\n",
    "# La loss suivante est celle que vous devriez obtenir\n",
    "target_loss = 2.35680883\n",
    "loss_error = np.abs(loss - target_loss)\n",
    "if loss_error < 1e-5:\n",
    "    print(\"Bravo pour la loss!\")\n",
    "else:\n",
    "    print(\"Il y a un bug au niveau de la loss...\")\n",
    "print('loss error: %f' % loss_error)\n",
    "\n",
    "# Le gradient suivant est celui que vous devriez obtenir pour les 3 premiers poids\n",
    "target_dW = np.array([-1.68817739, -1.37204569, -0.46158773, 1.9206649])\n",
    "dW_error = np.mean(np.abs(dW[0,0:4]-target_dW))\n",
    "if dW_error < 1e-7:\n",
    "    print(\"Bravo pour le gradient!\")\n",
    "else:\n",
    "    print(\"Il y a un bug au niveau du gradient...\")\n",
    "print('gradient error %f' % dW_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encore quelques vérifications d'usage\n",
    "En principe, à ce point-ci, le calcul de l'entropie croisée (et de son gradient) via la fonction *softmax_ce_naive_forward_backward* devrait fonctionner.  Mais avant de passer à la prochaine étape il nous reste deux vérifications à faire : s'assurer que la **régularisation** fonctionne et passer le teste du **gradient numérique**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.28130506e+00  1.27850939e+00  1.66059732e+00 ... -2.37019660e+00\n",
      "   5.14711721e-01 -3.97687527e+00]\n",
      " [-2.87029679e+00  1.66976915e+00  1.72528070e+00 ... -2.43685582e+00\n",
      "  -1.12453126e+00 -4.43379397e+00]\n",
      " [-5.15362069e+00  1.85533637e+00  3.00834886e+00 ... -2.40515039e+00\n",
      "  -2.78472419e+00 -5.35961770e+00]\n",
      " ...\n",
      " [-3.19806748e-01  9.41441138e-02 -3.14332935e-01 ... -6.04732343e-01\n",
      "   2.71569149e+00 -3.01589257e+00]\n",
      " [-1.55361825e+00 -6.93614011e-01  9.17299004e-01 ...  1.11015254e+00\n",
      "   3.57676871e-01 -3.54042096e+00]\n",
      " [-8.52464676e-03  1.59904102e-02 -9.81805597e-03 ...  5.80975993e-03\n",
      "  -4.04981619e-03 -1.65769038e-02]]\n",
      "[[-2.24881816e+00  1.26627427e+00  1.65003389e+00 ... -2.38542074e+00\n",
      "   5.21092503e-01 -3.98186268e+00]\n",
      " [-2.84105463e+00  1.62856633e+00  1.71883235e+00 ... -2.45441299e+00\n",
      "  -1.12368699e+00 -4.42213767e+00]\n",
      " [-5.17563307e+00  1.87823084e+00  3.02638068e+00 ... -2.42386578e+00\n",
      "  -2.79008196e+00 -5.34901059e+00]\n",
      " ...\n",
      " [-3.11725843e-01  1.11904666e-01 -3.42849085e-01 ... -5.89119213e-01\n",
      "   2.73778621e+00 -3.00531364e+00]\n",
      " [-1.55743002e+00 -6.99708628e-01  8.93634040e-01 ...  1.07981047e+00\n",
      "   3.52033380e-01 -3.53285112e+00]\n",
      " [ 2.38598124e-03  4.43685587e-02 -2.49535387e-02 ... -5.55396417e-03\n",
      "   8.92861690e-03 -2.40050316e-02]]\n",
      "2.402603143378467\n",
      "[[-2.24232078e+00  1.26382724e+00  1.64792120e+00 ... -2.38846557e+00\n",
      "   5.22368659e-01 -3.98286016e+00]\n",
      " [-2.83520620e+00  1.62032577e+00  1.71754268e+00 ... -2.45792442e+00\n",
      "  -1.12351813e+00 -4.41980641e+00]\n",
      " [-5.18003555e+00  1.88280973e+00  3.02998704e+00 ... -2.42760885e+00\n",
      "  -2.79115351e+00 -5.34688917e+00]\n",
      " ...\n",
      " [-3.10109661e-01  1.15456777e-01 -3.48552315e-01 ... -5.85996587e-01\n",
      "   2.74220515e+00 -3.00319786e+00]\n",
      " [-1.55819237e+00 -7.00927551e-01  8.88901047e-01 ...  1.07374205e+00\n",
      "   3.50904681e-01 -3.53133715e+00]\n",
      " [ 4.56810684e-03  5.00441884e-02 -2.79806353e-02 ... -7.82670899e-03\n",
      "   1.15243035e-02 -2.54906571e-02]]\n",
      "2.4056711482305806\n",
      "[[-2.23452392  1.26089081  1.64538598 ... -2.39211936  0.52390005\n",
      "  -3.98405714]\n",
      " [-2.82818808  1.61043709  1.71599508 ... -2.46213814 -1.12331551\n",
      "  -4.41700889]\n",
      " [-5.18531852  1.88830441  3.03431467 ... -2.43210055 -2.79243937\n",
      "  -5.34434346]\n",
      " ...\n",
      " [-0.30817024  0.11971931 -0.35539619 ... -0.58224944  2.74750789\n",
      "  -3.00065892]\n",
      " [-1.55910719 -0.70239026  0.88322146 ...  1.06645995  0.34955024\n",
      "  -3.52952038]\n",
      " [ 0.00718666  0.05685494 -0.03161315 ... -0.010554    0.01463913\n",
      "  -0.02727341]]\n",
      "2.4093527540531174\n",
      "[[-2.22516769  1.25736709  1.64234371 ... -2.39650391  0.52573771\n",
      "  -3.98549351]\n",
      " [-2.81976634  1.59857068  1.71413796 ... -2.46719461 -1.12307236\n",
      "  -4.41365188]\n",
      " [-5.19165809  1.89489802  3.03950784 ... -2.43749058 -2.79398241\n",
      "  -5.34128861]\n",
      " ...\n",
      " [-0.30584294  0.12483435 -0.36360884 ... -0.57775285  2.75387116\n",
      "  -2.99761219]\n",
      " [-1.56020498 -0.70414551  0.87640595 ...  1.05772144  0.34792492\n",
      "  -3.52734027]\n",
      " [ 0.01032892  0.06502785 -0.03597217 ... -0.01382676  0.01837692\n",
      "  -0.02941271]]\n",
      "2.413770681040161\n",
      "[[-2.21394021  1.25313863  1.63869298 ... -2.40176538  0.52794291\n",
      "  -3.98721716]\n",
      " [-2.80966025  1.58433099  1.71190941 ... -2.47326236 -1.12278058\n",
      "  -4.40962346]\n",
      " [-5.19926557  1.90281035  3.04573963 ... -2.44395862 -2.79583405\n",
      "  -5.3376228 ]\n",
      " ...\n",
      " [-0.30305018  0.1309724  -0.37346402 ... -0.57235696  2.7615071\n",
      "  -2.99395611]\n",
      " [-1.56152232 -0.70625181  0.86822733 ...  1.04723522  0.34597453\n",
      "  -3.52472413]\n",
      " [ 0.01409963  0.07483534 -0.04120299 ... -0.01775406  0.02286226\n",
      "  -0.03197987]]\n",
      "2.419072193424614\n",
      "[[-2.20046724  1.24806448  1.63431212 ... -2.40807913  0.53058915\n",
      "  -3.98928554]\n",
      " [-2.79753294  1.56724336  1.70923515 ... -2.48054367 -1.12243044\n",
      "  -4.40478936]\n",
      " [-5.20839455  1.91230514  3.05321779 ... -2.45172026 -2.79805602\n",
      "  -5.33322382]\n",
      " ...\n",
      " [-0.29969887  0.13833805 -0.38529024 ... -0.56588188  2.77067022\n",
      "  -2.98956882]\n",
      " [-1.56310314 -0.70877937  0.858413   ...  1.03465175  0.34363406\n",
      "  -3.52158476]\n",
      " [ 0.01862449  0.08660432 -0.04747998 ... -0.02246682  0.02824468\n",
      "  -0.03506046]]\n",
      "2.4254340082859573\n",
      "[[-2.18429968  1.2419755   1.62905507 ... -2.41565564  0.53376463\n",
      "  -3.99176759]\n",
      " [-2.78298017  1.5467382   1.70602604 ... -2.48928124 -1.12201027\n",
      "  -4.39898843]\n",
      " [-5.21934932  1.9236989   3.06219157 ... -2.46103424 -2.80072238\n",
      "  -5.32794504]\n",
      " ...\n",
      " [-0.29567729  0.14717684 -0.3994817  ... -0.55811179  2.78166597\n",
      "  -2.98430407]\n",
      " [-1.56500011 -0.71181244  0.8466358  ...  1.01955159  0.3408255\n",
      "  -3.51781752]\n",
      " [ 0.02405431  0.10072711 -0.05501237 ... -0.02812214  0.03470358\n",
      "  -0.03875717]]\n",
      "2.433068186119569\n",
      "[[-2.1648986   1.23466872  1.62274662 ... -2.42474744  0.53757522\n",
      "  -3.99474606]\n",
      " [-2.76551685  1.52213201  1.70217511 ... -2.49976633 -1.12150607\n",
      "  -4.39202732]\n",
      " [-5.23249504  1.93737141  3.07296011 ... -2.47221101 -2.80392202\n",
      "  -5.32161051]\n",
      " ...\n",
      " [-0.2908514   0.15778339 -0.41651146 ... -0.54878767  2.79486086\n",
      "  -2.97798637]\n",
      " [-1.56727648 -0.71545212  0.83250316 ...  1.0014314   0.33745522\n",
      "  -3.51329683]\n",
      " [ 0.03057011  0.11767445 -0.06405123 ... -0.03490852  0.04245426\n",
      "  -0.04319323]]\n",
      "2.4422291995199035\n",
      "[[-2.14161731  1.22590058  1.61517648 ... -2.43565761  0.54214792\n",
      "  -3.99832021]\n",
      " [-2.74456086  1.49260459  1.697554   ... -2.51234843 -1.12090104\n",
      "  -4.383674  ]\n",
      " [-5.24826991  1.95377841  3.08588236 ... -2.48562313 -2.80776159\n",
      "  -5.31400907]\n",
      " ...\n",
      " [-0.28506033  0.17051124 -0.43694716 ... -0.53759874  2.81069474\n",
      "  -2.97040513]\n",
      " [-1.57000813 -0.71981975  0.81554399 ...  0.97968717  0.33341089\n",
      "  -3.50787201]\n",
      " [ 0.03838906  0.13801125 -0.07489786 ... -0.04305217  0.05175507\n",
      "  -0.04851649]]\n",
      "2.4532224156003046\n",
      "[[-2.11367976  1.21537882  1.60609232 ... -2.44874981  0.54763515\n",
      "  -4.0026092 ]\n",
      " [-2.71941367  1.45717167  1.69200866 ... -2.52744695 -1.12017499\n",
      "  -4.37365   ]\n",
      " [-5.26719976  1.97346682  3.10138906 ... -2.50171768 -2.81236907\n",
      "  -5.30488734]\n",
      " ...\n",
      " [-0.27811105  0.18578466 -0.46147001 ... -0.52417202  2.82969539\n",
      "  -2.96130765]\n",
      " [-1.5732861  -0.72506089  0.79519299 ...  0.9535941   0.32855769\n",
      "  -3.50136221]\n",
      " [ 0.0477718   0.16241542 -0.08791382 ... -0.05282456  0.06291605\n",
      "  -0.05490441]]\n",
      "2.466414274896786\n",
      "Bravo!\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# Vérifions que le fait d'augmenter le terme de régularisation L2            #\n",
    "# augmente la loss...                                                        #\n",
    "##############################################################################\n",
    "success = True\n",
    "\n",
    "# Matrice de poids aléatoires + 500 données\n",
    "np.random.seed(1)\n",
    "W = np.random.randn(3073, 10) * 1e-4\n",
    "prev_loss, _ = softmax_ce_naive_forward_backward(X_dev, W, y_dev, 0.0)\n",
    "\n",
    "reg = 1e2\n",
    "for i in range(10):\n",
    "    loss, _ = softmax_ce_naive_forward_backward(X_dev, W, y_dev, reg)\n",
    "    print(loss)\n",
    "    if loss <= prev_loss:\n",
    "        success = False\n",
    "    prev_loss = loss\n",
    "    reg *= 1.2\n",
    "    \n",
    "if success:\n",
    "    print(\"Bravo!\")\n",
    "else:\n",
    "    print('Erreur!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------\n",
      "Gradient check : reg=0\n",
      "------------\n",
      "\n",
      "[[-2.28130506e+00  1.27850939e+00  1.66059732e+00 ... -2.37019660e+00\n",
      "   5.14711721e-01 -3.97687527e+00]\n",
      " [-2.87029679e+00  1.66976915e+00  1.72528070e+00 ... -2.43685582e+00\n",
      "  -1.12453126e+00 -4.43379397e+00]\n",
      " [-5.15362069e+00  1.85533637e+00  3.00834886e+00 ... -2.40515039e+00\n",
      "  -2.78472419e+00 -5.35961770e+00]\n",
      " ...\n",
      " [-3.19806748e-01  9.41441138e-02 -3.14332935e-01 ... -6.04732343e-01\n",
      "   2.71569149e+00 -3.01589257e+00]\n",
      " [-1.55361825e+00 -6.93614011e-01  9.17299004e-01 ...  1.11015254e+00\n",
      "   3.57676871e-01 -3.54042096e+00]\n",
      " [-8.52464676e-03  1.59904102e-02 -9.81805597e-03 ...  5.80975993e-03\n",
      "  -4.04981619e-03 -1.65769038e-02]]\n",
      "[[-2.28130389e+00  1.27851060e+00  1.66058731e+00 ... -2.37019415e+00\n",
      "   5.14712841e-01 -3.97687336e+00]\n",
      " [-2.87029678e+00  1.66976887e+00  1.72528523e+00 ... -2.43685478e+00\n",
      "  -1.12453156e+00 -4.43379372e+00]\n",
      " [-5.15362100e+00  1.85533509e+00  3.00836141e+00 ... -2.40515052e+00\n",
      "  -2.78472523e+00 -5.35961822e+00]\n",
      " ...\n",
      " [-3.19806632e-01  9.41453067e-02 -3.14340598e-01 ... -6.04730064e-01\n",
      "   2.71569224e+00 -3.01589028e+00]\n",
      " [-1.55362032e+00 -6.93615220e-01  9.17312970e-01 ...  1.11015258e+00\n",
      "   3.57675139e-01 -3.54042114e+00]\n",
      " [-8.52463331e-03  1.59904502e-02 -9.81822730e-03 ...  5.80976319e-03\n",
      "  -4.04980478e-03 -1.65768828e-02]]\n",
      "[[-2.28130624e+00  1.27850819e+00  1.66060733e+00 ... -2.37019906e+00\n",
      "   5.14710601e-01 -3.97687719e+00]\n",
      " [-2.87029681e+00  1.66976942e+00  1.72527616e+00 ... -2.43685686e+00\n",
      "  -1.12453097e+00 -4.43379422e+00]\n",
      " [-5.15362038e+00  1.85533764e+00  3.00833631e+00 ... -2.40515025e+00\n",
      "  -2.78472316e+00 -5.35961718e+00]\n",
      " ...\n",
      " [-3.19806865e-01  9.41429210e-02 -3.14325272e-01 ... -6.04734622e-01\n",
      "   2.71569073e+00 -3.01589485e+00]\n",
      " [-1.55361619e+00 -6.93612802e-01  9.17285038e-01 ...  1.11015251e+00\n",
      "   3.57678604e-01 -3.54042078e+00]\n",
      " [-8.52466024e-03  1.59903702e-02 -9.81788441e-03 ...  5.80975663e-03\n",
      "  -4.04982763e-03 -1.65769247e-02]]\n",
      "numerical: 0.583260, analytic 0.583260, relative error: 4.009323e-10\n",
      "[[-2.28132548e+00  1.27869773e+00  1.66057659e+00 ... -2.37021741e+00\n",
      "   5.14692087e-01 -3.97689652e+00]\n",
      " [-2.87031860e+00  1.66997081e+00  1.72525832e+00 ... -2.43687801e+00\n",
      "  -1.12455207e+00 -4.43381680e+00]\n",
      " [-5.15364401e+00  1.85555168e+00  3.00832496e+00 ... -2.40517398e+00\n",
      "  -2.78474659e+00 -5.35964172e+00]\n",
      " ...\n",
      " [-3.19809239e-01  9.41754107e-02 -3.14336023e-01 ... -6.04735409e-01\n",
      "   2.71568812e+00 -3.01589632e+00]\n",
      " [-1.55362171e+00 -6.93573279e-01  9.17295141e-01 ...  1.11014891e+00\n",
      "   3.57672488e-01 -3.54042574e+00]\n",
      " [-8.52458584e-03  1.59901322e-02 -9.81799793e-03 ...  5.80979486e-03\n",
      "  -4.04985921e-03 -1.65768131e-02]]\n",
      "[[-2.28128464e+00  1.27832106e+00  1.66061805e+00 ... -2.37017580e+00\n",
      "   5.14731354e-01 -3.97685402e+00]\n",
      " [-2.87027498e+00  1.66956748e+00  1.72530308e+00 ... -2.43683363e+00\n",
      "  -1.12451045e+00 -4.43377114e+00]\n",
      " [-5.15359737e+00  1.85512105e+00  3.00837276e+00 ... -2.40512680e+00\n",
      "  -2.78470180e+00 -5.35959367e+00]\n",
      " ...\n",
      " [-3.19804258e-01  9.41128161e-02 -3.14329846e-01 ... -6.04729277e-01\n",
      "   2.71569486e+00 -3.01588881e+00]\n",
      " [-1.55361480e+00 -6.93654743e-01  9.17302867e-01 ...  1.11015618e+00\n",
      "   3.57681255e-01 -3.54041619e+00]\n",
      " [-8.52470772e-03  1.59906885e-02 -9.81811404e-03 ...  5.80972496e-03\n",
      "  -4.04977321e-03 -1.65769945e-02]]\n",
      "numerical: 1.046395, analytic 1.046395, relative error: 5.054688e-10\n",
      "[[-2.28131772e+00  1.27849703e+00  1.66058362e+00 ... -2.37007363e+00\n",
      "   5.14697338e-01 -3.97688777e+00]\n",
      " [-2.87030776e+00  1.66975878e+00  1.72526890e+00 ... -2.43675051e+00\n",
      "  -1.12454351e+00 -4.43380479e+00]\n",
      " [-5.15363060e+00  1.85532727e+00  3.00833846e+00 ... -2.40505755e+00\n",
      "  -2.78473505e+00 -5.35962709e+00]\n",
      " ...\n",
      " [-3.19819639e-01  9.41313673e-02 -3.14345909e-01 ... -6.04611846e-01\n",
      "   2.71567829e+00 -3.01590529e+00]\n",
      " [-1.55363014e+00 -6.93625470e-01  9.17287120e-01 ...  1.11026167e+00\n",
      "   3.57664756e-01 -3.54043202e+00]\n",
      " [-8.52458945e-03  1.59904955e-02 -9.81801310e-03 ...  5.80947134e-03\n",
      "  -4.04982407e-03 -1.65768230e-02]]\n",
      "[[-2.28129240e+00  1.27852176e+00  1.66061102e+00 ... -2.37031957e+00\n",
      "   5.14726104e-01 -3.97686278e+00]\n",
      " [-2.87028582e+00  1.66977951e+00  1.72529249e+00 ... -2.43696113e+00\n",
      "  -1.12451902e+00 -4.43378315e+00]\n",
      " [-5.15361078e+00  1.85534546e+00  3.00835927e+00 ... -2.40524322e+00\n",
      "  -2.78471334e+00 -5.35960830e+00]\n",
      " ...\n",
      " [-3.19793858e-01  9.41568604e-02 -3.14319961e-01 ... -6.04852840e-01\n",
      "   2.71570469e+00 -3.01587984e+00]\n",
      " [-1.55360637e+00 -6.93602552e-01  9.17310888e-01 ...  1.11004342e+00\n",
      "   3.57688987e-01 -3.54040991e+00]\n",
      " [-8.52470410e-03  1.59903249e-02 -9.81809888e-03 ...  5.81004877e-03\n",
      "  -4.04980835e-03 -1.65769846e-02]]\n",
      "numerical: -0.713749, analytic -0.713749, relative error: 3.854087e-10\n",
      "[[-2.28132281e+00  1.27866798e+00  1.66058022e+00 ... -2.37021289e+00\n",
      "   5.14694619e-01 -3.97689136e+00]\n",
      " [-2.87031593e+00  1.66993916e+00  1.72526240e+00 ... -2.43687342e+00\n",
      "  -1.12454934e+00 -4.43381167e+00]\n",
      " [-5.15364073e+00  1.85551446e+00  3.00832982e+00 ... -2.40516859e+00\n",
      "  -2.78474337e+00 -5.35963623e+00]\n",
      " ...\n",
      " [-3.19815850e-01  9.42232567e-02 -3.14341407e-01 ... -6.04740290e-01\n",
      "   2.71568283e+00 -3.01590130e+00]\n",
      " [-1.55362685e+00 -6.93536554e-01  9.17290764e-01 ...  1.11014520e+00\n",
      "   3.57668458e-01 -3.54042952e+00]\n",
      " [-8.52457637e-03  1.59900891e-02 -9.81802011e-03 ...  5.80980170e-03\n",
      "  -4.04985171e-03 -1.65767991e-02]]\n",
      "[[-2.28128732e+00  1.27835081e+00  1.66061442e+00 ... -2.37018032e+00\n",
      "   5.14728822e-01 -3.97685919e+00]\n",
      " [-2.87027765e+00  1.66959913e+00  1.72529900e+00 ... -2.43683822e+00\n",
      "  -1.12451319e+00 -4.43377627e+00]\n",
      " [-5.15360065e+00  1.85515827e+00  3.00836790e+00 ... -2.40513219e+00\n",
      "  -2.78470502e+00 -5.35959917e+00]\n",
      " ...\n",
      " [-3.19797647e-01  9.40649709e-02 -3.14324463e-01 ... -6.04724397e-01\n",
      "   2.71570015e+00 -3.01588383e+00]\n",
      " [-1.55360966e+00 -6.93691467e-01  9.17307243e-01 ...  1.11015989e+00\n",
      "   3.57685285e-01 -3.54041241e+00]\n",
      " [-8.52471718e-03  1.59907316e-02 -9.81809187e-03 ...  5.80971812e-03\n",
      "  -4.04978070e-03 -1.65770085e-02]]\n",
      "numerical: 0.478499, analytic 0.478499, relative error: 4.353248e-10\n",
      "[[-2.28130901e+00  1.27850601e+00  1.66063001e+00 ... -2.37020068e+00\n",
      "   5.14708555e-01 -3.97687973e+00]\n",
      " [-2.87029964e+00  1.66976686e+00  1.72530404e+00 ... -2.43685887e+00\n",
      "  -1.12453326e+00 -4.43379759e+00]\n",
      " [-5.15362452e+00  1.85533321e+00  3.00837953e+00 ... -2.40515425e+00\n",
      "  -2.78472676e+00 -5.35962225e+00]\n",
      " ...\n",
      " [-3.19810857e-01  9.41398266e-02 -3.14291804e-01 ... -6.04738028e-01\n",
      "   2.71568730e+00 -3.01589743e+00]\n",
      " [-1.55362222e+00 -6.93618296e-01  9.17336974e-01 ...  1.11014702e+00\n",
      "   3.57673099e-01 -3.54042520e+00]\n",
      " [-8.52459209e-03  1.59904597e-02 -9.81841437e-03 ...  5.80986045e-03\n",
      "  -4.04983078e-03 -1.65768528e-02]]\n",
      "[[-2.28130112e+00  1.27851278e+00  1.66056464e+00 ... -2.37019253e+00\n",
      "   5.14714886e-01 -3.97687082e+00]\n",
      " [-2.87029394e+00  1.66977143e+00  1.72525735e+00 ... -2.43685277e+00\n",
      "  -1.12452927e+00 -4.43379035e+00]\n",
      " [-5.15361686e+00  1.85533952e+00  3.00831819e+00 ... -2.40514653e+00\n",
      "  -2.78472163e+00 -5.35961315e+00]\n",
      " ...\n",
      " [-3.19802640e-01  9.41484011e-02 -3.14374067e-01 ... -6.04726659e-01\n",
      "   2.71569567e+00 -3.01588770e+00]\n",
      " [-1.55361429e+00 -6.93609726e-01  9.17261034e-01 ...  1.11015807e+00\n",
      "   3.57680643e-01 -3.54041673e+00]\n",
      " [-8.52470146e-03  1.59903607e-02 -9.81769735e-03 ...  5.80965937e-03\n",
      "  -4.04980164e-03 -1.65769548e-02]]\n",
      "numerical: -0.149081, analytic -0.149081, relative error: 1.373790e-09\n",
      "[[-2.28132523e+00  1.27848939e+00  1.66057687e+00 ... -2.37021658e+00\n",
      "   5.14695057e-01 -3.97689576e+00]\n",
      " [-2.87031925e+00  1.66974686e+00  1.72525796e+00 ... -2.43687795e+00\n",
      "  -1.12454986e+00 -4.43381710e+00]\n",
      " [-5.15365038e+00  1.85530734e+00  3.00831943e+00 ... -2.40517941e+00\n",
      "  -2.78474921e+00 -5.35964763e+00]\n",
      " ...\n",
      " [-3.19816412e-01  9.41344160e-02 -3.14343266e-01 ... -6.04742859e-01\n",
      "   2.71568158e+00 -3.01590394e+00]\n",
      " [-1.55363465e+00 -6.93629649e-01  9.17282316e-01 ...  1.11013553e+00\n",
      "   3.57661148e-01 -3.54043854e+00]\n",
      " [-8.52443856e-03  1.59906196e-02 -9.81790219e-03 ...  5.80993498e-03\n",
      "  -4.04974141e-03 -1.65766903e-02]]\n",
      "[[-2.28128489e+00  1.27852940e+00  1.66061777e+00 ... -2.37017663e+00\n",
      "   5.14728384e-01 -3.97685479e+00]\n",
      " [-2.87027433e+00  1.66979143e+00  1.72530343e+00 ... -2.43683369e+00\n",
      "  -1.12451267e+00 -4.43377084e+00]\n",
      " [-5.15359100e+00  1.85536540e+00  3.00837829e+00 ... -2.40512137e+00\n",
      "  -2.78469918e+00 -5.35958777e+00]\n",
      " ...\n",
      " [-3.19797085e-01  9.41538115e-02 -3.14322603e-01 ... -6.04721828e-01\n",
      "   2.71570140e+00 -3.01588119e+00]\n",
      " [-1.55360185e+00 -6.93598373e-01  9.17315692e-01 ...  1.11016956e+00\n",
      "   3.57692595e-01 -3.54040339e+00]\n",
      " [-8.52485500e-03  1.59902007e-02 -9.81820979e-03 ...  5.80958484e-03\n",
      "  -4.04989101e-03 -1.65771172e-02]]\n",
      "numerical: -0.353161, analytic -0.353161, relative error: 2.364168e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.28131930e+00  1.27849490e+00  1.66058302e+00 ... -2.37006706e+00\n",
      "   5.14698570e-01 -3.97688926e+00]\n",
      " [-2.87031492e+00  1.66975055e+00  1.72526181e+00 ... -2.43668641e+00\n",
      "  -1.12454885e+00 -4.43381294e+00]\n",
      " [-5.15364679e+00  1.85530981e+00  3.00832153e+00 ... -2.40490648e+00\n",
      "  -2.78474991e+00 -5.35964535e+00]\n",
      " ...\n",
      " [-3.19809511e-01  9.41415808e-02 -3.14335647e-01 ... -6.04702096e-01\n",
      "   2.71568877e+00 -3.01589592e+00]\n",
      " [-1.55362562e+00 -6.93620581e-01  9.17292010e-01 ...  1.11022360e+00\n",
      "   3.57670030e-01 -3.54042916e+00]\n",
      " [-8.52459919e-03  1.59904593e-02 -9.81804825e-03 ...  5.80964138e-03\n",
      "  -4.04985768e-03 -1.65768248e-02]]\n",
      "[[-2.28129083e+00  1.27852389e+00  1.66061162e+00 ... -2.37032614e+00\n",
      "   5.14724872e-01 -3.97686129e+00]\n",
      " [-2.87027866e+00  1.66978774e+00  1.72529958e+00 ... -2.43702523e+00\n",
      "  -1.12451368e+00 -4.43377500e+00]\n",
      " [-5.15359459e+00  1.85536293e+00  3.00837619e+00 ... -2.40539430e+00\n",
      "  -2.78469848e+00 -5.35959005e+00]\n",
      " ...\n",
      " [-3.19803985e-01  9.41466470e-02 -3.14330223e-01 ... -6.04762591e-01\n",
      "   2.71569421e+00 -3.01588922e+00]\n",
      " [-1.55361089e+00 -6.93607441e-01  9.17305998e-01 ...  1.11008148e+00\n",
      "   3.57683713e-01 -3.54041277e+00]\n",
      " [-8.52469437e-03  1.59903611e-02 -9.81806373e-03 ...  5.80987880e-03\n",
      "  -4.04977475e-03 -1.65769828e-02]]\n",
      "numerical: -0.403691, analytic -0.403691, relative error: 1.429751e-09\n",
      "[[-2.28130730e+00  1.27850479e+00  1.66059032e+00 ... -2.37020219e+00\n",
      "   5.14709758e-01 -3.97688269e+00]\n",
      " [-2.87029736e+00  1.66976664e+00  1.72527564e+00 ... -2.43685950e+00\n",
      "  -1.12453134e+00 -4.43379966e+00]\n",
      " [-5.15361878e+00  1.85533698e+00  3.00834623e+00 ... -2.40515157e+00\n",
      "  -2.78472181e+00 -5.35962063e+00]\n",
      " ...\n",
      " [-3.19814571e-01  9.41349215e-02 -3.14342989e-01 ... -6.04741454e-01\n",
      "   2.71568489e+00 -3.01590306e+00]\n",
      " [-1.55362390e+00 -6.93620843e-01  9.17291222e-01 ...  1.11014602e+00\n",
      "   3.57672396e-01 -3.54042895e+00]\n",
      " [-8.52461478e-03  1.59904740e-02 -9.81795598e-03 ...  5.80987392e-03\n",
      "  -4.04979658e-03 -1.65768315e-02]]\n",
      "[[-2.28130283e+00  1.27851400e+00  1.66060432e+00 ... -2.37019102e+00\n",
      "   5.14713683e-01 -3.97686786e+00]\n",
      " [-2.87029622e+00  1.66977165e+00  1.72528576e+00 ... -2.43685214e+00\n",
      "  -1.12453118e+00 -4.43378828e+00]\n",
      " [-5.15362260e+00  1.85533576e+00  3.00835150e+00 ... -2.40514921e+00\n",
      "  -2.78472657e+00 -5.35961477e+00]\n",
      " ...\n",
      " [-3.19798926e-01  9.41533063e-02 -3.14322880e-01 ... -6.04723232e-01\n",
      "   2.71569809e+00 -3.01588207e+00]\n",
      " [-1.55361261e+00 -6.93607179e-01  9.17306786e-01 ...  1.11015907e+00\n",
      "   3.57681347e-01 -3.54041298e+00]\n",
      " [-8.52467877e-03  1.59903463e-02 -9.81815600e-03 ...  5.80964590e-03\n",
      "  -4.04983583e-03 -1.65769761e-02]]\n",
      "numerical: -4.101434, analytic -4.101434, relative error: 2.919873e-11\n",
      "[[-2.28131308e+00  1.27849951e+00  1.66058706e+00 ... -2.37020619e+00\n",
      "   5.14702987e-01 -3.97679286e+00]\n",
      " [-2.87030209e+00  1.66976220e+00  1.72527321e+00 ... -2.43686298e+00\n",
      "  -1.12453703e+00 -4.43373759e+00]\n",
      " [-5.15362381e+00  1.85533264e+00  3.00834434e+00 ... -2.40515443e+00\n",
      "  -2.78472716e+00 -5.35958667e+00]\n",
      " ...\n",
      " [-3.19814666e-01  9.41343600e-02 -3.14342838e-01 ... -6.04742135e-01\n",
      "   2.71568353e+00 -3.01581235e+00]\n",
      " [-1.55362445e+00 -6.93621639e-01  9.17291397e-01 ...  1.11014511e+00\n",
      "   3.57671358e-01 -3.54036163e+00]\n",
      " [-8.52460027e-03  1.59905050e-02 -9.81799053e-03 ...  5.80986538e-03\n",
      "  -4.04979897e-03 -1.65773625e-02]]\n",
      "[[-2.28129705e+00  1.27851928e+00  1.66060759e+00 ... -2.37018701e+00\n",
      "   5.14720455e-01 -3.97695769e+00]\n",
      " [-2.87029149e+00  1.66977609e+00  1.72528818e+00 ... -2.43684866e+00\n",
      "  -1.12452550e+00 -4.43385035e+00]\n",
      " [-5.15361757e+00  1.85534010e+00  3.00835339e+00 ... -2.40514635e+00\n",
      "  -2.78472123e+00 -5.35964873e+00]\n",
      " ...\n",
      " [-3.19798831e-01  9.41538680e-02 -3.14323032e-01 ... -6.04722552e-01\n",
      "   2.71569945e+00 -3.01597278e+00]\n",
      " [-1.55361206e+00 -6.93606383e-01  9.17306611e-01 ...  1.11015998e+00\n",
      "   3.57682385e-01 -3.54048030e+00]\n",
      " [-8.52469328e-03  1.59903154e-02 -9.81812144e-03 ...  5.80965444e-03\n",
      "  -4.04983344e-03 -1.65764448e-02]]\n",
      "numerical: 0.429481, analytic 0.429481, relative error: 1.020486e-09\n",
      "[[-2.28130790e+00  1.27850666e+00  1.66059333e+00 ... -2.37019968e+00\n",
      "   5.14745717e-01 -3.97687974e+00]\n",
      " [-2.87029819e+00  1.66976820e+00  1.72527830e+00 ... -2.43685707e+00\n",
      "  -1.12451278e+00 -4.43379624e+00]\n",
      " [-5.15362030e+00  1.85533744e+00  3.00834830e+00 ... -2.40514977e+00\n",
      "  -2.78472287e+00 -5.35961760e+00]\n",
      " ...\n",
      " [-3.19810617e-01  9.41402506e-02 -3.14338127e-01 ... -6.04736604e-01\n",
      "   2.71573492e+00 -3.01589711e+00]\n",
      " [-1.55362097e+00 -6.93616638e-01  9.17295339e-01 ...  1.11014964e+00\n",
      "   3.57708431e-01 -3.54042424e+00]\n",
      " [-8.52465985e-03  1.59904373e-02 -9.81803605e-03 ...  5.80978075e-03\n",
      "  -4.04966018e-03 -1.65769242e-02]]\n",
      "[[-2.28130222e+00  1.27851213e+00  1.66060131e+00 ... -2.37019353e+00\n",
      "   5.14677727e-01 -3.97687081e+00]\n",
      " [-2.87029539e+00  1.66977009e+00  1.72528309e+00 ... -2.43685457e+00\n",
      "  -1.12454975e+00 -4.43379170e+00]\n",
      " [-5.15362109e+00  1.85533529e+00  3.00834942e+00 ... -2.40515100e+00\n",
      "  -2.78472552e+00 -5.35961780e+00]\n",
      " ...\n",
      " [-3.19802880e-01  9.41479769e-02 -3.14327742e-01 ... -6.04728083e-01\n",
      "   2.71564806e+00 -3.01588802e+00]\n",
      " [-1.55361554e+00 -6.93611384e-01  9.17302669e-01 ...  1.11015545e+00\n",
      "   3.57645313e-01 -3.54041769e+00]\n",
      " [-8.52463370e-03  1.59903830e-02 -9.81807593e-03 ...  5.80973908e-03\n",
      "  -4.04997198e-03 -1.65768833e-02]]\n",
      "numerical: 1.908382, analytic 1.908382, relative error: 3.442286e-10\n",
      "\n",
      "------------\n",
      "Gradient check : reg=1e-2\n",
      "------------\n",
      "\n",
      "[[-2.24881816e+00  1.26627427e+00  1.65003389e+00 ... -2.38542074e+00\n",
      "   5.21092503e-01 -3.98186268e+00]\n",
      " [-2.84105463e+00  1.62856633e+00  1.71883235e+00 ... -2.45441299e+00\n",
      "  -1.12368699e+00 -4.42213767e+00]\n",
      " [-5.17563307e+00  1.87823084e+00  3.02638068e+00 ... -2.42386578e+00\n",
      "  -2.79008196e+00 -5.34901059e+00]\n",
      " ...\n",
      " [-3.11725843e-01  1.11904666e-01 -3.42849085e-01 ... -5.89119213e-01\n",
      "   2.73778621e+00 -3.00531364e+00]\n",
      " [-1.55743002e+00 -6.99708628e-01  8.93634040e-01 ...  1.07981047e+00\n",
      "   3.52033380e-01 -3.53285112e+00]\n",
      " [ 2.38598124e-03  4.43685587e-02 -2.49535387e-02 ... -5.55396417e-03\n",
      "   8.92861690e-03 -2.40050316e-02]]\n",
      "[[-2.24882238e+00  1.26626925e+00  1.65002837e+00 ... -2.38537732e+00\n",
      "   5.21088481e-01 -3.98186783e+00]\n",
      " [-2.84105991e+00  1.62856021e+00  1.71882508e+00 ... -2.45435770e+00\n",
      "  -1.12369217e+00 -4.42214390e+00]\n",
      " [-5.17564083e+00  1.87822250e+00  3.02637047e+00 ... -2.42378629e+00\n",
      "  -2.79008995e+00 -5.34901939e+00]\n",
      " ...\n",
      " [-3.11727351e-01  1.11901789e-01 -3.42852253e-01 ... -5.89101990e-01\n",
      "   2.73778503e+00 -3.00531474e+00]\n",
      " [-1.55743624e+00 -6.99715529e-01  8.93626040e-01 ...  1.07987033e+00\n",
      "   3.52027572e-01 -3.53285711e+00]\n",
      " [ 2.38601172e-03  4.43686453e-02 -2.49534883e-02 ... -5.55412464e-03\n",
      "   8.92862193e-03 -2.40050404e-02]]\n",
      "[[-2.24881393e+00  1.26627928e+00  1.65003941e+00 ... -2.38546417e+00\n",
      "   5.21096524e-01 -3.98185754e+00]\n",
      " [-2.84104935e+00  1.62857245e+00  1.71883962e+00 ... -2.45446828e+00\n",
      "  -1.12368181e+00 -4.42213144e+00]\n",
      " [-5.17562532e+00  1.87823918e+00  3.02639088e+00 ... -2.42394526e+00\n",
      "  -2.79007396e+00 -5.34900179e+00]\n",
      " ...\n",
      " [-3.11724334e-01  1.11907544e-01 -3.42845917e-01 ... -5.89136437e-01\n",
      "   2.73778738e+00 -3.00531254e+00]\n",
      " [-1.55742379e+00 -6.99701727e-01  8.93642040e-01 ...  1.07975061e+00\n",
      "   3.52039187e-01 -3.53284512e+00]\n",
      " [ 2.38595074e-03  4.43684720e-02 -2.49535892e-02 ... -5.55380345e-03\n",
      "   8.92861184e-03 -2.40050227e-02]]\n",
      "numerical: 0.506834, analytic 0.481477, relative error: 2.565664e-02\n",
      "[[-2.24882782e+00  1.26626537e+00  1.65002464e+00 ... -2.38533388e+00\n",
      "   5.21083202e-01 -3.98187104e+00]\n",
      " [-2.84106692e+00  1.62855457e+00  1.71881981e+00 ... -2.45429762e+00\n",
      "  -1.12369922e+00 -4.42214996e+00]\n",
      " [-5.17565157e+00  1.87821262e+00  3.02636123e+00 ... -2.42368990e+00\n",
      "  -2.79010054e+00 -5.34903021e+00]\n",
      " ...\n",
      " [-3.11731325e-01  1.11899096e-01 -3.42854042e-01 ... -5.89067722e-01\n",
      "   2.73778143e+00 -3.00531855e+00]\n",
      " [-1.55744131e+00 -6.99719824e-01  8.93623216e-01 ...  1.07991717e+00\n",
      "   3.52022703e-01 -3.53286219e+00]\n",
      " [ 2.38603842e-03  4.43686058e-02 -2.49535027e-02 ... -5.55416403e-03\n",
      "   8.92860111e-03 -2.40049526e-02]]\n",
      "[[-2.24880849e+00  1.26628316e+00  1.65004314e+00 ... -2.38550760e+00\n",
      "   5.21101803e-01 -3.98185433e+00]\n",
      " [-2.84104234e+00  1.62857809e+00  1.71884490e+00 ... -2.45452836e+00\n",
      "  -1.12367476e+00 -4.42212537e+00]\n",
      " [-5.17561458e+00  1.87824906e+00  3.02640012e+00 ... -2.42404165e+00\n",
      "  -2.79006337e+00 -5.34899097e+00]\n",
      " ...\n",
      " [-3.11720361e-01  1.11910237e-01 -3.42844128e-01 ... -5.89170703e-01\n",
      "   2.73779099e+00 -3.00530873e+00]\n",
      " [-1.55741872e+00 -6.99697432e-01  8.93644864e-01 ...  1.07970377e+00\n",
      "   3.52044056e-01 -3.53284004e+00]\n",
      " [ 2.38592404e-03  4.43685115e-02 -2.49535748e-02 ... -5.55376403e-03\n",
      "   8.92863265e-03 -2.40051106e-02]]\n",
      "numerical: 1.261845, analytic 1.250865, relative error: 4.369754e-03\n",
      "[[-2.24883743e+00  1.26625567e+00  1.65001329e+00 ... -2.38543944e+00\n",
      "   5.21069767e-01 -3.98187986e+00]\n",
      " [-2.84107173e+00  1.62855000e+00  1.71881383e+00 ... -2.45442958e+00\n",
      "  -1.12370705e+00 -4.42215247e+00]\n",
      " [-5.17564925e+00  1.87821549e+00  3.02636332e+00 ... -2.42388130e+00\n",
      "  -2.79010100e+00 -5.34902402e+00]\n",
      " ...\n",
      " [-3.11735132e-01  1.11896452e-01 -3.42858457e-01 ... -5.89126895e-01\n",
      "   2.73777540e+00 -3.00532086e+00]\n",
      " [-1.55743921e+00 -6.99716840e-01  8.93625043e-01 ...  1.07980304e+00\n",
      "   3.52022773e-01 -3.53285800e+00]\n",
      " [ 2.38591696e-03  4.43684976e-02 -2.49536275e-02 ... -5.55403768e-03\n",
      "   8.92844784e-03 -2.40050588e-02]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.24879889e+00  1.26629287e+00  1.65005448e+00 ... -2.38540205e+00\n",
      "   5.21115238e-01 -3.98184550e+00]\n",
      " [-2.84103753e+00  1.62858266e+00  1.71885087e+00 ... -2.45439640e+00\n",
      "  -1.12366693e+00 -4.42212287e+00]\n",
      " [-5.17561690e+00  1.87824619e+00  3.02639803e+00 ... -2.42385025e+00\n",
      "  -2.79006291e+00 -5.34899716e+00]\n",
      " ...\n",
      " [-3.11716553e-01  1.11912881e-01 -3.42839713e-01 ... -5.89111531e-01\n",
      "   2.73779702e+00 -3.00530642e+00]\n",
      " [-1.55742082e+00 -6.99700416e-01  8.93643036e-01 ...  1.07981789e+00\n",
      "   3.52043986e-01 -3.53284424e+00]\n",
      " [ 2.38604550e-03  4.43686197e-02 -2.49534500e-02 ... -5.55389069e-03\n",
      "   8.92878593e-03 -2.40050043e-02]]\n",
      "numerical: 1.168015, analytic 1.155938, relative error: 5.196695e-03\n",
      "[[-2.24882927e+00  1.26626268e+00  1.65002273e+00 ... -2.38531697e+00\n",
      "   5.21081585e-01 -3.98187351e+00]\n",
      " [-2.84106810e+00  1.62855240e+00  1.71881878e+00 ... -2.45428807e+00\n",
      "  -1.12370004e+00 -4.42215132e+00]\n",
      " [-5.17564861e+00  1.87821485e+00  3.02636467e+00 ... -2.42372096e+00\n",
      "  -2.79009724e+00 -5.34902656e+00]\n",
      " ...\n",
      " [-3.11731313e-01  1.11899934e-01 -3.42854156e-01 ... -5.89069506e-01\n",
      "   2.73778139e+00 -3.00531944e+00]\n",
      " [-1.55743524e+00 -6.99713299e-01  8.93629079e-01 ...  1.07986039e+00\n",
      "   3.52028720e-01 -3.53285721e+00]\n",
      " [ 2.38602428e-03  4.43686025e-02 -2.49535188e-02 ... -5.55410675e-03\n",
      "   8.92857878e-03 -2.40049569e-02]]\n",
      "[[-2.24880704e+00  1.26628585e+00  1.65004504e+00 ... -2.38552451e+00\n",
      "   5.21103420e-01 -3.98185185e+00]\n",
      " [-2.84104116e+00  1.62858026e+00  1.71884592e+00 ... -2.45453791e+00\n",
      "  -1.12367394e+00 -4.42212401e+00]\n",
      " [-5.17561754e+00  1.87824683e+00  3.02639668e+00 ... -2.42401059e+00\n",
      "  -2.79006667e+00 -5.34899462e+00]\n",
      " ...\n",
      " [-3.11720372e-01  1.11909399e-01 -3.42844014e-01 ... -5.89168921e-01\n",
      "   2.73779103e+00 -3.00530784e+00]\n",
      " [-1.55742479e+00 -6.99703956e-01  8.93639000e-01 ...  1.07976055e+00\n",
      "   3.52038040e-01 -3.53284502e+00]\n",
      " [ 2.38593818e-03  4.43685148e-02 -2.49535587e-02 ... -5.55382134e-03\n",
      "   8.92865500e-03 -2.40051063e-02]]\n",
      "numerical: -0.404677, analytic -0.417484, relative error: 1.557762e-02\n",
      "[[-2.24881082e+00  1.26627291e+00  1.65003385e+00 ... -2.38542035e+00\n",
      "   5.21092331e-01 -3.98186276e+00]\n",
      " [-2.84103622e+00  1.62856379e+00  1.71883107e+00 ... -2.45441351e+00\n",
      "  -1.12368821e+00 -4.42213886e+00]\n",
      " [-5.17558010e+00  1.87822446e+00  3.02637587e+00 ... -2.42387040e+00\n",
      "  -2.79008680e+00 -5.34901544e+00]\n",
      " ...\n",
      " [-3.11718119e-01  1.11903959e-01 -3.42850297e-01 ... -5.89119072e-01\n",
      "   2.73778528e+00 -3.00531261e+00]\n",
      " [-1.55737448e+00 -6.99714191e-01  8.93627922e-01 ...  1.07980556e+00\n",
      "   3.52027404e-01 -3.53285557e+00]\n",
      " [ 2.38586847e-03  4.43685909e-02 -2.49535136e-02 ... -5.55394570e-03\n",
      "   8.92862444e-03 -2.40050184e-02]]\n",
      "[[-2.24882549e+00  1.26627562e+00  1.65003392e+00 ... -2.38542113e+00\n",
      "   5.21092675e-01 -3.98186261e+00]\n",
      " [-2.84107304e+00  1.62856888e+00  1.71883364e+00 ... -2.45441247e+00\n",
      "  -1.12368577e+00 -4.42213647e+00]\n",
      " [-5.17568605e+00  1.87823722e+00  3.02638548e+00 ... -2.42386115e+00\n",
      "  -2.79007711e+00 -5.34900574e+00]\n",
      " ...\n",
      " [-3.11733565e-01  1.11905374e-01 -3.42847873e-01 ... -5.89119354e-01\n",
      "   2.73778714e+00 -3.00531467e+00]\n",
      " [-1.55748555e+00 -6.99703064e-01  8.93640158e-01 ...  1.07981538e+00\n",
      "   3.52039355e-01 -3.53284667e+00]\n",
      " [ 2.38609426e-03  4.43685264e-02 -2.49535639e-02 ... -5.55398266e-03\n",
      "   8.92860933e-03 -2.40050447e-02]]\n",
      "numerical: -3.447799, analytic -3.440221, relative error: 1.100196e-03\n",
      "[[-2.24882627e+00  1.26626497e+00  1.65002525e+00 ... -2.38533890e+00\n",
      "   5.21083972e-01 -3.98187189e+00]\n",
      " [-2.84106062e+00  1.62855957e+00  1.71882615e+00 ... -2.45435342e+00\n",
      "  -1.12369316e+00 -4.42214415e+00]\n",
      " [-5.17563755e+00  1.87822651e+00  3.02637698e+00 ... -2.42382495e+00\n",
      "  -2.79008640e+00 -5.34901442e+00]\n",
      " ...\n",
      " [-3.11748287e-01  1.11880792e-01 -3.42873681e-01 ... -5.88900011e-01\n",
      "   2.73776191e+00 -3.00533780e+00]\n",
      " [-1.55745003e+00 -6.99729422e-01  8.93612569e-01 ...  1.08000313e+00\n",
      "   3.52011715e-01 -3.53287173e+00]\n",
      " [ 2.38604673e-03  4.43686621e-02 -2.49534195e-02 ... -5.55458883e-03\n",
      "   8.92865372e-03 -2.40048744e-02]]\n",
      "[[-2.24881004e+00  1.26628357e+00  1.65004253e+00 ... -2.38550258e+00\n",
      "   5.21101033e-01 -3.98185348e+00]\n",
      " [-2.84104864e+00  1.62857309e+00  1.71883856e+00 ... -2.45447256e+00\n",
      "  -1.12368082e+00 -4.42213118e+00]\n",
      " [-5.17562860e+00  1.87823517e+00  3.02638437e+00 ... -2.42390660e+00\n",
      "  -2.79007751e+00 -5.34900676e+00]\n",
      " ...\n",
      " [-3.11703398e-01  1.11928541e-01 -3.42824488e-01 ... -5.89338416e-01\n",
      "   2.73781051e+00 -3.00528949e+00]\n",
      " [-1.55741000e+00 -6.99687833e-01  8.93655511e-01 ...  1.07961780e+00\n",
      "   3.52055044e-01 -3.53283050e+00]\n",
      " [ 2.38591573e-03  4.43684553e-02 -2.49536579e-02 ... -5.55333926e-03\n",
      "   8.92858005e-03 -2.40051888e-02]]\n",
      "numerical: -0.618183, analytic -0.618164, relative error: 1.531801e-05\n",
      "[[-2.24884657e+00  1.26624512e+00  1.65000423e+00 ... -2.38545033e+00\n",
      "   5.21065182e-01 -3.98159940e+00]\n",
      " [-2.84108024e+00  1.62854019e+00  1.71880537e+00 ... -2.45443955e+00\n",
      "  -1.12371098e+00 -4.42190261e+00]\n",
      " [-5.17565672e+00  1.87820743e+00  3.02635609e+00 ... -2.42389000e+00\n",
      "  -2.79010363e+00 -5.34879726e+00]\n",
      " ...\n",
      " [-3.11738188e-01  1.11891821e-01 -3.42861683e-01 ... -5.89131520e-01\n",
      "   2.73777443e+00 -3.00519838e+00]\n",
      " [-1.55744172e+00 -6.99720463e-01  8.93622228e-01 ...  1.07979879e+00\n",
      "   3.52022266e-01 -3.53274254e+00]\n",
      " [ 2.38608884e-03  4.43686725e-02 -2.49534103e-02 ... -5.55385459e-03\n",
      "   8.92862838e-03 -2.40057283e-02]]\n",
      "[[-2.24878974e+00  1.26630341e+00  1.65006354e+00 ... -2.38539116e+00\n",
      "   5.21119824e-01 -3.98212597e+00]\n",
      " [-2.84102902e+00  1.62859247e+00  1.71885934e+00 ... -2.45438643e+00\n",
      "  -1.12366300e+00 -4.42237272e+00]\n",
      " [-5.17560943e+00  1.87825425e+00  3.02640526e+00 ... -2.42384155e+00\n",
      "  -2.79006029e+00 -5.34922392e+00]\n",
      " ...\n",
      " [-3.11713497e-01  1.11917512e-01 -3.42836486e-01 ... -5.89106906e-01\n",
      "   2.73779799e+00 -3.00542891e+00]\n",
      " [-1.55741831e+00 -6.99696792e-01  8.93645852e-01 ...  1.07982215e+00\n",
      "   3.52044493e-01 -3.53295969e+00]\n",
      " [ 2.38587361e-03  4.43684449e-02 -2.49536672e-02 ... -5.55407378e-03\n",
      "   8.92860539e-03 -2.40043345e-02]]\n",
      "numerical: -3.695363, analytic -3.703944, relative error: 1.159763e-03\n",
      "[[-2.24883338e+00  1.26625826e+00  1.65001965e+00 ... -2.38543533e+00\n",
      "   5.21074172e-01 -3.98187695e+00]\n",
      " [-2.84106710e+00  1.62855347e+00  1.71882123e+00 ... -2.45442453e+00\n",
      "  -1.12370193e+00 -4.42214857e+00]\n",
      " [-5.17564364e+00  1.87822084e+00  3.02637230e+00 ... -2.42387442e+00\n",
      "  -2.79009460e+00 -5.34901837e+00]\n",
      " ...\n",
      " [-3.11750738e-01  1.11880874e-01 -3.42872818e-01 ... -5.89142677e-01\n",
      "   2.73775865e+00 -3.00533663e+00]\n",
      " [-1.55745307e+00 -6.99730293e-01  8.93612871e-01 ...  1.07978949e+00\n",
      "   3.52007741e-01 -3.53287126e+00]\n",
      " [ 2.38596673e-03  4.43685765e-02 -2.49535260e-02 ... -5.55390817e-03\n",
      "   8.92854516e-03 -2.40049501e-02]]\n",
      "[[-2.24880293e+00  1.26629027e+00  1.65004813e+00 ... -2.38540615e+00\n",
      "   5.21110833e-01 -3.98184841e+00]\n",
      " [-2.84104216e+00  1.62857919e+00  1.71884347e+00 ... -2.45440145e+00\n",
      "  -1.12367205e+00 -4.42212676e+00]\n",
      " [-5.17562251e+00  1.87824084e+00  3.02638905e+00 ... -2.42385713e+00\n",
      "  -2.79006931e+00 -5.34900281e+00]\n",
      " ...\n",
      " [-3.11700948e-01  1.11928459e-01 -3.42825352e-01 ... -5.89095749e-01\n",
      "   2.73781377e+00 -3.00529066e+00]\n",
      " [-1.55740696e+00 -6.99686962e-01  8.93655208e-01 ...  1.07983145e+00\n",
      "   3.52059017e-01 -3.53283097e+00]\n",
      " [ 2.38599573e-03  4.43685408e-02 -2.49535514e-02 ... -5.55402019e-03\n",
      "   8.92868861e-03 -2.40051131e-02]]\n",
      "numerical: -0.354086, analytic -0.353220, relative error: 1.225479e-03\n",
      "[[-2.24882661e+00  1.26626810e+00  1.65002924e+00 ... -2.38542536e+00\n",
      "   5.21086900e-01 -3.98186682e+00]\n",
      " [-2.84106627e+00  1.62855696e+00  1.71882408e+00 ... -2.45442079e+00\n",
      "  -1.12369646e+00 -4.42214582e+00]\n",
      " [-5.17565037e+00  1.87821612e+00  3.02636650e+00 ... -2.42387936e+00\n",
      "  -2.79009785e+00 -5.34902497e+00]\n",
      " ...\n",
      " [-3.11730843e-01  1.11900641e-01 -3.42851634e-01 ... -5.89121543e-01\n",
      "   2.73778203e+00 -3.00531602e+00]\n",
      " [-1.55743963e+00 -6.99716478e-01  8.93626627e-01 ...  1.07980379e+00\n",
      "   3.52024273e-01 -3.53285819e+00]\n",
      " [ 2.38596287e-03  4.43685628e-02 -2.49535290e-02 ... -5.55394784e-03\n",
      "   8.92858360e-03 -2.40050122e-02]]\n",
      "[[-2.24880970e+00  1.26628043e+00  1.65003854e+00 ... -2.38541612e+00\n",
      "   5.21098105e-01 -3.98185855e+00]\n",
      " [-2.84104300e+00  1.62857570e+00  1.71884062e+00 ... -2.45440519e+00\n",
      "  -1.12367752e+00 -4.42212952e+00]\n",
      " [-5.17561578e+00  1.87824556e+00  3.02639485e+00 ... -2.42385219e+00\n",
      "  -2.79006606e+00 -5.34899621e+00]\n",
      " ...\n",
      " [-3.11720843e-01  1.11908692e-01 -3.42846537e-01 ... -5.89116884e-01\n",
      "   2.73779039e+00 -3.00531126e+00]\n",
      " [-1.55742040e+00 -6.99700777e-01  8.93641452e-01 ...  1.07981715e+00\n",
      "   3.52042486e-01 -3.53284404e+00]\n",
      " [ 2.38599959e-03  4.43685545e-02 -2.49535485e-02 ... -5.55398053e-03\n",
      "   8.92865017e-03 -2.40050510e-02]]\n",
      "numerical: 4.449339, analytic 4.442956, relative error: 7.177076e-04\n",
      "[[-2.24884967e+00  1.26624025e+00  1.64999878e+00 ... -2.38511142e+00\n",
      "   5.21058794e-01 -3.98189762e+00]\n",
      " [-2.84108243e+00  1.62853613e+00  1.71880067e+00 ... -2.45413957e+00\n",
      "  -1.12371653e+00 -4.42216785e+00]\n",
      " [-5.17566014e+00  1.87820204e+00  3.02635051e+00 ... -2.42360357e+00\n",
      "  -2.79011046e+00 -5.34903883e+00]\n",
      " ...\n",
      " [-3.11732003e-01  1.11897128e-01 -3.42857100e-01 ... -5.89049152e-01\n",
      "   2.73777857e+00 -3.00532123e+00]\n",
      " [-1.55743683e+00 -6.99716452e-01  8.93626343e-01 ...  1.07988423e+00\n",
      "   3.52025329e-01 -3.53285918e+00]\n",
      " [ 2.38602800e-03  4.43686015e-02 -2.49534850e-02 ... -5.55406953e-03\n",
      "   8.92854260e-03 -2.40049394e-02]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.24878664e+00  1.26630828e+00  1.65006899e+00 ... -2.38573007e+00\n",
      "   5.21126211e-01 -3.98182774e+00]\n",
      " [-2.84102684e+00  1.62859653e+00  1.71886403e+00 ... -2.45468640e+00\n",
      "  -1.12365745e+00 -4.42210748e+00]\n",
      " [-5.17560601e+00  1.87825964e+00  3.02641084e+00 ... -2.42412798e+00\n",
      "  -2.79005345e+00 -5.34898234e+00]\n",
      " ...\n",
      " [-3.11719683e-01  1.11912205e-01 -3.42841070e-01 ... -5.89189276e-01\n",
      "   2.73779385e+00 -3.00530605e+00]\n",
      " [-1.55742320e+00 -6.99700803e-01  8.93641737e-01 ...  1.07973670e+00\n",
      "   3.52041430e-01 -3.53284305e+00]\n",
      " [ 2.38593446e-03  4.43685158e-02 -2.49535925e-02 ... -5.55385849e-03\n",
      "   8.92869117e-03 -2.40051238e-02]]\n",
      "numerical: -0.726355, analytic -0.722099, relative error: 2.938460e-03\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# Maintenant testons le gradient numérique avec et sans régularisation       #\n",
    "# Les erreurs relatives devraient être inférieures à 1e-6                    #\n",
    "##############################################################################\n",
    "from utils.gradients import check_gradient_sparse\n",
    "\n",
    "print(\"\\n------------\\nGradient check : reg=0\\n------------\\n\")\n",
    "check_gradient_sparse(softmax_ce_naive_forward_backward, W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# Faire un autre test de gradients avec régularisation \n",
    "print(\"\\n------------\\nGradient check : reg=1e-2\\n------------\\n\")\n",
    "check_gradient_sparse(softmax_ce_naive_forward_backward, W, X_dev, y_dev, 1e2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax et gradients vectorisés\n",
    "Passons maintenant aux choses sérieuses. Vous devez ici coder la version vectorisée de l'entropie croisée et du gradient dans la fonction **softmax_ce_forward_backward**.  Ce code s'apparente à la réponse que vous avec donné au début."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.28130506e+00  1.27850939e+00  1.66059732e+00 ... -2.37019661e+00\n",
      "   5.14711722e-01 -3.97687527e+00]\n",
      " [-2.87029679e+00  1.66976914e+00  1.72528070e+00 ... -2.43685582e+00\n",
      "  -1.12453126e+00 -4.43379397e+00]\n",
      " [-5.15362069e+00  1.85533637e+00  3.00834886e+00 ... -2.40515039e+00\n",
      "  -2.78472420e+00 -5.35961770e+00]\n",
      " ...\n",
      " [-3.19806748e-01  9.41441155e-02 -3.14332938e-01 ... -6.04732342e-01\n",
      "   2.71569149e+00 -3.01589256e+00]\n",
      " [-1.55361825e+00 -6.93614012e-01  9.17299002e-01 ...  1.11015254e+00\n",
      "   3.57676871e-01 -3.54042096e+00]\n",
      " [-8.52464567e-03  1.59904130e-02 -9.81805749e-03 ...  5.80975879e-03\n",
      "  -4.04981490e-03 -1.65769045e-02]]\n",
      "naive loss: 2.387263e+00 computed in 0.147961s\n",
      "vectorized loss: 2.387263e+00 computed in 0.007005s\n",
      "bravo pour la loss!\n",
      "Loss difference: 0.000000\n",
      "il y a un bug au niveau du gradient\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# TODO: Implémenter le calcul de perte et du gradient de façon vectorielle   #\n",
    "# dans la fonction softmax_ce_forward_backward située dans le fichier        #\n",
    "# utils.loss.                                                                #\n",
    "# Les deux versions devraient calculer les mêmes résultats, mais la version  #\n",
    "# vectorielle devrait être BEAUCOUP PLUS RAPIDE.                             #\n",
    "##############################################################################\n",
    "start = time.time()\n",
    "loss_naive, grad_naive = softmax_ce_naive_forward_backward(X_dev, W, y_dev, 0.00001)\n",
    "end = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, end - start))\n",
    "\n",
    "from utils.loss import softmax_ce_forward_backward\n",
    "start = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_ce_forward_backward(X_dev, W, y_dev, 0.00001)\n",
    "end = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, end - start))\n",
    "\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "loss_diff = np.abs(loss_naive - loss_vectorized)\n",
    "if loss_diff < 1e-7:\n",
    "    print('bravo pour la loss!')\n",
    "else:\n",
    "    print('il y a un bug au niveau de la loss')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "\n",
    "if grad_difference < 1e-7:\n",
    "    print('bravo pour le gradient !')\n",
    "else:\n",
    "    print('il y a un bug au niveau du gradient')\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraînement\n",
    "À l'aide de la classe **LinearClassifier** ainsi que de la fonction vectorisée **softmax_ce_forward_backward** que vous venez de coder, vous devez maintenant entraîner un réseau de neurones multiclasses linéaire à l'aide d'une **descente de gradient**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train vs val acc 0.280490 / 0.296000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from model.LinearClassifier import LinearClassifier\n",
    "from visualization.utils import visualize_loss\n",
    "import itertools as it\n",
    "lr = 1e-7\n",
    "reg = 1\n",
    "\n",
    "classifier = LinearClassifier(softmax_ce_forward_backward)\n",
    "#\n",
    "# TODO : ajouter code à la fonction train.  Si tout fonctionne bien, la courbe de la loss devrait décroitre\n",
    "#\n",
    "train_loss_history = classifier.train(X_train, y_train, learning_rate=lr, reg=reg, num_iter=3000, verbose = False)\n",
    "\n",
    "y_train_pred = classifier.predict(X_train)\n",
    "y_val_pred = classifier.predict(X_val)\n",
    "\n",
    "acc_train = np.mean(y_train == y_train_pred)\n",
    "acc_val = np.mean(y_val == y_val_pred)\n",
    "\n",
    "print('train vs val acc %f / %f' %(acc_train, acc_val))\n",
    "\n",
    "visualize_loss(train_loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recherche d'hyper-paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-11ffe98cd348>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mreg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mregularization_strengths\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mvalidation_loss_history\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[0my_val_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_val\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0my_val_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\TP Réseaux de neurones\\TP1\\TP1_RN\\model\\LinearClassifier.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, X, y, learning_rate, weigth_scale, reg, num_iter, batch_size, verbose)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[1;31m#forward & backward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_backward_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[1;31m#descente de gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\TP Réseaux de neurones\\TP1\\TP1_RN\\utils\\loss.py\u001b[0m in \u001b[0;36msoftmax_ce_forward_backward\u001b[1;34m(X, W, y, reg)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;31m#calcul du gradient vectorisé\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m     \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m     \u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;31m#creation de la matrice des 1-hot-vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[0mS_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mS\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from model.LinearClassifier import LinearClassifier\n",
    "from visualization.utils import visualize_loss\n",
    "import itertools as it\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = np.linspace(1e-7, 1e-5, 5)\n",
    "regularization_strengths = np.linspace(1e3, 1e7, 5)\n",
    "best_loss_history = None\n",
    "best_classifier = None\n",
    "\n",
    "##############################################################################\n",
    "# TODO: Utilisez l'ensemble de validation pour régler les hyper-paramètres   #\n",
    "#  (force de régularisation et vitesse d'apprentissage). Vous devez          #\n",
    "#  expérimenter différentes plages de valeurs pour les taux d'apprentissage  #\n",
    "#  et les forces de régularisation; si tout va bien, avec num_iter = 1000    #\n",
    "#  vous devriez obtenir une précision de classification supérieur à 0.38 sur #\n",
    "#  l'ensemble de validation, et de 0.37 sur l'ensemble de test.              #\n",
    "#  Mettre les résultats des meilleurs hyper-paramètres dans les variables    #\n",
    "#  best_XYZ ci haut.                                                         #\n",
    "##############################################################################\n",
    "\n",
    "print(len(learning_rates))\n",
    "print(len(regularization_strengths))\n",
    "\n",
    "best_lr = 0.0\n",
    "best_reg = 0.0\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for reg in regularization_strengths:\n",
    "        \n",
    "        classifier = LinearClassifier(softmax_ce_forward_backward)\n",
    "        validation_loss_history = classifier.train(X_val, y_val, learning_rate=lr, reg=reg, num_iter=1000)\n",
    "        y_val_pred = classifier.predict(X_val)\n",
    "        pred = np.mean(y_val == y_val_pred)\n",
    "        \n",
    "        if(pred > best_val):\n",
    "            best_lr = lr\n",
    "            best_reg = reg\n",
    "            best_val = pred\n",
    "            best_classifier = classifier\n",
    "            best_loss_history = validation_loss_history\n",
    "        \n",
    "\n",
    "################################################################################\n",
    "#                             FIN DE VOTRE CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)\n",
    "\n",
    "visualize_loss(best_loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On évalue le modèle sur l'ensemble de test\n",
    "y_test_pred = best_classifier.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('Test set accuracy: %f' % (test_accuracy) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des poids appris pour chaque classe\n",
    "w = best_classifier.W[:-1,:] # retire le biais\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "\n",
    "    # Redimensionne les poids pour qu'ils soient entre 0 et 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
